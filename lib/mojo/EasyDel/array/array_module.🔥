from utils.vector import InlinedFixedVector
from random import rand
from sys.info import simdwidthof
from math.math import (
    sqrt,
    sin,
    cos,
    tanh,
    tan,
    log,
    log2,
    atan,
    exp,
    exp2,
    min,
    pow,
    log10,
    log1p,
    logb,
    asin,
    acos,
    asinh,
    acosh,
)
import math
from algorithm.functional import (
    vectorize,
    vectorize_unroll,
    Static2DTileUnitFunc,
    parallelize,
)
from runtime.llcl import num_cores
from memory.memory import memset_zero
from .array_utils import matmul_shape

alias dims_average_size = 5
alias debuging = True

# parts of this code is inspiered from https://github.com/andresnowak/Micro-Mojograd


fn do_check(res: Bool, string: StringRef):
    if not res:
        # print(string)
        ...


struct ArrayShape:
    var _shape: Pointer[Int]
    var _length: Int
    var _size: Int
    var _allocated: Bool

    fn __init__(inout self, shape: VariadicList[Int]) -> None:
        self._length = len(shape)
        self._shape = Pointer[Int]().alloc(self._length)
        for i in range(self._length):
            self._shape.store(i, shape[i])
        self._size = 1
        self._allocated = True
        self._size = self.product_dims()

    fn __init__(inout self, shape: DynamicVector[Int]) -> None:
        self._length = len(shape)
        self._shape = Pointer[Int]().alloc(self._length)
        for i in range(self._length):
            self._shape.store(i, shape[i])
        self._size = 1
        self._allocated = True
        self._size = self.product_dims()

    fn __init__[off: Int](inout self, shape: InlinedFixedVector[off, Int]) -> None:
        self._length = len(shape)
        self._shape = Pointer[Int]().alloc(self._length)
        for i in range(self._length):
            self._shape.store(i, shape[i])
        self._size = 1
        self._allocated = True
        self._size = self.product_dims()

    fn __init__(inout self, *elms: Int) -> None:
        let shape: VariadicList[Int] = VariadicList[Int](elms)
        self._length = len(shape)
        self._shape = Pointer[Int]().alloc(self._length)
        for i in range(self._length):
            self._shape.store(i, shape[i])
        self._size = 1
        self._allocated = True
        self._size = self.product_dims()

    fn __copyinit__(inout self: Self, ext: Self):
        self._size = ext._size
        self._shape = ext._shape
        self._length = ext._length
        self._allocated = ext._allocated

    fn __moveinit__(inout self: Self, owned ext: Self):
        self._shape = ext._shape ^
        self._length = ext._length
        self._size = ext._size
        self._allocated = ext._allocated

    fn __len__(self) -> Int:
        return self._size

    fn __eq__(self, other: ArrayShape) -> Bool:
        let mr = self.rank()
        for i in range(mr):
            if self._shape[i] != self._shape[i]:
                return False

        if mr != other.rank():
            return False
        return True

    fn __is__(self, other: ArrayShape) -> Bool:
        let mr = self.rank()
        for i in range(mr):
            if self._shape[i] != self._shape[i]:
                return False

        if mr != other.rank():
            return False
        return True

    fn __eq_matmul__(self, other: ArrayShape) -> Bool:
        let mr = self.rank()
        let tr = other.rank()
        if mr != tr:
            return False
        if mr == 1:
            return self == other
        for i in range(mr - 2):
            if self.dim(i) != other.dim(i):
                return False
        if self.dim(mr - 2) != other.dim(mr - 1):
            return False
        if self._allocated == False or other._allocated == False:
            return False
        return True

    fn __getitem__(self, idx: Int) -> Int:
        return self._shape[self.__ntp__(idx, self._length)]

    @staticmethod
    fn __ntp__(i: Int, m: Int) -> Int:
        if i < 0:
            return i + m
        return i

    fn rank(self: Self) -> Int:
        return self._length

    fn product_dims(self) -> Int:
        var res = 1
        for i in range(self.rank()):
            res *= self._shape[i]
        return res

    fn num_elements(self: Self) -> Int:
        return self._size

    fn dim(self, index: Int) -> Int:
        return self._shape[self.__ntp__(index, self._length)]

    fn shape(self: Self) -> Pointer[Int]:
        return self._shape

    @always_inline
    fn get_1d_pos[off: Int](self, index: InlinedFixedVector[off, Int]) -> Int:
        var product: Int = 1
        var position: Int = 0
        for i in range(self.rank() - 1, 0, -1):
            product *= self._shape[i]
            position += self.__ntp__(index[i - 1], self._shape[i - 1]) * product

        position += self.__ntp__(index[self.rank() - 1], self._shape[self.rank() - 1])
        return position

    @always_inline
    fn get_1d_pos[off: Int](self, index: StaticIntTuple[off]) -> Int:
        var product: Int = 1
        var position: Int = 0
        for i in range(self.rank() - 1, 0, -1):
            product *= self._shape[i]
            position += self.__ntp__(index[i - 1], self._shape[i - 1]) * product

        position += self.__ntp__(index[self.rank() - 1], self._shape[self.rank() - 1])
        return position

    fn shape_str(self: Self):
        print_no_newline("[")
        for i in range(self.rank()):
            if i == self.rank() - 1:
                print_no_newline(self[i])
            else:
                print_no_newline(self[i], ",")
        print("]")


struct Array[T: DType]:
    var data: DTypePointer[T]
    var array_shape: ArrayShape
    var allocated: Int
    alias nelts: Int = simdwidthof[T]() * 2
    alias ArrayPointer: AnyType = DTypePointer[T]

    fn __init__(inout self: Self, array_shape: ArrayShape) -> None:
        r"""Init Array From ArrayShape(Alloc Zero)."""
        self.array_shape = array_shape
        self.data = self.ArrayPointer.alloc(0)
        self.allocated = 0

    fn __init__(inout self: Self, A: Self, B: Self) -> None:
        r"""Init Array From Two other Arrays A and B For Matmul(Alloc One)."""
        self.__init__(matmul_shape[T](A, B))
        self.alloc()

    fn __init__(inout self: Self, *dim: Int):
        r"""Init Array from Int Args(Alloc Zero)."""
        self.array_shape = ArrayShape(VariadicList(dim))
        self.data = self.ArrayPointer.alloc(0)
        self.allocated = 0

    fn __init__(inout self: Self, init: Bool, *dim: Int) -> None:
        r"""Init Array from Int Args(Depends on Passed Bool)."""
        self.__init__(dim)
        if init:
            self.alloc()
            self.random()

    fn __init__(inout self: Self, vl: VariadicList[Int]) -> None:
        r"""Init Array from VariadicList[Int](Alloc Zero)."""
        self.array_shape = ArrayShape(vl)
        self.data = self.ArrayPointer.alloc(0)
        self.allocated = 0

    fn __init__(
        inout self: Self, value: DynamicVector[FloatLiteral], shape: ArrayShape
    ) -> None:
        r"""Init Array from ArrayShape and load data from DynamicVector[FloatLiteral](Alloc One).
        """
        self.array_shape = shape
        do_check(len(value) == self.array_shape.num_elements(), "Data Size miss match")
        self.data = self.ArrayPointer.alloc(self.array_shape.num_elements())
        self.allocated = 1
        for i in range(self.array_shape.num_elements()):
            self.data.simd_store[1](i, value[i])

    fn __init__(
        inout self: Self, value: VariadicList[FloatLiteral], shape: ArrayShape
    ) -> None:
        r"""Init Array from ArrayShape and load data from VariadicList[FloatLiteral](Alloc One).
        """
        self.array_shape = shape
        do_check(len(value) == self.array_shape.num_elements(), "Data Size miss match")
        self.allocated = 1
        self.data = self.ArrayPointer.alloc(self.array_shape.num_elements())
        for i in range(self.array_shape.num_elements()):
            self.data.simd_store[1](i, value[i])

    fn __init__(inout self: Self, pointer: DTypePointer[T], *dim: Int) -> None:
        self.data = pointer
        self.array_shape = ArrayShape(dim)
        self.allocated = 1

    fn __moveinit__(inout self, owned ext: Self):
        self.data = ext.data
        self.array_shape = ext.array_shape
        self.allocated = ext.allocated

    fn __copyinit__(inout self, ext: Self):
        self.allocated = ext.allocated
        if self.allocated == 1:
            self.data = self.ArrayPointer.alloc(ext.array_shape.num_elements())
            self.array_shape = ext.array_shape

            @parameter
            fn _do[_nelts: Int](f: Int):
                let dt = ext.data.simd_load[_nelts](0)
                self.data.simd_store[_nelts](0, dt)

            vectorize[Self.nelts, _do](ext.array_shape.num_elements())
        else:
            self.data = self.ArrayPointer.alloc(0)
            self.array_shape = ext.array_shape

    fn __del__(owned self):
        if self.allocated == 1:
            self.data.free()

    fn set_data_from_buffer(inout self: Self, buffer: DTypePointer[T]) -> None:
        self.data = buffer
        # self.allocated = 1

    fn alloc(inout self: Self) -> None:
        r"""Allocate or Init The Array."""
        self.data = self.ArrayPointer.alloc(self.array_shape.num_elements())
        self.allocated = 1

    fn random(inout self: Self) -> None:
        r"""Randomize The Data if the Array is Allocated."""
        if self.allocated == 1:
            rand[T](self.data, self.array_shape.num_elements())

    fn dim(self: Self, i: Int) -> Int:
        return self.array_shape.dim(i)

    fn rank(self: Self) -> Int:
        return self.array_shape.rank()

    fn num_elements(self: Self) -> Int:
        return self.array_shape.num_elements()

    fn get_dynamic_axis(self: Self, *dims: Int) raises -> Int:
        let variadic: VariadicList[Int] = VariadicList[Int](dims)
        let slice_dv: DynamicVector[Int] = DynamicVector[Int]()
        if len(variadic) > self.rank():
            raise Error("Index out Of the Range (len(variadic) > self.rank())")
        var pad: Int = 0
        for i in range(len(variadic)):
            var product: Int = 1
            for f in range(i + 1, self.rank()):
                product *= self.array_shape._shape[f]
            pad += variadic[i] * product
        if pad > self.num_elements():
            raise Error("Index out Of the Range (pad > self.num_elements)")
        return pad

    fn transpose(self: Self, *dims: Int) raises -> Self:
        var res_shape: DynamicVector[Int] = DynamicVector[Int]()
        let _dims = VariadicList[Int](dims)
        if len(_dims) != self.rank():
            raise Error(
                "The Passed input dims to swap are not accept able for function"
            )
        if len(_dims) != 3:
            raise Error("Only 3D arrays are accepted for transpose right now")
        for f in range(len(_dims)):
            res_shape.push_back(self.dim(_dims[f]))
        var res = Self(res_shape)
        res.alloc()
        res.fill(0.0)
        # let SL :Int= self.dim(0)
        let RL: Int = self.dim(1)
        let CL: Int = self.dim(2)
        let RR: Int = res.dim(1)
        let CR: Int = res.dim(2)

        @parameter
        fn self_index(layer: Int, row: Int, cols: Int) -> Int:
            return layer * RL * CL + row * CL + cols

        fn res_index(layer: Int, row: Int, cols: Int) -> Int:
            return layer * RR * CR + row * CR + cols

        @parameter
        fn index_s(inout C: Int, i: Int, j: Int, k: Int, s: Int):
            if _dims[s] == 0:
                C = i
            elif _dims[s] == 1:
                C = j
            elif _dims[s] == 2:
                C = k

        for i in range(res.dim(0)):
            var s = 0
            var d = 0
            var e = 0
            for j in range(res.dim(-2)):
                for k in range(res.dim(-1)):
                    index_s(s, i, j, k, 0)
                    index_s(d, i, j, k, 1)
                    index_s(e, i, j, k, 2)
                    res[res_index(i, j, k)] = self[self_index(s, d, e)]
        return res

    fn reshape(inout self, *dims: Int) raises:
        let current_nelm = self.num_elements()
        let vs: VariadicList[Int] = VariadicList[Int](dims)
        var dynamic_index: Bool = False
        for i in range(len(vs)):
            if vs[i] == -1:
                dynamic_index = True
        if dynamic_index:
            var vvs: DynamicVector[Int] = DynamicVector[Int]()
            var num_elms: Int = 1
            for i in range(len(vs)):
                if vs[i] != -1:
                    num_elms *= vs[i]

            for i in range(len(vs)):
                if vs[i] == -1:
                    vvs.push_back(current_nelm // num_elms)
                else:
                    vvs.push_back(vs[i])
            let array_shape = ArrayShape(vvs)
            if array_shape.num_elements() != current_nelm:
                raise Error(
                    "Given new shape and number of elements in array won't Match"
                )
            self.array_shape = array_shape
        else:
            var num_elms: Int = 1
            for i in range(len(vs)):
                num_elms *= vs[i]
            if num_elms != current_nelm:
                raise Error(
                    "Given new shape and number of elements in array won't Match"
                )
            self.array_shape = ArrayShape(vs)

    @always_inline
    fn load[
        nelts: Int, off: Int
    ](self, index: InlinedFixedVector[off, Int]) -> SIMD[T, nelts]:
        let position = self.array_shape.get_1d_pos(index)
        self.assertation(position, self.array_shape._size)
        return self.data.simd_load[nelts](position)

    @always_inline
    fn load[nelts: Int, off: Int](self, index: StaticIntTuple[off]) -> SIMD[T, nelts]:
        let position = self.array_shape.get_1d_pos(index)
        self.assertation(position, self.array_shape._size)
        return self.data.simd_load[nelts](position)

    @always_inline
    fn load[nelts: Int](self, index: Int) -> SIMD[T, nelts]:
        let position = self.array_shape.__ntp__(index, self.array_shape._size)
        self.assertation(position, self.array_shape._size)
        return self.data.simd_load[nelts](position)

    # i * r * Te.dim(2) + k * r + j

    @always_inline
    fn store[
        nelts: Int
    ](self, d1: Int, d2: Int, d3: Int, val: SIMD[T, nelts]) raises -> None:
        if self.rank() != 3:
            raise Error("Miss match os requested shape (3D Store)")
        let index = d1 * self.dim(-2) * self.dim(-1) + d2 * self.dim(-1) + d3
        if index > self.num_elements():
            raise Error("Out OF range in Store 3D")
        self.data.simd_store[nelts](index, val)

    @always_inline
    fn store[nelts: Int](self, d1: Int, d2: Int, val: SIMD[T, nelts]) raises -> None:
        if self.rank() != 2:
            raise Error("Miss match os requested shape (2D Store)")
        let index = d1 * self.dim(-1) + d2
        if index > self.num_elements():
            raise Error("Out OF range in Store 2D")
        self.data.simd_store[nelts](index, val)

    @always_inline
    fn load[nelts: Int](self, d1: Int, d2: Int, d3: Int) raises -> SIMD[T, nelts]:
        if self.rank() != 3:
            raise Error("Miss match os requested shape (3D Load)")
        let index = d1 * self.dim(-2) * self.dim(-1) + d2 * self.dim(-1) + d3
        if index > self.num_elements():
            raise Error("Out OF range in Load 3D")
        return self.data.simd_load[nelts](index)

    @always_inline
    fn load[nelts: Int](self, d1: Int, d2: Int) raises -> SIMD[T, nelts]:
        if self.rank() != 2:
            raise Error("Miss match os requested shape (2D Load)")
        let index = d1 * self.dim(-1) + d2
        if index > self.num_elements():
            raise Error("Out OF range in Load 2D")
        return self.data.simd_load[nelts](index)

    @always_inline
    fn store[
        nelts: Int, off: Int
    ](self, index: InlinedFixedVector[off, Int], val: SIMD[T, nelts]) -> None:
        let position = self.array_shape.get_1d_pos(index)
        self.assertation(position, self.array_shape._size)
        self.data.simd_store[nelts](position, val)

    @always_inline
    fn store[
        nelts: Int, off: Int
    ](self, index: StaticIntTuple[off], val: SIMD[T, nelts]) -> None:
        let position = self.array_shape.get_1d_pos(index)
        self.assertation(position, self.array_shape._size)
        self.data.simd_store[nelts](position, val)

    @always_inline
    fn store[nelts: Int](self, index: Int, val: SIMD[T, nelts]) -> None:
        """Access the data as a 1D array."""
        let position = self.array_shape.__ntp__(index, self.array_shape._size)
        self.assertation(position, self.array_shape._size)
        self.data.simd_store[nelts](position, val)

    fn assertation(self: Self, i: Int, j: Int) -> None:
        do_check(True if i > j else False, "Index out of range")

    @always_inline
    fn __setitem__(self, index: Int, val: SIMD[T, 1]) -> None:
        return self.store[1](index, val)

    @always_inline
    fn __setitem__[
        off: Int
    ](self, index: InlinedFixedVector[off, Int], val: SIMD[T, 1]):
        return self.store[1](index, val)

    @always_inline
    fn __setitem__[off: Int](self, index: StaticIntTuple[off], val: SIMD[T, 1]):
        return self.store[1](index, val)

    @always_inline
    fn __setitem__(self, d1: Int, d2: Int, d3: Int, val: SIMD[T, 1]) raises -> None:
        return self.store[1](d1, d2, d3, val)

    @always_inline
    fn __setitem__(self, d1: Int, d2: Int, val: SIMD[T, 1]) raises -> None:
        return self.store[1](d1, d2, val)

    @always_inline
    fn __getitem__[off: Int](self, index: InlinedFixedVector[off, Int]) -> SIMD[T, 1]:
        return self.load[1, off](index)

    @always_inline
    fn __getitem__[off: Int](self, index: StaticIntTuple[off]) -> SIMD[T, 1]:
        return self.load[1, off](index)

    @always_inline
    fn __getitem__(self, index: Int) -> SIMD[T, 1]:
        return self.load[1](index)

    @always_inline
    fn __getitem__(self, d1: Int, d2: Int, d3: Int) raises -> SIMD[T, 1]:
        return self.load[1](d1, d2, d3)

    @always_inline
    fn __getitem__(self, d1: Int, d2: Int) raises -> SIMD[T, 1]:
        return self.load[1](d1, d2)

    fn __element_wise_tensor_operation__[
        nelts: Int,
        outer_loop_func: fn[func: fn (Int) capturing -> None] (Int) capturing -> None,
        op_func: fn[T: DType, simd_width: Int] (
            x: SIMD[T, simd_width], y: SIMD[T, simd_width]
        ) -> SIMD[T, simd_width],
    ](self, other: Self) -> Self:
        do_check(
            self.array_shape == other.array_shape,
            "dimension aren't equal, can't do operation element wise.",
        )

        let res = Self(self.array_shape)
        res.alloc()
        let size = self.array_shape.num_elements()

        let last_dim = self.array_shape[-1]
        var dims_rest = size // last_dim

        @parameter
        fn _ol(i: Int):
            @parameter
            fn _iv[nelts: Int](j: Int):
                let index = i * last_dim + j

                res.store[nelts](
                    index,
                    op_func[T, nelts](
                        self.load[nelts](index), other.load[nelts](index)
                    ),
                )

            vectorize[nelts, _iv](last_dim)

        outer_loop_func[_ol](dims_rest)

        return res ^

    fn __element_wise_tensor_operation__[
        nelts: Int,
        outer_loop_func: fn[func: fn (Int) capturing -> None] (Int) capturing -> None,
        op_func: fn[T: DType, simd_width: Int] (x: SIMD[T, simd_width]) -> SIMD[
            T, simd_width
        ],
    ](self) -> Self:
        let res = Self(self.array_shape)
        let size = self.array_shape.num_elements()
        res.alloc()
        let last_dim = self.array_shape[-1]
        var dims_rest = size // last_dim

        @parameter
        fn _ol(i: Int):
            @parameter
            fn _iv[nelts: Int](j: Int):
                let index = i * last_dim + j

                res.store[nelts](
                    index,
                    op_func[T, nelts](self.load[nelts](index)),
                )

            vectorize[nelts, _iv](last_dim)

        outer_loop_func[_ol](dims_rest)

        return res ^

    fn __apply_math__[
        func: fn[type: DType, simd_width: Int] (arg: SIMD[type, simd_width]) -> SIMD[
            type, simd_width
        ],
    ](self: Self) -> Self:
        var res: Self = Self(self.array_shape)
        res.alloc()

        @parameter
        fn _do(size: Int):
            let ra = self.data.offset(size).simd_load[1](0)
            let rs: SIMD[T, 1] = func[T, 1](ra)
            res.data.offset(size).simd_store[1](0, rs)

        parallelize[_do](self.array_shape.num_elements())
        return res ^

    fn __apply_math__[
        nelts: Int,
        func: fn[TP: DType, simd_width: Int] (arg: SIMD[TP, simd_width]) -> SIMD[
            TP, simd_width
        ],
    ](self: Self) -> Self:
        var res: Self = self

        @parameter
        fn _do[_nelts: Int](size: Int):
            let rs: SIMD[T, _nelts] = func[T, _nelts](
                self.data.offset(size).simd_load[_nelts](0)
            )
            res.data.offset(size).simd_store[_nelts](0, rs)

        vectorize[nelts, _do](self.array_shape.num_elements())
        return res ^

    fn __apply_math__[
        func: fn[TP: DType, simd_width: Int] (arg: SIMD[TP, simd_width]) -> SIMD[
            TP, simd_width
        ],
    ](self: Self, num_cores: Int) -> Self:
        var res: Self = self

        @parameter
        fn _do(size: Int):
            let rs: SIMD[T, 1] = func[T, 1](
                self.data.offset(size).simd_load[1](0)
            ).reduce_add()
            res.data.offset(size).simd_store[1](0, rs)

        parallelize[_do](self.array_shape.num_elements(), num_cores)
        return res ^

    fn __add__(self: Self, other: Self) -> Self:
        return self.add[Self.nelts](other)

    fn __mul__(self: Self, other: Self) -> Self:
        return self.mul[Self.nelts](other)

    fn __matmul__(self: Self, other: Self) -> Self:
        return self.matmul[Self.nelts](other)

    fn __sub__(self: Self, other: Self) -> Self:
        return self.sub[Self.nelts](other)

    fn __truediv__(self: Self, other: Self) -> Self:
        return self.true_div[Self.nelts](other)

    fn __mod__(self: Self, other: Self) -> Self:
        return self.mod[Self.nelts](other)

    fn __len__(self: Self) -> Int:
        return self.array_shape.num_elements()

    fn __eq__(self, other: Self) -> Bool:
        return self.eq[Self.nelts](other)

    fn __ne__(self, other: Self) -> Bool:
        return not self.eq[Self.nelts](other)

    fn __floordiv__(self: Self, other: Self) -> Self:
        ...

    fn __pow__(self: Self, other: Self) -> Self:
        return self.pow[Self.nelts](other)

    fn __iadd__(inout self: Self, other: Self) -> None:
        self = self.add[Self.nelts](other)

    fn __isub__(inout self: Self, other: Self) -> None:
        self = self.sub[Self.nelts](other)

    fn __ipow__(inout self: Self, other: Self) -> None:
        self = self.pow[Self.nelts](other)

    fn __imod__(inout self: Self, other: Self) -> None:
        self = self.mod[Self.nelts](other)

    fn __idiv__(inout self: Self, other: Self) -> None:
        self = self.true_div[Self.nelts](other)

    fn __itruediv__(inout self: Self, other: Self) -> None:
        self = self.true_div[Self.nelts](other)

    fn __ifloordiv__(inout self: Self, other: Self) -> None:
        ...

    fn add[nelts: Int](self: Self, other: Self) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int):
            for i in range(size):
                func(i)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.add](
            other
        )

    fn add[nelts: Int](self: Self, other: Self, num_cores: Int) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int) -> None:
            parallelize[func](size, num_cores)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.add](
            other
        )

    fn mod[nelts: Int](self: Self, other: Self) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int):
            for i in range(size):
                func(i)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.mod](
            other
        )

    fn mod[nelts: Int](self: Self, other: Self, num_cores: Int) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int) -> None:
            parallelize[func](size, num_cores)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.mod](
            other
        )

    fn mul[nelts: Int](self: Self, other: Self) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int):
            for i in range(size):
                func(i)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.mul](
            other
        )

    fn mul[nelts: Int](self: Self, other: Self, num_cores: Int) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int) -> None:
            parallelize[func](size, num_cores)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.mul](
            other
        )

    fn sub[nelts: Int](self: Self, other: Self) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int):
            for i in range(size):
                func(i)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.sub](
            other
        )

    fn sub[nelts: Int](self: Self, other: Self, num_cores: Int) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int) -> None:
            parallelize[func](size, num_cores)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.sub](
            other
        )

    fn true_div[nelts: Int](self: Self, other: Self) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int):
            for i in range(size):
                func(i)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.div](
            other
        )

    fn true_div[nelts: Int](self: Self, other: Self, num_cores: Int) -> Self:
        @parameter
        fn outer_loop[func: fn (Int) capturing -> None](size: Int) -> None:
            parallelize[func](size, num_cores)

        return self.__element_wise_tensor_operation__[nelts, outer_loop, math.div](
            other
        )

    fn eq[nelts: Int](self, other: Self) -> Bool:
        do_check(
            self.array_shape == other.array_shape,
            "dimension aren't equal can't performe eq operation.",
        )

        var flag = True
        let size = self.array_shape.num_elements()

        @parameter
        fn iterate_vectorize[nelts: Int](i: Int):
            if self.load[nelts](i) != other.load[nelts](i):
                flag = False

        vectorize[nelts, iterate_vectorize](size)

        return flag

    fn eq[nelts: Int](self, other: Self, num_cores: Int, n_cores: Int) -> Bool:
        do_check(
            self.array_shape == other.array_shape,
            "dimension aren't equal can't performe eq operation.",
        )

        var flag = True
        let size = self.array_shape.num_elements()

        let first_dim = self.array_shape[0]
        let dims_rest = size // first_dim  # the rest of the dimensions

        @parameter
        fn iterate_parallel(i: Int):
            @parameter
            fn iterate_vectorize[nelts: Int](j: Int):
                let index = i * dims_rest + j

                if self.load[nelts](index) != other.load[nelts](index):
                    flag = False

            vectorize[nelts, iterate_vectorize](dims_rest)

        parallelize[iterate_parallel](first_dim, n_cores)

        return flag

    fn pow[nelts: Int](self: Self, other: Self, num_cores: Int) -> Self:
        do_check(
            self.array_shape.num_elements() == other.array_shape.num_elements(),
            "Arrays Don't have same size",
        )
        var res = Self(self.array_shape)
        res.alloc()
        let last_dim: Int = res.array_shape.dim(-1)
        let res_size: Int = res.array_shape._size // last_dim

        @parameter
        fn _ol(i: Int):
            @parameter
            fn _lp[_nelts: Int](j: Int):
                let index: Int = i * last_dim + j
                res.data.simd_store[_nelts](
                    index,
                    self.data.simd_load[_nelts](index)
                    ** res.data.simd_load[_nelts](index),
                )

            vectorize[nelts, _lp](last_dim)

        parallelize[_ol](res_size, num_cores)
        return res ^

    fn pow[nelts: Int](self: Self, other: Self) -> Self:
        do_check(
            self.array_shape.num_elements() == other.array_shape.num_elements(),
            "Arrays Don't have same size",
        )
        var res = Self(self.array_shape)
        res.alloc()
        let last_dim: Int = res.array_shape.dim(-1)
        let res_size: Int = res.array_shape._size // last_dim

        @parameter
        fn _ol(i: Int):
            @parameter
            fn _lp[_nelts: Int](j: Int):
                let index: Int = i * last_dim + j
                res.data.simd_store[_nelts](
                    index,
                    self.data.simd_load[_nelts](index)
                    ** res.data.simd_load[_nelts](index),
                )

            vectorize[nelts, _lp](last_dim)

        for i in range(res_size):
            _ol(i)
        return res ^

    fn dot[nelts: Int](self: Self, other: Self) -> Self:
        do_check(
            self.array_shape == other.array_shape,
            "dimensions aren't the same",
        )
        do_check(
            self.array_shape.rank() == 1 and other.array_shape.rank() == 1,
            "only 1D arrays are accepted",
        )
        var shp = DynamicVector[Int]()
        for s in range(self.array_shape._length):
            shp.push_back(self.array_shape._shape[s])
        var res = Self(ArrayShape(shp))

        @parameter
        fn _do[_nelts: Int](size: Int):
            res.data.simd_store[_nelts](
                size,
                res[size]
                + (
                    self.data.simd_load[_nelts](size)
                    * other.data.simd_load[_nelts](size)
                ).reduce_add(),
            )

        vectorize[nelts, _do](self.array_shape.num_elements())
        return res ^

    @staticmethod
    fn tile[
        tiled_fn: Static2DTileUnitFunc, tile_x: Int, tile_y: Int
    ](end_x: Int, end_y: Int):
        for y in range(0, end_y, tile_y):
            for x in range(0, end_x, tile_x):
                tiled_fn[tile_x, tile_y](x, y)

    fn __matmul_ot[
        nelts: Int,
        outer_loop_func: fn[func: fn (Int) capturing -> None] (Int) capturing -> None,
    ](self, other: Self) -> Self:
        if not (self.rank() == 2 and other.rank() == 1):
            print("Report Matmul Bug")

        var res = Self(self.dim(0))
        res.fill(0.0)

        @parameter
        fn _loop(i: Int):
            var ar: SIMD[T, nelts] = SIMD[T, nelts](0.0)

            @parameter
            fn element_wise[_nelts: Int](j: Int):
                let a_i = i * self.dim(-1) + j
                if _nelts < nelts:
                    ar[0] += (self.load[1](a_i) * other.load[_nelts](j)).reduce_add()
                else:
                    ar += self.load[nelts](a_i) * other.load[nelts](j)

            vectorize[nelts, element_wise](self.dim(-1))
            res[i] = ar.reduce_add()

        outer_loop_func[_loop](self.dim(-2))
        return res ^

    fn __matmul[
        nelts: Int,
        outer_loop_func: fn[func: fn (Int) capturing -> None] (Int) capturing -> None,
    ](self, other: Self) -> Self:
        if self.array_shape.rank() == 1 and other.array_shape.rank() == 1:
            return self.dot[nelts](other)
        if self.rank() == 2 and other.rank() == 1:
            return self.__matmul_ot[nelts, outer_loop_func](other)
        do_check(
            not self.array_shape.__eq_matmul__(other.array_shape),
            "dimensions don't conform for matmul operation",
        )

        var res_dims = InlinedFixedVector[dims_average_size, Int](self.rank())
        for i in range(self.rank() - 1):
            res_dims.append(self.dim(i))
        res_dims.append(other.dim(-1))

        var C = Self(ArrayShape(res_dims))
        C.fill(0.0)

        C.fill(0.0)
        if self.rank() != other.rank():
            print("Can not performe operation Dimensions won't match")
            return C ^
        let C_C: Int = C.dim(-1)
        let A_C: Int = self.dim(-1)
        let C_R: Int = C.dim(-2)
        let C_P: Int = C.dim(-2) * C.dim(-1)
        let B_P: Int = other.dim(-2) * other.dim(-1)
        let A_P: Int = self.dim(-2) * self.dim(-1)

        @parameter
        fn CI(y: Int, x: Int) -> Int:
            return y * C.dim(-1) + x

        @parameter
        fn AI(y: Int, x: Int) -> Int:
            return y * self.dim(-1) + x

        @parameter
        fn BI(y: Int, x: Int) -> Int:
            return y * other.dim(-1) + x

        for s in range((C.num_elements() // (C_C * C_R))):
            let pad_ci = s * C_P
            let pad_ai = s * A_P
            let pad_bi = s * B_P

            @parameter
            fn loop_(i: Int) -> None:
                for j in range(A_C):

                    @parameter
                    fn _mul[_nelts: Int](k: Int) -> None:
                        let ci: Int = CI(i, k) + pad_ci
                        let ai: Int = AI(i, j) + pad_ai
                        let bi: Int = BI(j, k) + pad_bi
                        C.store[_nelts](
                            ci, C.load[_nelts](ci) + self[ai] * other.load[_nelts](bi)
                        )

                    vectorize[nelts, _mul](C_C)

            outer_loop_func[loop_](C_R)
        return C ^

    @always_inline
    fn matmul[nelts: Int](self, other: Self, num_cores: Int, n_cores: Int) -> Self:
        @parameter
        fn matmul_p[outer_loop: fn (Int) capturing -> None](range_size: Int):
            parallelize[outer_loop](range_size, n_cores)

        let result_array = self.__matmul[nelts, matmul_p](other)

        return result_array

    @always_inline
    fn matmul[nelts: Int](self, other: Self) -> Self:
        @parameter
        fn matmul_v[outer_loop: fn (Int) capturing -> None](range_size: Int):
            for i in range(range_size):
                outer_loop(i)

        let result_array: Self = self.__matmul[nelts, matmul_v](other)
        # if debuging:
        #     for i in range(result_array.num_elements()):
        #         print("TRIGGERING RES MTML : ", i, " AS ", result_array[i])
        return result_array

    # cos

    fn cos(inout self: Self) -> Self:
        return self.__apply_math__[cos]()

    fn cos[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, cos]()

    fn cos(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[cos](num_cores)

    # sin

    fn sin(inout self: Self) -> Self:
        return self.__apply_math__[sin]()

    fn sin[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, sin]()

    fn sin(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[sin](num_cores)

    # log

    fn log(inout self: Self) -> Self:
        return self.__apply_math__[log]()

    fn log[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, log]()

    fn log(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[log](num_cores)

    # log2

    fn log2(inout self: Self) -> Self:
        return self.__apply_math__[log2]()

    fn log2[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, log2]()

    fn log2(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[log2](num_cores)

    # tan

    fn tan(inout self: Self) -> Self:
        return self.__apply_math__[tan]()

    fn tan[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, tan]()

    fn tan(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[tan](num_cores)

    # tanh

    fn tanh(inout self: Self) -> Self:
        return self.__apply_math__[tanh]()

    fn tanh[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, tanh]()

    fn tanh(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[tanh](num_cores)

    # sqrt

    # fn sqrt(inout self: Self) -> Self:
    #     return self.__apply_math__[sqrt]()

    # fn sqrt[nelts: Int](inout self: Self) -> Self:
    #     return self.__apply_math__[nelts, sqrt]()

    # fn sqrt(inout self: Self, num_cores: Int) -> Self:
    #     return self.__apply_math__[sqrt](num_cores)

    # atan

    fn atan(inout self: Self) -> Self:
        return self.__apply_math__[atan]()

    fn atan[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, atan]()

    fn atan(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[atan](num_cores)

    # exp

    fn exp(inout self: Self) -> Self:
        return self.__apply_math__[exp]()

    fn exp[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, exp]()

    fn exp(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[exp](num_cores)

    # exp2

    fn exp2(inout self: Self) -> Self:
        return self.__apply_math__[exp2]()

    fn exp2[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, exp2]()

    fn exp2(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[exp2](num_cores)

    # log10
    fn log10(inout self: Self) -> Self:
        return self.__apply_math__[log10]()

    fn log10[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, log10]()

    fn log10(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[log10](num_cores)

    # log1p

    fn log1p(inout self: Self) -> Self:
        return self.__apply_math__[log1p]()

    fn log1p[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, log1p]()

    fn log1p(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[log1p](num_cores)

    # logb

    fn logb(inout self: Self) -> Self:
        return self.__apply_math__[logb]()

    fn logb[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, logb]()

    fn logb(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[logb](num_cores)

    # asin

    fn asin(inout self: Self) -> Self:
        return self.__apply_math__[asin]()

    fn asin[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, asin]()

    fn asin(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[asin](num_cores)

    # acos

    fn acos(inout self: Self) -> Self:
        return self.__apply_math__[acos]()

    fn acos[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, acos]()

    fn acos(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[acos](num_cores)

    # asinh

    fn asinh(inout self: Self) -> Self:
        return self.__apply_math__[asinh]()

    fn asinh[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, asinh]()

    fn asinh(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[asinh](num_cores)

    # acosh

    fn acosh(inout self: Self) -> Self:
        return self.__apply_math__[acosh]()

    fn acosh[nelts: Int](inout self: Self) -> Self:
        return self.__apply_math__[nelts, acosh]()

    fn acosh(inout self: Self, num_cores: Int) -> Self:
        return self.__apply_math__[acosh](num_cores)

    fn fill(inout self: Self, val: SIMD[T, 1]) -> None:
        for i in range(self.num_elements()):
            self.data.store(i, val)

    fn print_array(self):
        let size = self.array_shape.num_elements()

        var product = InlinedFixedVector[dims_average_size, Int](
            self.array_shape.rank() + 1
        )
        product.append(1)
        for index in range(self.array_shape.rank()):
            product.append(
                product[index] * self.array_shape[self.array_shape.rank() - 1 - index]
            )

        var count = 0
        for i in range(size + 1):
            let r_dt = self[i]
            count = 0
            for j in range(self.array_shape.rank()):
                if i % product[j + 1] == 0 and i != 0:
                    print_no_newline("]")
                    count += 1

            if i > 0 and i < size:
                print_no_newline(",")

            if i < size:
                for i in range(count):
                    print()
                    for i in range(self.array_shape.rank() - 1):
                        print_no_newline(" ")

            for j in range(self.array_shape.rank()):
                if i % product[j + 1] == 0 and i != size:
                    print_no_newline("[")

            if i < size:
                print_no_newline(r_dt)

        print()
