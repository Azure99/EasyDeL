from python import PythonObject
from .array_module import ArrayShape, Array, dims_average_size
from algorithm.functional import (
    vectorize,
    parallelize,
    vectorize_unroll,
    Static2DTileUnitFunc as Tile2D,
)
from math import min,max


@always_inline
fn convert_numpy_to_easydel_array[
    T: DType
](np_array: PythonObject, array_spec: ArrayShape) raises -> Array[T]:
    let size: Int = array_spec.num_elements()
    var dynamic_vector = DynamicVector[FloatLiteral](size)
    dynamic_vector.reserve(size)
    try:
        for i in range(size):
            if size == 1:
                dynamic_vector.push_back(np_array.__index__())
            else:
                dynamic_vector.push_back(np_array.reshape(-1)[i].__index__())
    except:
        print("couldn't make it")
        return Array[T](array_spec)
    return Array[T](dynamic_vector, array_spec)


@always_inline
fn matmul_2d[
    T: DType, nelts: Int
](C: Array[T], A: Array[T], B: Array[T], num_cores: Int = 1) -> None:
    @parameter
    fn CI(y: Int, x: Int) -> Int:
        return y * C.dim(-1) + x

    @parameter
    fn AI(y: Int, x: Int) -> Int:
        return y * A.dim(-1) + x

    @parameter
    fn BI(y: Int, x: Int) -> Int:
        return y * B.dim(-1) + x

    @parameter
    fn loop_(i: Int) -> None:
        for j in range(A.dim(-1)):

            @parameter
            fn _mul[_nelts: Int](k: Int) -> None:
                let ci: Int = CI(i, k)
                let ai: Int = AI(i, j)
                let bi: Int = BI(j, k)

                C.store[_nelts](ci, C.load[_nelts](ci) + A[ai] * B.load[_nelts](bi))

            vectorize[nelts, _mul](C.dim(-1))

    parallelize[loop_](C.dim(-2), num_cores)


@always_inline
fn matmul_EA[
    T: DType, nelts: Int
](C: Array[T], A: Array[T], B: Array[T], num_cores: Int = 1) -> None:
    if A.rank() == 2 and B.rank() == 1 and (C.rank() == 1 or C.rank() == 2):
        matmul_single_row[T, nelts](C, A, B, num_cores)
        return
    if A.rank() != B.rank():
        print("Can not performe operation Dimensions won't match")
        return
    let C_C: Int = C.dim(-1)
    let A_C: Int = A.dim(-1)
    let C_R: Int = C.dim(-2)
    let C_P: Int = C.dim(-2) * C.dim(-1)
    let B_P: Int = B.dim(-2) * B.dim(-1)
    let A_P: Int = A.dim(-2) * A.dim(-1)

    @parameter
    fn CI(y: Int, x: Int) -> Int:
        return y * C.dim(-1) + x

    @parameter
    fn AI(y: Int, x: Int) -> Int:
        return y * A.dim(-1) + x

    @parameter
    fn BI(y: Int, x: Int) -> Int:
        return y * B.dim(-1) + x

    for s in range((C.num_elements() // (C_C * C_R))):
        let pad_ci = s * C_P
        let pad_ai = s * A_P
        let pad_bi = s * B_P

        @parameter
        fn loop_(i: Int) -> None:
            for j in range(A_C):

                @parameter
                fn _mul[_nelts: Int](k: Int) -> None:
                    let ci: Int = CI(i, k) + pad_ci
                    let ai: Int = AI(i, j) + pad_ai
                    let bi: Int = BI(j, k) + pad_bi
                    C.store[_nelts](ci, C.load[_nelts](ci) + A[ai] * B.load[_nelts](bi))

                vectorize[nelts, _mul](C_C)

        parallelize[loop_](C_R, num_cores)


@always_inline
fn matmul_shape[T: DType](A: Array[T], B: Array[T]) -> ArrayShape:
    if A.rank() == 2 and B.rank() == 1:
        return ArrayShape(A.dim(0))
    else:
        var res_dims = InlinedFixedVector[dims_average_size, Int](A.rank())
        for i in range(A.rank() - 1):
            res_dims.append(A.dim(i))
        res_dims.append(B.dim(-1))

        return ArrayShape(res_dims)


@always_inline
fn matmul_single_row[
    T: DType, nelts: Int
](C: Array[T], A: Array[T], B: Array[T], num_cores: Int = 1):
    if not (A.rank() == 2 and B.rank() == 1 and (C.rank() == 1 or C.rank() == 2)):
        print("Report Matmul Bug in matmul_single_row")
    let A_C: Int = A.dim(-1)

    @parameter
    fn _loop(i: Int):
        var tensor = SIMD[T, nelts](0.0)

        @parameter
        fn dot[_nelts: Int](j: Int):
            if _nelts < nelts:
                tensor[0] += (A.load[_nelts](i, j) * B.load[_nelts](j)).reduce_add()
            else:
                tensor += A.load[nelts](i, j) * B.load[nelts](j)

        vectorize[nelts, dot](A_C)
        C[i] = tensor.reduce_add()

    parallelize[_loop](A.dim(-2), num_cores)


@always_inline
fn matmul_single_row[
    T: DType, nelts: Int
](
    C: DTypePointer[T],
    A: DTypePointer[T],
    B: DTypePointer[T],
    AC: Int,
    AR: Int,
    num_cores: Int = 1,
):
    @parameter
    fn _loop(i: Int):
        var tensor = SIMD[T, nelts](0)

        @parameter
        fn dot[_nelts: Int](j: Int):
            if _nelts < nelts:
                tensor[0] += (
                    A.simd_load[_nelts](i * AC + j) * B.simd_load[_nelts](j)
                ).reduce_add()
            else:
                tensor += A.simd_load[nelts](i * AC + j) * B.simd_load[nelts](j)

        vectorize[nelts, dot](AC)
        C.store(i, tensor.reduce_add())

    parallelize[_loop](AR, num_cores)


fn T_AXIS_1_0_2[T: DType, nelts: Int](array: Array[T], num_workers: Int) -> Array[T]:
    """
    Transposes a 3D array in axis 0 and 1.
    """

    var transposed_array = Array[T](array.dim(1), array.dim(0), array.dim(2))
    transposed_array.alloc(0.0)
    for i in range(array.dim(0)):

        @parameter
        fn _row(j: Int):
            @parameter
            fn _cols[_nelts: Int](k: Int):
                let dt: SIMD[T, _nelts] = array.load[_nelts](i, j, k)
                transposed_array.store[_nelts](j, i, k, dt)

            vectorize[nelts, _cols](array.dim(2))

        parallelize[_row](array.dim(1), num_workers)
    return transposed_array


fn T_AXIS_0_2_1[T: DType, nelts: Int](array: Array[T], num_workers: Int) -> Array[T]:
    """
    Transposes an array in axis 2 and 1.
    """

    var transposed_array = Array[T](array.dim(0), array.dim(2), array.dim(1))
    transposed_array.alloc(0.0)
    for i in range(array.dim(0)):

        @parameter
        fn _row(j: Int):
            @parameter
            fn _cols[_nelts: Int](k: Int):
                let dt: SIMD[T, _nelts] = array.load[_nelts](i, j, k)
                transposed_array.store[_nelts](i, k, j, dt)

            vectorize[nelts, _cols](array.dim(2))

        parallelize[_row](array.dim(1), num_workers)
    return transposed_array


fn T_AXIS_2_1_0[T: DType, nelts: Int](array: Array[T], num_workers: Int) -> Array[T]:
    """
    Transposes an array in axis 0 and 2.
    """

    var transposed_array = Array[T](array.dim(2), array.dim(1), array.dim(0))
    transposed_array.alloc(0.0)
    for i in range(array.dim(0)):

        @parameter
        fn _row(j: Int):
            @parameter
            fn _cols[_nelts: Int](k: Int):
                let dt: SIMD[T, _nelts] = array.load[_nelts](i, j, k)
                transposed_array.store[_nelts](k, j, i, dt)

            vectorize[nelts, _cols](array.dim(2))

        parallelize[_row](array.dim(1), num_workers)
    return transposed_array


fn T_2D[T: DType, nelts: Int](array: Array[T], num_workers: Int) -> Array[T]:
    """
    Transposes an array.
    """

    var transposed_array = Array[T](array.dim(1), array.dim(0))
    transposed_array.alloc(0.0)

    @parameter
    fn _row(i: Int):
        @parameter
        fn _cols[_nelts: Int](j: Int):
            let dt: SIMD[T, _nelts] = array.load[_nelts](i, j)
            transposed_array.store[_nelts](j, i, dt)

        vectorize[nelts, _cols](array.dim(1))

    parallelize[_row](array.dim(0), num_workers)
    return transposed_array


fn concatenate[
    T: DType
](A: Array[T], B: Array[T], inout axis: Int = 0, stragedy: String = "ord") -> Array[T]:
    debug_assert(A.rank() == B.rank(), "A,B Shapes Wont Match for concatenate")
    if axis < 0:
        axis = A.rank() - axis
    var cat_shape: DynamicVector[Int] = DynamicVector[Int]()
    for i in range(A.rank()):
        if i != axis:
            debug_assert(A.dim(i) == B.dim(i), "A,B Shapes Wont Match for concatenate")
            cat_shape.push_back(A.dim(i))
        else:
            cat_shape.push_back(A.dim(i) + B.dim(i))
    var cat_array: Array[T] = Array[T](cat_shape)
    cat_array.alloc(0.0)
    if stragedy == "ord":

        @parameter
        fn _r[nelts: Int](I: Int):
            if I < A.num_elements():
                cat_array.store[nelts](I, A.load[nelts](I))
            else:
                cat_array.store[nelts](I, B.load[nelts](I))

        vectorize[Array[T].nelts, _r](cat_array.num_elements())
    return cat_array


fn CAT_3D_AXIS_2[T: DType, cores: Int](A: Array[T], B: Array[T]) -> Array[T]:
    """
    Return Concatenated Array At axis -1.
    """
    var x: Array[T] = Array[T](A.dim(0), A.dim(1), A.dim(-1) + B.dim(-1))
    let A_A: Int = A.dim(-1)
    x.alloc(0.0)
    for i in range(A.dim(0)):

        @parameter
        fn _row(j: Int):
            @parameter
            fn _cols[nelts: Int](k: Int):
                if k < A_A:
                    x.store[nelts](i, j, k, A.load[nelts](i, j, k))
                else:
                    x.store[nelts](i, j, k, B.load[nelts](i, j, k - A_A))

            vectorize[Array[T].nelts, _cols](x.dim(-1))

        parallelize[_row](A.dim(1), cores)
    return x


fn rotate_half[T: DType, cores: Int](A: Array[T]) -> Array[T]:
    """
    Let make it More Pythonic.
    """
    debug_assert(A.rank() == 3, "Only 3D Arrays Are accepted")
    let half: Int = A.dim(-1) // 2
    let rest: Int = A.dim(-1) - half

    var x1: Array[T] = Array[T](A.dim(0), A.dim(1), half)
    var x2: Array[T] = Array[T](A.dim(0), A.dim(1), rest)

    x1.alloc(0.0)
    x2.alloc(0.0)
    for i in range(A.dim(0)):
        # X1 Rotation
        @parameter
        fn x1_row(j: Int):
            @parameter
            fn x1_cols[nelts: Int](k: Int):
                x1.store[nelts](i, j, k, A.load[nelts](i, j, k))

            vectorize[Array[T].nelts, x1_cols](half)

        parallelize[x1_row](A.dim(1), cores)

        # X2 Rotation
        @parameter
        fn x2_row(j: Int):
            @parameter
            fn x2_cols[nelts: Int](k: Int):
                x2.store[nelts](i, j, k, -A.load[nelts](i, j, half + k))

            vectorize[Array[T].nelts, x2_cols](rest)

        parallelize[x2_row](A.dim(1), cores)

    var cated: Array[T] = CAT_3D_AXIS_2[T, cores](x2, x1)
    return cated


@always_inline
fn arange[T: DType, cores: Int](start: Int, end: Int) -> Array[T]:
    let rng = end - start
    var array: Array[T] = Array[T](rng)
    array.alloc()

    @parameter
    fn _row(i: Int):
        array.store[1](i, start + i)

    parallelize[_row](rng, cores)
    return array

## I Used https://github.com/TilliFe/Infermo/blob/main/infermo/operators/ to learn this Algorithms

fn shape_a[T:DType](depth: Int, a: Array[T], b: Array[T]) -> Int:
    let diff = max(b.rank() - a.rank(),0)
    if(depth < diff):
        return 1
    return a.shape[depth - diff]

fn shape_b[T:DType](depth: Int, a: Array[T], b: Array[T]) -> Int:
    let diff = max(a.rank() - b.rank(),0)
    if(depth < diff):
        return 1
    return b.shape[depth - diff]

fn recursive_broadcast[
    T:DType,
    nelts:Int,
    cores:Int,
    kernel: fn[T:DType,nelts:Int,cores:Int](inout c: Array[T],a: Array[T],b: Array[T],a_index: Int,b_index: Int, c_index: Int, depth: Int ) -> None,
    base_case: fn[T:DType](depth: Int, a: Array[T], b: Array[T]) -> Bool 
](
    inout c: Array[T], 
    a: Array[T], 
    b: Array[T], 
    a_index: Int=0, 
    b_index: Int=0, 
    c_index: Int=0, 
    depth: Int=0
):

    if(base_case[T](depth,a,b)):
        kernel[T,nelts,cores](c,a,b,a_index,b_index,c_index,depth)
        return

    let a_shape = shape_a(depth,a,b)
    let b_shape = shape_b(depth,a,b)
    let c_shape = c.shape[depth]
    if(a_shape != 1 and b_shape == 1):
        for s in range(a_shape):
            recursive_broadcast[T,nelts,cores,kernel,base_case](
                c,a,b,
                a_shape*a_index + s, 
                b_shape*b_index,
                c_shape*c_index + s, 
                depth+1
            )
    elif(a_shape == 1 and b_shape != 1):
        for s in range(b_shape):
            recursive_broadcast[T,nelts,cores,kernel,base_case](
                c,a,b,
                a_shape*a_index, 
                b_shape*b_index + s, 
                c_shape*c_index + s,
                depth+1
            )
    else:
        for s in range(a_shape):
            recursive_broadcast[T,nelts,cores,kernel,base_case](
                c,a,b,
                a_shape*a_index + s, 
                b_shape*b_index + s, 
                c_shape*c_index + s,
                depth+1
            )


@parameter
fn base_case_matmul[T:DType](depth: Int, a: Array[T], b: Array[T]) -> Bool:
    return depth == max(a.rank(),b.rank())-2

@parameter
fn kernel_matmul[T:DType,nelts:Int,cores:Int](inout c: Array[T], a: Array[T], b: Array[T], a_index: Int, b_index: Int, c_index: Int, depth: Int) -> None:
 
    let offset_a = a_index * a.shape[a.rank()-2] * a.shape[a.rank()-1]
    let offset_b = b_index * b.shape[b.rank()-2] * b.shape[b.rank()-1]
    let offset_c = c_index * c.shape[c.rank()-2] * c.shape[c.rank()-1] 

    let M = a.shape[a.rank()-2]
    let K = b.shape[b.rank()-2]
    let N = b.shape[b.rank()-1]

    @parameter
    fn calc_row_fw(m: Int):
        for k in range(K):
            @parameter
            fn dot_fw[nelts: Int](n: Int):
                c.data.simd_store[nelts](
                    offset_c + m*N+n, 
                    c.data.simd_load[nelts](offset_c + m*N+n) + a.data.load(offset_a + m*K+k) * b.data.simd_load[nelts](offset_b + k*N+n))
            vectorize[nelts, dot_fw](N)
    parallelize[calc_row_fw](M, cores if cores > 0 else M) 

@always_inline
fn matmul[T:DType,nelts:Int,cores:Int](inout C: Array[T], A: Array[T], B: Array[T]):
    recursive_broadcast[T,nelts,cores,kernel_matmul, base_case_matmul](C,A,B)

@always_inline
fn matmul[T:DType,cores:Int](inout C: Array[T], A: Array[T], B: Array[T]):
    recursive_broadcast[T,Array[T].nelts,cores,kernel_matmul, base_case_matmul](C,A,B)

@always_inline
fn matmul[T:DType](inout C: Array[T], A: Array[T], B: Array[T]):
    recursive_broadcast[T,Array[T].nelts,1,kernel_matmul, base_case_matmul](C,A,B)

@always_inline
fn matmul(inout C: Array[DType.float32], A: Array[DType.float32], B: Array[DType.float32]):
    recursive_broadcast[DType.float32,Array[DType.float32].nelts,1,kernel_matmul, base_case_matmul](C,A,B)

@always_inline
fn matmul(inout C: Array[DType.float16], A: Array[DType.float16], B: Array[DType.float16]):
    recursive_broadcast[DType.float16,Array[DType.float16].nelts,1,kernel_matmul, base_case_matmul](C,A,B)

@always_inline
fn matmul(inout C: Array[DType.bfloat16], A: Array[DType.bfloat16], B: Array[DType.bfloat16]):
    recursive_broadcast[DType.bfloat16,Array[DType.bfloat16].nelts,1,kernel_matmul, base_case_matmul](C,A,B)

@always_inline
fn matmul[cores:Int](inout C: Array[DType.float32], A: Array[DType.float32], B: Array[DType.float32]):
    recursive_broadcast[DType.float32,Array[DType.float32].nelts,cores,kernel_matmul, base_case_matmul](C,A,B)
    
@always_inline
fn matmul[cores:Int](inout C: Array[DType.float16], A: Array[DType.float16], B: Array[DType.float16]):
    recursive_broadcast[DType.float16,Array[DType.float16].nelts,cores,kernel_matmul, base_case_matmul](C,A,B)

@always_inline
fn matmul[cores:Int](inout C: Array[DType.bfloat16], A: Array[DType.bfloat16], B: Array[DType.bfloat16]):
    recursive_broadcast[DType.bfloat16,Array[DType.bfloat16].nelts,cores,kernel_matmul, base_case_matmul](C,A,B)
