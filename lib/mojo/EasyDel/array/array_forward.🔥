from python import PythonObject
from .array_module import ArrayShape, Array, dims_average_size
from algorithm.functional import (
    vectorize,
    parallelize,
    vectorize_unroll,
    Static2DTileUnitFunc as Tile2D,
)
from math import (
    max,
    min,
    sqrt,
    abs,
    pow,
    exp2,
    exp,
    log2,
    log,
    cos,
    sin,
    tan,
    asin,
    acos,
    atan,
    cosh,
    sinh,
    tanh,
)
import math
from memory import memset_zero, memcpy
from random import rand


fn a_over_shape[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Int:
    let _df = max(B.rank() - A.rank(), 0)
    if depth < _df:
        return 1
    return A.shape[depth - _df]


fn b_over_shape[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Int:
    let _df = max(A.rank() - B.rank(), 0)
    if depth < _df:
        return 1
    return B.shape[depth - _df]


fn a_over_strides[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Int:
    let _df = max(B.rank() - A.rank(), 0)
    if depth < _df:
        return A.strides[0]
    return A.strides[depth - _df]


fn b_over_strides[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Int:
    let _df = max(A.rank() - B.rank(), 0)
    if depth < _df:
        return B.strides[0]
    return B.strides[depth - _df]


@always_inline
fn convert_numpy_to_easydel_array[
    DT: DType
](np_array: PythonObject, array_spec: ArrayShape) raises -> Array[DT]:
    let size: Int = array_spec.num_elements()
    var dynamic_vector = DynamicVector[FloatLiteral](size)
    dynamic_vector.reserve(size)
    try:
        for i in range(size):
            if size == 1:
                dynamic_vector.push_back(np_array.__index__())
            else:
                dynamic_vector.push_back(np_array.reshape(-1)[i].__index__())
    except:
        print("couldn't make it")
        return Array[DT](array_spec)
    return Array[DT](dynamic_vector, array_spec)


@always_inline
fn matmul_2d[
    DT: DType, nelts: Int
](C: Array[DT], A: Array[DT], B: Array[DT], num_cores: Int = 1) -> None:
    @parameter
    fn CI(y: Int, x: Int) -> Int:
        return y * C.dim(-1) + x

    @parameter
    fn AI(y: Int, x: Int) -> Int:
        return y * A.dim(-1) + x

    @parameter
    fn BI(y: Int, x: Int) -> Int:
        return y * B.dim(-1) + x

    @parameter
    fn loop_(i: Int) -> None:
        for j in range(A.dim(-1)):

            @parameter
            fn _mul[_nelts: Int](k: Int) -> None:
                let ci: Int = CI(i, k)
                let ai: Int = AI(i, j)
                let bi: Int = BI(j, k)

                C.store[_nelts](ci, C.load[_nelts](ci) + A[ai] * B.load[_nelts](bi))

            vectorize[nelts, _mul](C.dim(-1))

    parallelize[loop_](C.dim(-2), num_cores)


@always_inline
fn matmul_3D[
    DT: DType, nelts: Int
](C: Array[DT], A: Array[DT], B: Array[DT], num_cores: Int = 1) -> None:
    if A.rank() == Int(2) and B.rank() == 1 and (C.rank() == 1 or C.rank() == 2):
        matmul_single_row[DT, nelts](C, A, B, num_cores)
        return
    if A.rank() != B.rank():
        print("Can not performe operation Dimensions won't match")
        return
    let C_C: Int = C.dim(-1)
    let A_C: Int = A.dim(-1)
    let C_R: Int = C.dim(-2)
    let C_P: Int = C.dim(-2) * C.dim(-1)
    let B_P: Int = B.dim(-2) * B.dim(-1)
    let A_P: Int = A.dim(-2) * A.dim(-1)

    @parameter
    fn CI(y: Int, x: Int) -> Int:
        return y * C.dim(-1) + x

    @parameter
    fn AI(y: Int, x: Int) -> Int:
        return y * A.dim(-1) + x

    @parameter
    fn BI(y: Int, x: Int) -> Int:
        return y * B.dim(-1) + x

    for s in range((C.num_elements() // (C_C * C_R))):
        let pad_ci = s * C_P
        let pad_ai = s * A_P
        let pad_bi = s * B_P

        @parameter
        fn loop_(i: Int) -> None:
            for j in range(A_C):

                @parameter
                fn _mul[_nelts: Int](k: Int) -> None:
                    let ci: Int = CI(i, k) + pad_ci
                    let ai: Int = AI(i, j) + pad_ai
                    let bi: Int = BI(j, k) + pad_bi
                    C.store[_nelts](ci, C.load[_nelts](ci) + A[ai] * B.load[_nelts](bi))

                vectorize[nelts, _mul](C_C)

        parallelize[loop_](C_R, num_cores)


@always_inline
fn matmul_single_row[
    DT: DType, nelts: Int
](C: Array[DT], A: Array[DT], B: Array[DT], num_cores: Int = 1):
    if not (A.rank() == Int(2) and B.rank() == 1 and (C.rank() == 1 or C.rank() == 2)):
        print("Report Matmul Bug in matmul_single_row")
    let A_C: Int = A.dim(-1)

    @parameter
    fn _loop(i: Int):
        var carry = SIMD[DT, nelts](0.0)

        @parameter
        fn dot[_nelts: Int](j: Int):
            if _nelts < nelts:
                carry[0] += (A.load[_nelts](i, j) * B.load[_nelts](j)).reduce_add()
            else:
                carry += A.load[nelts](i, j) * B.load[nelts](j)

        vectorize[nelts, dot](A_C)
        C[i] = carry.reduce_add()

    parallelize[_loop](A.dim(-2), num_cores)


@always_inline
fn matmul_single_row[
    DT: DType, nelts: Int
](
    C: DTypePointer[DT],
    A: DTypePointer[DT],
    B: DTypePointer[DT],
    AC: Int,
    AR: Int,
    num_cores: Int = 1,
):
    @parameter
    fn _loop(i: Int):
        var carry = SIMD[DT, nelts](0)

        @parameter
        fn dot[_nelts: Int](j: Int):
            if _nelts < nelts:
                carry[0] += (
                    A.simd_load[_nelts](i * AC + j) * B.simd_load[_nelts](j)
                ).reduce_add()
            else:
                carry += A.simd_load[nelts](i * AC + j) * B.simd_load[nelts](j)

        vectorize[nelts, dot](AC)
        C.store(i, carry.reduce_add())

    parallelize[_loop](AR, num_cores)


@always_inline
fn T_AXIS_1_0_2[DT: DType, nelts: Int](array: Array[DT], num_workers: Int) -> Array[DT]:
    """
    Transposes A 3D array in axis 0 and 1.
    """

    var transposed_array = Array[DT](array.dim(1), array.dim(0), array.dim(2))
    transposed_array.alloc(0.0)
    for i in range(array.dim(0)):

        @parameter
        fn _row(j: Int):
            @parameter
            fn _cols[_nelts: Int](k: Int):
                let dt: SIMD[DT, _nelts] = array.load[_nelts](i, j, k)
                transposed_array.store[_nelts](VariadicList[Int](j, i, k), dt)

            vectorize[nelts, _cols](array.dim(2))

        parallelize[_row](array.dim(1), num_workers)
    return transposed_array


@always_inline
fn T_AXIS_0_2_1[DT: DType, nelts: Int](array: Array[DT], num_workers: Int) -> Array[DT]:
    """
    Transposes an array in axis 2 and 1.
    """

    var transposed_array = Array[DT](array.dim(0), array.dim(2), array.dim(1))
    transposed_array.alloc(0.0)
    for i in range(array.dim(0)):

        @parameter
        fn _row(j: Int):
            @parameter
            fn _cols[_nelts: Int](k: Int):
                let dt: SIMD[DT, _nelts] = array.load[_nelts](i, j, k)
                transposed_array.store[_nelts](VariadicList[Int](i, k, j), dt)

            vectorize[nelts, _cols](array.dim(2))

        parallelize[_row](array.dim(1), num_workers)
    return transposed_array


@always_inline
fn T_AXIS_2_1_0[DT: DType, nelts: Int](array: Array[DT], num_workers: Int) -> Array[DT]:
    """
    Transposes an array in axis 0 and 2.
    """

    var transposed_array = Array[DT](array.dim(2), array.dim(1), array.dim(0))
    transposed_array.alloc(0.0)
    for i in range(array.dim(0)):

        @parameter
        fn _row(j: Int):
            @parameter
            fn _cols[_nelts: Int](k: Int):
                let dt: SIMD[DT, _nelts] = array.load[_nelts](i, j, k)
                transposed_array.store[_nelts](VariadicList[Int](k, j, i), dt)

            vectorize[nelts, _cols](array.dim(2))

        parallelize[_row](array.dim(1), num_workers)
    return transposed_array


@always_inline
fn concatenate[
    DT: DType
](A: Array[DT], B: Array[DT], inout axis: Int = 0, stragedy: String = "ord") -> Array[DT]:
    debug_assert(A.rank() == B.rank(), "A,B Shapes Wont Match for concatenate")
    if axis < Int(0):
        axis = A.rank() - axis
    var cat_shape: DynamicVector[Int] = DynamicVector[Int]()
    for i in range(A.rank()):
        if i != axis:
            debug_assert(A.dim(i) == B.dim(i), "A,B Shapes Wont Match for concatenate")
            cat_shape.push_back(A.dim(i))
        else:
            cat_shape.push_back(A.dim(i) + B.dim(i))
    var cat_array: Array[DT] = Array[DT](cat_shape)
    cat_array.alloc(0.0)
    if stragedy == "ord":

        @parameter
        fn _r[nelts: Int](I: Int):
            if I < A.num_elements():
                cat_array.store[nelts](I, A.load[nelts](I))
            else:
                cat_array.store[nelts](I, B.load[nelts](I))

        vectorize[Array[DT].nelts, _r](cat_array.num_elements())
    return cat_array


@always_inline
fn CAT_4D_AXIS_3[DT: DType, nelts:Int,cores: Int](A: Array[DT], B: Array[DT]) -> Array[DT]:
    """
    Return Concatenated Array At axis -1.
    """
    var x: Array[DT] = Array[DT](A.dim(0), A.dim(1), A.dim(2), A.dim(-1) + B.dim(-1))
    let A_A: Int = A.dim(-1)
    x.alloc(0.0)
    for b in range(A.dim(0)):
        for i in range(A.dim(1)):
            @parameter
            fn _row(j: Int):
                @parameter
                fn _cols[_nelts: Int](k: Int):
                    if k < A_A:
                        x.store[_nelts](VariadicList[Int](b,i, j, k), A.load[_nelts](b,i, j, k))
                    else:
                        x.store[_nelts](VariadicList[Int](b,i, j, k), B.load[_nelts](b,i, j, k - A_A))

                vectorize[nelts, _cols](x.dim(-1))

            parallelize[_row](A.dim(2), cores)
    return x


@always_inline
fn rotate_half[DT: DType, nelts:Int,cores: Int](A: Array[DT]) -> Array[DT]:
    """
    Let make it More Pythonic.
    """
    debug_assert(A.rank() == 3, "Only 3D Arrays Are accepted")
    let half: Int = A.dim(-1) // 2
    let rest: Int = A.dim(-1) - half

    var x1: Array[DT] = Array[DT](A.dim(0),A.dim(1), A.dim(2), half)
    var x2: Array[DT] = Array[DT](A.dim(0),A.dim(1), A.dim(2), rest)

    x1.alloc(0.0)
    x2.alloc(0.0)
    for b in range(A.dim(0)):
        for i in range(A.dim(1)):
            # X1 Rotation
            @parameter
            fn x1_row(j: Int):
                @parameter
                fn x1_cols[nelts: Int](k: Int):
                    x1.store[nelts](VariadicList[Int](b,i, j, k), A.load[nelts](b,i, j, k))

                vectorize[Array[DT].nelts, x1_cols](half)

            parallelize[x1_row](A.dim(2), cores)

            # X2 Rotation
            @parameter
            fn x2_row(j: Int):
                @parameter
                fn x2_cols[nelts: Int](k: Int):
                    x2.store[nelts](VariadicList[Int](b,i, j, k), -A.load[nelts](b,i, j, half + k))

                vectorize[Array[DT].nelts, x2_cols](rest)

            parallelize[x2_row](A.dim(2), cores)

    var cated: Array[DT] = CAT_4D_AXIS_3[DT, nelts,cores](x2, x1)
    return cated


@always_inline
fn arange[DT: DType, cores: Int](start: Int, end: Int) -> Array[DT]:
    let rng = end - start
    var array: Array[DT] = Array[DT](rng)
    array.alloc()

    @parameter
    fn _row(i: Int):
        array.store[1](i, start + i)

    parallelize[_row](rng, cores)
    return array


## I Used https://github.com/TilliFe/Infermo/blob/main/infermo/operators/ to learn this Algorithms


fn recursive_broadcast[
    DT: DType,
    nelts: Int,
    cores: Int,
    kernel: fn[DT:DType,nelts:Int,cores:Int](
        inout C: Array[DT],
        A: Array[DT],
        B: Array[DT],
        a_index: Int,
        b_index: Int,
        c_index: Int,
        depth: Int
        ) -> None,
    base_case: fn[DT:DType](depth: Int, A: Array[DT], B: Array[DT]) -> Bool,
    parallelized: Bool,
](
    inout C: Array[DT],
    A: Array[DT],
    B: Array[DT],
    a_index: Int = 0,
    b_index: Int = 0,
    c_index: Int = 0,
    depth: Int = 0,
):
    if base_case[DT](depth, A, B):
        kernel[DT, nelts, cores](C, A, B, a_index, b_index, c_index, depth)
        return

    let a_shape = a_over_shape[DT](depth, A, B)
    let b_shape = b_over_shape[DT](depth, A, B)
    let c_shape = C.shape[depth]
    if a_shape != 1 and b_shape == 1:
        if parallelized:

            @parameter
            fn _cols_a(s: Int):
                recursive_broadcast[
                    DT, nelts, cores, kernel, base_case, parallelized
                ](
                    C,
                    A,
                    B,
                    a_shape * a_index + s,
                    b_shape * b_index,
                    c_shape * c_index + s,
                    depth + 1,
                )

            parallelize[_cols_a](a_shape, cores)

        for s in range(a_shape):
            recursive_broadcast[DT, nelts, cores, kernel, base_case, parallelized](
                C,
                A,
                B,
                a_shape * a_index + s,
                b_shape * b_index,
                c_shape * c_index + s,
                depth + 1,
            )
    elif a_shape == 1 and b_shape != 1:
        if parallelized:

            @parameter
            fn _cols_b(s: Int):
                recursive_broadcast[
                    DT, nelts, cores, kernel, base_case, parallelized
                ](
                    C,
                    A,
                    B,
                    a_shape * a_index,
                    b_shape * b_index + s,
                    c_shape * c_index + s,
                    depth + 1,
                )

            parallelize[_cols_b](b_shape, cores)
        else:
            for s in range(b_shape):
                recursive_broadcast[
                    DT, nelts, cores, kernel, base_case, parallelized
                ](
                    C,
                    A,
                    B,
                    a_shape * a_index,
                    b_shape * b_index + s,
                    c_shape * c_index + s,
                    depth + 1,
                )
    else:
        if parallelized:

            @parameter
            fn _cols_a_s(s: Int):
                recursive_broadcast[
                    DT, nelts, cores, kernel, base_case, parallelized
                ](
                    C,
                    A,
                    B,
                    a_shape * a_index + s,
                    b_shape * b_index + s,
                    c_shape * c_index + s,
                    depth + 1,
                )

            parallelize[_cols_a_s](a_shape, cores)
        else:
            for s in range(a_shape):
                recursive_broadcast[
                    DT, nelts, cores, kernel, base_case, parallelized
                ](
                    C,
                    A,
                    B,
                    a_shape * a_index + s,
                    b_shape * b_index + s,
                    c_shape * c_index + s,
                    depth + 1,
                )


@always_inline
fn kernel_matmul[
    DT: DType, nelts: Int, cores: Int
](
    inout C: Array[DT],
    A: Array[DT],
    B: Array[DT],
    a_index: Int,
    b_index: Int,
    c_index: Int,
    depth: Int,
) -> None:
    let offset_a = a_index * A.shape[A.rank() - 2] * A.shape[A.rank() - 1]
    let offset_b = b_index * B.shape[B.rank() - 2] * B.shape[B.rank() - 1]
    let offset_c = c_index * C.shape[C.rank() - 2] * C.shape[C.rank() - 1]

    let M = A.shape[A.rank() - 2]
    let K = B.shape[B.rank() - 2]
    let N = B.shape[B.rank() - 1]

    @parameter
    fn calc_row(m: Int):
        for k in range(K):

            @parameter
            fn dot[nelts: Int](n: Int):
                C.data.simd_store[nelts](
                    offset_c + m * N + n,
                    C.data.simd_load[nelts](offset_c + m * N + n)
                    + A.data.load(offset_a + m * K + k)
                    * B.data.simd_load[nelts](offset_b + k * N + n),
                )

            vectorize[nelts, dot](N)

    parallelize[calc_row](M, cores if cores > 0 else M)


@always_inline
fn matmul[
    DT: DType, nelts: Int = Array[DT].nelts, cores: Int = 1,parallelized:Bool=True
](inout C: Array[DT], A: Array[DT], B: Array[DT]):
    recursive_broadcast[DT, nelts, cores, kernel_matmul, base_case_matmul,parallelized](C, A, B)


@always_inline
fn matmul(
    inout C: Array[DType.float32], A: Array[DType.float32], B: Array[DType.float32]
):
    recursive_broadcast[
        DType.float32, Array[DType.float32].nelts, 1, kernel_matmul, base_case_matmul,True
    ](C, A, B)


@always_inline
fn matmul(
    inout C: Array[DType.float16], A: Array[DType.float16], B: Array[DType.float16]
):
    recursive_broadcast[
        DType.float16, Array[DType.float16].nelts, 1, kernel_matmul, base_case_matmul,True
    ](C, A, B)


@always_inline
fn matmul(
    inout C: Array[DType.bfloat16], A: Array[DType.bfloat16], B: Array[DType.bfloat16]
):
    recursive_broadcast[
        DType.bfloat16, Array[DType.bfloat16].nelts, 1, kernel_matmul, base_case_matmul,True
    ](C, A, B)


@always_inline
fn matmul[
    cores: Int,parallelized:Bool=True
](inout C: Array[DType.float32], A: Array[DType.float32], B: Array[DType.float32]):
    recursive_broadcast[
        DType.float32,
        Array[DType.float32].nelts,
        cores,
        kernel_matmul,
        base_case_matmul,
        parallelized
    ](C, A, B)


@always_inline
fn matmul[
    cores: Int,parallelized:Bool=True
](inout C: Array[DType.float16], A: Array[DType.float16], B: Array[DType.float16]):
    recursive_broadcast[
        DType.float16,
        Array[DType.float16].nelts,
        cores,
        kernel_matmul,
        base_case_matmul,
        parallelized
    ](C, A, B)


@always_inline
fn matmul[
    cores: Int,parallelized:Bool=True
](inout C: Array[DType.bfloat16], A: Array[DType.bfloat16], B: Array[DType.bfloat16]):
    recursive_broadcast[
        DType.bfloat16,
        Array[DType.bfloat16].nelts,
        cores,
        kernel_matmul,
        base_case_matmul,
        parallelized
    ](C, A, B)


@always_inline
fn matmul[DT: DType, nelts: Int, cores: Int,parallelized:Bool=True](A: Array[DT], B: Array[DT]) -> Array[DT]:
    var C: Array[DT] = Array[DT](A, B)
    recursive_broadcast[DT, nelts, cores, kernel_matmul, base_case_matmul,parallelized](C, A, B)
    return C


@always_inline
fn matmul[DT: DType, cores: Int = 1,parallelized:Bool=True](A: Array[DT], B: Array[DT]) -> Array[DT]:
    var C: Array[DT] = Array[DT](A, B)
    recursive_broadcast[DT, Array[DT].nelts, cores, kernel_matmul, base_case_matmul,parallelized](
        C, A, B
    )
    return C


@always_inline
fn matmul(A: Array[DType.float32], B: Array[DType.float32]) -> Array[DType.float32]:
    var C: Array[DType.float32] = Array[DType.float32](A, B)
    recursive_broadcast[
        DType.float32, Array[DType.float32].nelts, 1, kernel_matmul, base_case_matmul,True
    ](C, A, B)
    return C


@always_inline
fn matmul(A: Array[DType.float16], B: Array[DType.float16]) -> Array[DType.float16]:
    var C: Array[DType.float16] = Array[DType.float16](A, B)
    recursive_broadcast[
        DType.float16, Array[DType.float16].nelts, 1, kernel_matmul, base_case_matmul,True
    ](C, A, B)
    return C


@always_inline
fn matmul(A: Array[DType.bfloat16], B: Array[DType.bfloat16]) -> Array[DType.bfloat16]:
    var C: Array[DType.bfloat16] = Array[DType.bfloat16](A, B)
    recursive_broadcast[
        DType.bfloat16, Array[DType.bfloat16].nelts, 1, kernel_matmul, base_case_matmul,True
    ](C, A, B)
    return C


@always_inline
fn matmul[
    cores: Int,parallelized:Bool=True
](A: Array[DType.float32], B: Array[DType.float32]) -> Array[DType.float32]:
    var C: Array[DType.float32] = Array[DType.float32](A, B)
    recursive_broadcast[
        DType.float32,
        Array[DType.float32].nelts,
        cores,
        kernel_matmul,
        base_case_matmul,
        parallelized
    ](C, A, B)
    return C


@always_inline
fn matmul[
    cores: Int,parallelized:Bool=True
](A: Array[DType.float16], B: Array[DType.float16]) -> Array[DType.float16]:
    var C: Array[DType.float16] = Array[DType.float16](A, B)
    recursive_broadcast[
        DType.float16,
        Array[DType.float16].nelts,
        cores,
        kernel_matmul,
        base_case_matmul,
        parallelized
    ](C, A, B)
    return C


@always_inline
fn matmul[
    cores: Int,parallelized:Bool=True
](A: Array[DType.bfloat16], B: Array[DType.bfloat16]) -> Array[DType.bfloat16]:
    var C: Array[DType.bfloat16] = Array[DType.bfloat16](A, B)
    recursive_broadcast[
        DType.bfloat16,
        Array[DType.bfloat16].nelts,
        cores,
        kernel_matmul,
        base_case_matmul,
        parallelized
    ](C, A, B)
    return C


@always_inline
fn sigmoid[
    DT: DType
](inout x: Array[DT], number_of_cores: Int = 1, size: Int = 0) -> None:
    let num_elements: Int = x.num_elements() if size == 0 else size

    @parameter
    fn _row(size: Int):
        x[size] = 1.0 / (1.0 + math.exp(-x[size]))

    parallelize[_row](num_elements, number_of_cores)


@always_inline
fn silu[DT: DType](inout x: Array[DT], number_of_cores: Int = 1, size: Int = 0) -> None:
    let num_elements: Int = x.num_elements() if size == 0 else size

    @parameter
    fn _row(size: Int):
        let dt: SIMD[DT, 1] = x[size]
        x[size] = dt * (1.0 / (1.0 + math.exp(-dt)))

    parallelize[_row](num_elements, number_of_cores)


@always_inline
fn relu[DT: DType](inout x: Array[DT], number_of_cores: Int = 1, size: Int = 0) -> None:
    let num_elements: Int = x.num_elements() if size == 0 else size

    @parameter
    fn _row(size: Int):
        let dt: SIMD[DT, 1] = x[size]
        x[size] = dt if dt > 0 else 0.0

    parallelize[_row](num_elements, number_of_cores)


@always_inline
fn leaky_relu[
    DT: DType
](
    inout x: Array[DT], drop: SIMD[DT, 1] = 0.1, number_of_cores: Int = 1, size: Int = 0
) -> None:
    let num_elements: Int = x.num_elements() if size == 0 else size

    @parameter
    fn _row(size: Int):
        let dt: SIMD[DT, 1] = x[size]
        x[size] = dt if dt > drop else drop

    parallelize[_row](num_elements, number_of_cores)


# POINTER FUNC #


@always_inline
fn softmax[DT: DType, nelts: Int](inout x: DTypePointer[DT], num_elements: Int) -> None:
    var max_val: SIMD[DT, 1] = SIMD[DT, 1](-1e9)

    @parameter
    fn _max[_nelts: Int](j: Int):
        let val = x.simd_load[_nelts](j).reduce_max()
        if val > max_val:
            max_val = val

    vectorize[nelts, _max](num_elements)
    var sum_val: SIMD[DT, 1] = 0.0

    @parameter
    fn _sum_exp[_nelts: Int](j: Int):
        x.simd_store[_nelts](j, math.exp(x.simd_load[_nelts](j) - max_val))
        sum_val += x.simd_load[_nelts](j).reduce_add()

    vectorize[nelts, _sum_exp](num_elements)

    @parameter
    fn _norm[_nelts: Int](j: Int):
        x.simd_store[_nelts](j, x.simd_load[_nelts](j) / sum_val)

    vectorize[nelts, _norm](num_elements)


@always_inline
fn sigmoid[
    DT: DType
](inout x: DTypePointer[DT], num_elements: Int, number_of_cores: Int = 1) -> None:
    @parameter
    fn _row(size: Int):
        x.store(size, 1.0 / (1.0 + math.exp(-x.load(size))))

    parallelize[_row](num_elements, number_of_cores)


@always_inline
fn silu[
    DT: DType
](inout x: DTypePointer[DT], num_elements: Int, number_of_cores: Int = 1) -> None:
    @parameter
    fn _row(size: Int):
        let dt: SIMD[DT, 1] = x.load(size)
        x.store(size, dt * (1.0 / (1.0 + math.exp(-dt))))

    parallelize[_row](num_elements, number_of_cores)


@always_inline
fn relu[
    DT: DType
](inout x: DTypePointer[DT], num_elements: Int, number_of_cores: Int = 1) -> None:
    @parameter
    fn _row(size: Int):
        let dt: SIMD[DT, 1] = x.load(size)
        x.store(size, dt if dt > 0 else 0.0)

    parallelize[_row](num_elements, number_of_cores)


@always_inline
fn leaky_relu[
    DT: DType
](
    inout x: DTypePointer[DT],
    num_elements: Int,
    drop: SIMD[DT, 1] = 0.1,
    number_of_cores: Int = 1,
) -> None:
    @parameter
    fn _row(size: Int):
        let dt: SIMD[DT, 1] = x.load(size)
        x.store(size, dt if dt > drop else drop)

    parallelize[_row](num_elements, number_of_cores)


@always_inline
fn sample[DT: DType](array: Array[DT]) -> Int:
    let number_of_cols = array.dim(-1)
    let random_value = DTypePointer[DT].alloc(1)
    rand[DT](random_value, 1)
    var cdf: SIMD[DT, 1] = 0.0
    for i in range(number_of_cols):
        cdf += array[i]
        if random_value.load(0) < cdf:
            return i
    return number_of_cols - 1


@always_inline
fn softmax[DT: DType, nelts: Int,cores:Int](inout B:Array[DT],A: Array[DT], axis: Int = -1):
    

    let N = A.dim(axis)
    @parameter
    fn rows(s:Int):
        var max_element: SIMD[DT, 1] = 0.0

        @parameter
        fn v_max[nelts: Int](i: Int):
            let _x = B.data.simd_load[nelts](s * N + i).reduce_max()
            max_element = max(max_element, _x)

        vectorize[nelts, v_max](N)
        var sum_loop: SIMD[DT, 1] = 0.0

        @parameter
        fn v_exp[nelts: Int](i: Int):
            let _x = math.exp(A.data.simd_load[nelts](s * N + i) - max_element)
            B.data.simd_store[nelts](s * N + i, _x)
            sum_loop += _x.reduce_add()

        vectorize[nelts, v_exp](N)

        @parameter
        fn v_div[nelts: Int](i: Int):
            B.data.simd_store[nelts](
                s * N + i, B.data.simd_load[nelts](s * N + i) / sum_loop
            )

        vectorize[nelts, v_div](N)
    parallelize[rows](B.num_elements() // N,cores)



@always_inline
fn softmax[DT: DType,nelts:Int,cores:Int](A: Array[DT], axis: Int = -1) -> Array[DT]:
    var B: Array[DT] = Array[DT](A)
    B.alloc(0.0)
    softmax[DT,nelts,cores ](B,A, axis)
    return B


@always_inline
fn mse[DT: DType, nelts: Int](inout C: Array[DT], A: Array[DT], B: Array[DT]):
    @parameter
    fn v_mse[nelts: Int](index: Int):
        let error = (
            A.data.simd_load[nelts](index) - B.data.simd_load[nelts](index)
        ) * (A.data.simd_load[nelts](index) - B.data.simd_load[nelts](index))
        C.store[1](0, C.data.load(0) + error.reduce_add())

    vectorize[nelts, v_mse](A.num_elements())
    C.store[1](0, C.data.load(0) / SIMD[DT, 1](A.num_elements()))


@always_inline
fn ce[DT: DType, nelts: Int](inout C: Array[DT], A: Array[DT], B: Array[DT]):
    let N = A.dim(-1)
    let epsilon = SIMD[DT, 1](1e-8)

    @parameter
    fn v_ce[nelts: Int](index: Int):
        let error = -A.data.simd_load[nelts](index) * log(
            B.data.simd_load[nelts](index) + epsilon
        )
        C.store[1](0, C.data.load(0) + error.reduce_add())

    vectorize[nelts, v_ce](A.num_elements())
    C.store[1](0, C.data.load(0) / (SIMD[DT, 1](A.num_elements()) / SIMD[DT, 1](N)))


@always_inline
fn mean[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    let dim_len: Int = B.extra_parameters.load(0)

    # Calculate total number of elements in dims
    var total_elements_in_dims: Int = 1
    for d in range(dim_len):
        let dim: Int = B.extra_parameters.load(d + 1)
        total_elements_in_dims *= A.shape[dim]

    var in_dims = DynamicVector[Bool](B.rank())
    for d in range(B.rank()):
        in_dims[d] = False
    for d in range(dim_len):
        in_dims[B.extra_parameters.load(d + 1)] = True

    # Iterate over all elements in the Array[DT]
    for i in range(A.num_elements()):
        var indeces = DynamicVector[Int]()
        for dim in range(A.rank()):
            indeces.push_back((i // A.strides[dim]) % A.shape[dim])
        var output_index = 0
        for dim in range(B.rank()):
            if not in_dims[dim]:
                output_index += indeces[dim] * B.strides[dim]

        B.data.store(output_index, B.data.load(output_index) + A.data.load(i))

    # Divide each element in output Array[DT] by total number of elements in dims
    for i in range(B.num_elements()):
        let value: SIMD[DT, 1] = B.data.load(i) / SIMD[DT, 1](total_elements_in_dims)
        B.data.store(i, value)


@always_inline
fn variance[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    let dim_len: Int = B.extra_parameters.load(0)
    let mean_output = DTypePointer[DT].alloc(B.num_elements())
    memset_zero(mean_output, B.num_elements())

    # Calculate total number of elements in dims
    var total_elements_in_dims: Int = 1
    for d in range(dim_len):
        let dim: Int = B.extra_parameters.load(d + 1)
        total_elements_in_dims *= A.shape[dim]

    var in_dims = DynamicVector[Bool](B.rank())
    for d in range(B.rank()):
        in_dims[d] = False
    for d in range(dim_len):
        in_dims[B.extra_parameters.load(d + 1)] = True

    # Iterate over all elements in the Array[DT]
    for i in range(A.num_elements()):
        var indeces = DynamicVector[Int]()
        for dim in range(A.rank()):
            indeces.push_back((i // A.strides[dim]) % A.shape[dim])

        var output_index = 0
        for dim in range(B.rank()):
            if not in_dims[dim]:
                output_index += indeces[dim] * B.strides[dim]

        mean_output.store(output_index, mean_output.load(output_index) + A.data.load(i))

    # Divide each element in output Array[DT] by total number of elements in dims
    for i in range(B.num_elements()):
        let value: SIMD[DT, 1] = mean_output.load(i) / SIMD[DT, 1](total_elements_in_dims)
        mean_output.store(i, value)

    # Iterate over all elements in the Array[DT] again to calculate squared _dferences from the mean
    for i in range(A.num_elements()):
        var indeces = DynamicVector[Int]()
        for dim in range(A.rank()):
            indeces.push_back((i // A.strides[dim]) % A.shape[dim])

        var output_index = 0
        for dim in range(B.rank()):
            if not in_dims[dim]:
                output_index += indeces[dim] * B.strides[dim]

        let _df = A.data.load(i) - mean_output.load(output_index)
        B.data.store(output_index, B.data.load(output_index) + _df * _df)

    # Divide each element in squared__df_output Array[DT] by total number of elements in dims to get the variance
    for i in range(B.num_elements()):
        let value: SIMD[DT, 1] = B.data.load(i) / SIMD[DT, 1](total_elements_in_dims - 1)
        B.data.store(i, value)


@always_inline
fn std[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    let dim_len: Int = B.extra_parameters.load(0)
    let mean_output = DTypePointer[DT].alloc(B.num_elements())
    memset_zero(mean_output, B.num_elements())

    # Calculate total number of elements in dims
    var total_elements_in_dims: Int = 1
    for d in range(dim_len):
        let dim: Int = B.extra_parameters.load(d + 1)
        total_elements_in_dims *= A.shape[dim]

    var in_dims = DynamicVector[Bool](B.rank())
    for d in range(B.rank()):
        in_dims[d] = False
    for d in range(dim_len):
        in_dims[B.extra_parameters.load(d + 1)] = True

    # Iterate over all elements in the Array[DT]
    for i in range(A.num_elements()):
        var indeces = DynamicVector[Int]()
        for dim in range(A.rank()):
            indeces.push_back((i // A.strides[dim]) % A.shape[dim])

        var output_index = 0
        for dim in range(B.rank()):
            if not in_dims[dim]:
                output_index += indeces[dim] * B.strides[dim]

        mean_output.store(output_index, mean_output.load(output_index) + A.data.load(i))

    for i in range(B.num_elements()):
        let value: SIMD[DT, 1] = mean_output.load(i) / SIMD[DT, 1](total_elements_in_dims)
        mean_output.store(i, value)

    for i in range(A.num_elements()):
        var indeces = DynamicVector[Int]()
        for dim in range(A.rank()):
            indeces.push_back((i // A.strides[dim]) % A.shape[dim])

        var output_index = 0
        for dim in range(B.rank()):
            if not in_dims[dim]:
                output_index += indeces[dim] * B.strides[dim]

        let _df = A.data.load(i) - mean_output.load(output_index)
        B.data.store(output_index, B.data.load(output_index) + _df * _df)

    for i in range(B.num_elements()):
        let value: SIMD[DT, 1] = sqrt(
            B.data.load(i) / SIMD[DT, 1](total_elements_in_dims - 1)
        )
        B.data.store(i, value)


@always_inline
fn kernel_mul[
    DT: DType, nelts: Int, cores: Int
](
    inout C: Array[DT],
    A: Array[DT],
    B: Array[DT],
    a_index: Int,
    b_index: Int,
    c_index: Int,
    depth: Int,
) -> None:
    let offset_a = a_index * a_over_shape[DT](depth, A, B) * a_over_strides[DT](
        depth, A, B
    )
    let offset_b = b_index * b_over_shape[DT](depth, A, B) * b_over_strides[DT](
        depth, A, B
    )
    let c_rest = C.shape[depth] * C.strides[depth]
    let offset_c = c_index * c_rest

    @parameter
    fn v_mul[nelts: Int](i: Int):
        C.data.simd_store[nelts](
            offset_c + i,
            A.data.simd_load[nelts](offset_a + i)
            * B.data.simd_load[nelts](offset_b + i),
        )

    vectorize[nelts, v_mul](c_rest)


@always_inline
fn kernel_add[
    DT: DType, nelts: Int, cores: Int
](
    inout C: Array[DT],
    A: Array[DT],
    B: Array[DT],
    a_index: Int,
    b_index: Int,
    c_index: Int,
    depth: Int,
) -> None:
    let offset_a = a_index * a_over_shape[DT](depth, A, B) * a_over_strides[DT](
        depth, A, B
    )
    let offset_b = b_index * b_over_shape[DT](depth, A, B) * b_over_strides[DT](
        depth, A, B
    )
    let c_rest = C.shape[depth] * C.strides[depth]
    let offset_c = c_index * c_rest

    @parameter
    fn v_add[nelts: Int](i: Int):
        C.data.simd_store[nelts](
            offset_c + i,
            A.data.simd_load[nelts](offset_a + i)
            + B.data.simd_load[nelts](offset_b + i),
        )

    vectorize[nelts, v_add](c_rest)


@always_inline
fn kernel_sub[
    DT: DType, nelts: Int, cores: Int
](
    inout C: Array[DT],
    A: Array[DT],
    B: Array[DT],
    a_index: Int,
    b_index: Int,
    c_index: Int,
    depth: Int,
) -> None:
    let offset_a = a_index * a_over_shape[DT](depth, A, B) * a_over_strides[DT](
        depth, A, B
    )
    let offset_b = b_index * b_over_shape[DT](depth, A, B) * b_over_strides[DT](
        depth, A, B
    )
    let c_rest = C.shape[depth] * C.strides[depth]
    let offset_c = c_index * c_rest

    @parameter
    fn v_sub[nelts: Int](i: Int):
        C.data.simd_store[nelts](
            offset_c + i,
            A.data.simd_load[nelts](offset_a + i)
            - B.data.simd_load[nelts](offset_b + i),
        )

    vectorize[nelts, v_sub](c_rest)


@always_inline
fn kernel_div[
    DT: DType, nelts: Int, cores: Int
](
    inout C: Array[DT],
    A: Array[DT],
    B: Array[DT],
    a_index: Int,
    b_index: Int,
    c_index: Int,
    depth: Int,
) -> None:
    let offset_a = a_index * a_over_shape[DT](depth, A, B) * a_over_strides[DT](
        depth, A, B
    )
    let offset_b = b_index * b_over_shape[DT](depth, A, B) * b_over_strides[DT](
        depth, A, B
    )
    let c_rest = C.shape[depth] * C.strides[depth]
    let offset_c = c_index * c_rest

    @parameter
    fn v_div[nelts: Int](i: Int):
        C.data.simd_store[nelts](
            offset_c + i,
            A.data.simd_load[nelts](offset_a + i)
            / B.data.simd_load[nelts](offset_b + i),
        )

    vectorize[nelts, v_div](c_rest)


@always_inline
fn base_case_matmul[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Bool:
    return depth == max(A.rank(), B.rank()) - Int(2)


@always_inline
fn base_case_mul[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Bool:
    return a_over_strides(depth, A, B) * a_over_shape[DT](depth, A, B) == b_over_strides[
        DT
    ](depth, A, B) * b_over_shape[DT](depth, A, B)


@always_inline
fn base_case_add[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Bool:
    return a_over_strides[DT](depth, A, B) * a_over_shape[DT](
        depth, A, B
    ) == b_over_strides[DT](depth, A, B) * b_over_shape[DT](depth, A, B)


@always_inline
fn base_case_sub[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Bool:
    return a_over_strides[DT](depth, A, B) * a_over_shape[DT](
        depth, A, B
    ) == b_over_strides[DT](depth, A, B) * b_over_shape[DT](depth, A, B)


@always_inline
fn base_case_div[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Bool:
    return a_over_strides[DT](depth, A, B) * a_over_shape[DT](
        depth, A, B
    ) == b_over_strides[DT](depth, A, B) * b_over_shape[DT](depth, A, B)


@always_inline
fn base_case_pow[DT: DType](depth: Int, A: Array[DT], B: Array[DT]) -> Bool:
    return a_over_strides[DT](depth, A, B) * a_over_shape[DT](
        depth, A, B
    ) == b_over_strides[DT](depth, A, B) * b_over_shape[DT](depth, A, B)


@always_inline
fn kernel_pow[
    DT: DType, nelts: Int, cores: Int
](
    inout C: Array[DT],
    A: Array[DT],
    B: Array[DT],
    a_index: Int,
    b_index: Int,
    c_index: Int,
    depth: Int,
) -> None:
    let offset_a = a_index * a_over_shape[DT](depth, A, B) * a_over_strides[DT](
        depth, A, B
    )
    let offset_b = b_index * b_over_shape[DT](depth, A, B) * b_over_strides[DT](
        depth, A, B
    )
    let c_rest = C.shape[depth] * C.strides[depth]
    let offset_c = c_index * c_rest

    @parameter
    fn v_pow[nelts: Int](i: Int):
        C.data.simd_store[nelts](
            offset_c + i,
            pow(
                A.data.simd_load[nelts](offset_a + i),
                B.data.simd_load[nelts](offset_b + i),
            ),
        )

    vectorize[nelts, v_pow](c_rest)


@always_inline
fn array_pow[
    DT: DType, nelts: Int, cores: Int,parallelized:Bool=True
](inout C: Array[DT], A: Array[DT], B: Array[DT]):
    recursive_broadcast[DT, nelts, cores, kernel_pow, base_case_pow,parallelized](C, A, B)


fn array_pow_all[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    let e = B.extra_parameters.load(0)

    @parameter
    fn v_pow_all[nelts: Int](i: Int):
        let temp = pow(A.data.simd_load[nelts](i), e)
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_pow_all](A.num_elements())


@always_inline
fn array_exp2[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_exp2[nelts: Int](i: Int):
        let temp = exp2(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_exp2](A.num_elements())


@always_inline
fn array_exp[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_exp[nelts: Int](i: Int):
        let temp = exp(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_exp](A.num_elements())


@always_inline
fn array_log2[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_log2[nelts: Int](i: Int):
        let temp = log2(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_log2](A.num_elements())


@always_inline
fn array_log[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_log[nelts: Int](i: Int):
        let temp = log(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_log](A.num_elements())


@always_inline
fn array_sin[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_sin[nelts: Int](i: Int):
        let temp = sin(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_sin](A.num_elements())


@always_inline
fn array_cos[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_cos[nelts: Int](i: Int):
        let temp = cos(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_cos](A.num_elements())


@always_inline
fn array_tan[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_tan[nelts: Int](i: Int):
        let temp = tan(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_tan](A.num_elements())


@always_inline
fn array_asin[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_asin[nelts: Int](i: Int):
        let temp = asin(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_asin](A.num_elements())


@always_inline
fn array_acos[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_acos[nelts: Int](i: Int):
        let temp = acos(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_acos](A.num_elements())


@always_inline
fn array_atan[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_atan[nelts: Int](i: Int):
        let temp = atan(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_atan](A.num_elements())


@always_inline
fn array_sinh[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_sinh[nelts: Int](i: Int):
        let temp = sinh(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_sinh](A.num_elements())


@always_inline
fn array_cosh[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_cosh[nelts: Int](i: Int):
        let temp = cosh(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_cosh](A.num_elements())


@always_inline
fn array_tanh[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_tanh[nelts: Int](i: Int):
        let temp = tanh(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_tanh](A.num_elements())


@always_inline
fn array_relu[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_relu[nelts: Int](i: Int):
        let zeros = SIMD[DT, nelts]()
        B.data.simd_store[nelts](
            i,
            (A.data.simd_load[nelts](i) > zeros).cast[DT]() * A.data.simd_load[nelts](i),
        )

    vectorize[nelts, v_relu](B.num_elements())


@always_inline
fn array_copy[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    memcpy(B.data, A.data, A.num_elements())


@always_inline
fn array_div[
    DT: DType, nelts: Int, cores: Int,parallelized:Bool=True
](inout C: Array[DT], A: Array[DT], B: Array[DT]):
    recursive_broadcast[DT, nelts, cores, kernel_div, base_case_div,parallelized](C, A, B)


@always_inline
fn array_sqrt[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_sqrt[nelts: Int](i: Int):
        let temp = sqrt(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_sqrt](A.num_elements())


@always_inline
fn array_abs[DT: DType, nelts: Int](inout B: Array[DT], A: Array[DT]):
    @parameter
    fn v_abs[nelts: Int](i: Int):
        let temp = abs(A.data.simd_load[nelts](i))
        B.data.simd_store[nelts](i, temp)

    vectorize[nelts, v_abs](A.num_elements())


@always_inline
fn array_mul[
    DT: DType, nelts: Int, cores: Int,parallelized:Bool=True
](inout C: Array[DT], A: Array[DT], B: Array[DT]):
    recursive_broadcast[DT, nelts, cores, kernel_mul, base_case_mul,parallelized](C, A, B)


@always_inline
fn array_add[
    DT: DType, nelts: Int, cores: Int,parallelized:Bool=True
](inout C: Array[DT], A: Array[DT], B: Array[DT]):
    recursive_broadcast[DT, nelts, cores, kernel_add, base_case_add,parallelized](C, A, B)


@always_inline
fn array_sub[
    DT: DType, nelts: Int, cores: Int,parallelized:Bool=True
](inout C: Array[DT], A: Array[DT], B: Array[DT]):
    recursive_broadcast[DT, nelts, cores, kernel_sub, base_case_sub,parallelized](C, A, B)


# More Automated Functions


@always_inline
fn array_pow[DT: DType, nelts: Int, cores: Int](A: Array[DT], B: Array[DT]) -> Array[DT]:
    var C: Array[DT] = Array[DT](B.array_shape)
    C.alloc(0.0)
    array_pow[DT, nelts, cores](C, A, B)
    return C


fn array_pow_all[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_pow_all[DT, nelts](B, A)
    return B


@always_inline
fn array_exp2[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_exp2[DT, nelts](B, A)
    return B


@always_inline
fn array_exp[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_exp[DT, nelts](B, A)
    return B


@always_inline
fn array_log2[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_log2[DT, nelts](B, A)
    return B


@always_inline
fn array_log[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_log[DT, nelts](B, A)
    return B


@always_inline
fn array_sin[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_sin[DT, nelts](B, A)
    return B


@always_inline
fn array_cos[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_cos[DT, nelts](B, A)
    return B


@always_inline
fn array_tan[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_tan[DT, nelts](B, A)
    return B


@always_inline
fn array_asin[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_asin[DT, nelts](B, A)
    return B


@always_inline
fn array_acos[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_acos[DT, nelts](B, A)
    return B


@always_inline
fn array_atan[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_atan[DT, nelts](B, A)
    return B


@always_inline
fn array_sinh[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_sinh[DT, nelts](B, A)
    return B


@always_inline
fn array_cosh[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_cosh[DT, nelts](B, A)
    return B


@always_inline
fn array_tanh[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_tanh[DT, nelts](B, A)
    return B


@always_inline
fn array_relu[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_relu[DT, nelts](B, A)
    return B


@always_inline
fn array_sqrt[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_sqrt[DT, nelts](B, A)
    return B


@always_inline
fn array_abs[DT: DType, nelts: Int](A: Array[DT]) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.array_shape)
    B.alloc(0.0)
    array_abs[DT, nelts](B, A)
    return B


@always_inline
fn array_mul[DT: DType, nelts: Int, cores: Int](A: Array[DT], B: Array[DT]) -> Array[DT]:
    var C: Array[DT] = Array[DT](B.array_shape)
    C.alloc(0.0)
    array_mul[DT, nelts, cores](C, A, B)
    return C


@always_inline
fn array_add[DT: DType, nelts: Int, cores: Int](A: Array[DT], B: Array[DT]) -> Array[DT]:
    var C: Array[DT] = Array[DT](B.array_shape)
    C.alloc(0.0)
    array_add[DT, nelts, cores](C, A, B)
    return C


@always_inline
fn array_sub[DT: DType, nelts: Int, cores: Int](A: Array[DT], B: Array[DT]) -> Array[DT]:
    var C: Array[DT] = Array[DT](B.array_shape)
    C.alloc(0.0)
    array_sub[DT, nelts, cores](C, A, B)
    return C


@always_inline
fn array_div[DT: DType, nelts: Int, cores: Int](A: Array[DT], B: Array[DT]) -> Array[DT]:
    var C: Array[DT] = Array[DT](B.array_shape)
    C.alloc(0.0)
    array_div[DT, nelts, cores](C, A, B)
    return C
