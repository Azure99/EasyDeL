from ..utilities import FileBuffer
from ..array import Array, ArrayShape, PointerOperation
from runtime.llcl import num_cores
import math
from algorithm.functional import parallelize
from ..linen import embedding, rms_layernorm_2d, rope, qkv_tranpose, repeat_kv
from algorithm.functional import vectorize, parallelize


struct LlamaConfig:
    var hidden_size: Int
    var num_attention_heads: Int
    var num_hidden_layers: Int
    var vocab_size: Int
    var epsilon: Float32
    var number_rep_kv: Int
    var max_position_embeddings: Int
    var num_key_value_heads: Int
    var head_dims: Int
    var kv_dims: Int
    var intermediate_size: Int

    fn __init__(
        inout self: Self,
        hidden_size: Int,
        num_attention_heads: Int,
        num_hidden_layers: Int,
        vocab_size: Int,
        epsilon: Float32,
        number_rep_kv: Int,
        max_position_embeddings: Int,
        num_key_value_heads: Int,
        intermediate_size: Int,
    ) -> None:
        self.hidden_size = hidden_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.vocab_size = vocab_size
        self.epsilon = epsilon
        self.number_rep_kv = number_rep_kv
        self.num_key_value_heads = num_key_value_heads
        self.intermediate_size = intermediate_size
        self.head_dims = hidden_size // num_attention_heads
        self.kv_dims = hidden_size // num_key_value_heads

    fn __init__(inout self: Self) -> None:
        self.hidden_size = 512
        self.max_position_embeddings = 2048
        self.num_attention_heads = 8
        self.num_hidden_layers = 8
        self.vocab_size = 32000
        self.epsilon = 1e-5
        self.number_rep_kv = 1
        self.num_key_value_heads = 1
        self.intermediate_size = self.hidden_size * 4
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = self.hidden_size // self.num_key_value_heads

    fn __init__(inout self: Self, inout buffer: FileBuffer) raises -> None:
        self.hidden_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.intermediate_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_hidden_layers = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_attention_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_key_value_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)

        self.vocab_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.max_position_embeddings = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = (
            self.num_key_value_heads * self.hidden_size
        ) // self.num_attention_heads
        self.number_rep_kv = self.num_attention_heads // self.num_key_value_heads
        self.epsilon = 1e-5
        return None

    fn print_config(self: Self) -> None:
        print("\033[1;36mHidden Size             : ", self.hidden_size)
        print("Max Position Embeddings : ", self.max_position_embeddings)
        print("Num Attention Heads     : ", self.num_attention_heads)
        print("Num Hidden Layers       : ", self.num_hidden_layers)
        print("Vocab Size              : ", self.vocab_size)
        print("RMS Norm Epsilon        : ", self.epsilon)
        print("Number Repeat Key Value : ", self.number_rep_kv)
        print("Number Key Value Heads  : ", self.num_key_value_heads)
        print("Intermediate Size       : ", self.intermediate_size)
        print("HEAD DIMS               : ", self.head_dims)
        print("KV DIMS                 : ", self.kv_dims)
        print_no_newline("\033[1;0m")


struct LlamaWeights[DT: DType]:
    var wte: Array[DT]
    var fcr: Array[DT]
    var fci: Array[DT]
    var input_layernorm: Array[DT]
    var q_proj: Array[DT]
    var k_proj: Array[DT]
    var v_proj: Array[DT]
    var o_proj: Array[DT]
    var post_layernorm: Array[DT]
    var w1: Array[DT]
    var w3: Array[DT]
    var w2: Array[DT]
    var norm: Array[DT]
    var lm_head: Array[DT]

    fn __init__(
        inout self,
        config: LlamaConfig,
        shared_weights: Bool,
        inout buf: FileBuffer,
    ) raises:
        let size: Int = sizeof[DT]()
        self.wte = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.hidden_size * config.vocab_size
            ),
            config.vocab_size,
            config.hidden_size,
        )

        self.input_layernorm = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.hidden_size * config.num_hidden_layers
            ),
            config.hidden_size,
            config.num_hidden_layers,
        )

        self.q_proj = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.hidden_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.hidden_size,
        )

        self.k_proj = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.kv_dims * config.hidden_size
            ),
            config.num_hidden_layers,
            config.kv_dims,
            config.hidden_size,
        )

        self.v_proj = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.kv_dims * config.hidden_size
            ),
            config.num_hidden_layers,
            config.kv_dims,
            config.hidden_size,
        )

        self.o_proj = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.hidden_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.hidden_size,
        )

        self.post_layernorm = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.hidden_size * config.num_hidden_layers
            ),
            config.hidden_size,
            config.num_hidden_layers,
        )

        self.w1 = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.intermediate_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.intermediate_size,
        )

        self.w2 = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.intermediate_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.intermediate_size,
        )

        self.w3 = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.intermediate_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.intermediate_size,
        )

        self.norm = Array[DT](
            buf.read_numerical_value_dynamic[DT](config.hidden_size),
            1,
            config.hidden_size,
        )

        self.fcr = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.max_position_embeddings * (config.head_dims // 2)
            ),
            config.max_position_embeddings,
            config.head_dims // 2,
        )

        self.fci = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.max_position_embeddings * (config.head_dims // 2)
            ),
            config.max_position_embeddings,
            config.head_dims // 2,
        )

        self.lm_head = Array[DT](config.hidden_size, config.vocab_size)
        if shared_weights:
            self.lm_head.set_data_from_buffer(self.wte.data)
        else:
            self.lm_head.set_data_from_buffer(
                buf.read_numerical_value_dynamic[DT](self.lm_head.num_elements())
            )


struct LlamaState[DT: DType]:
    var k_cache: Array[DT]
    var v_cache: Array[DT]

    var position_index: Int
    var number_of_cores: Int

    fn __init__(inout self: Self, config: LlamaConfig):
        self.k_cache = Array[DT](
            config.num_hidden_layers,
            config.num_key_value_heads,
            config.max_position_embeddings,
            config.head_dims,
        )
        self.v_cache = Array[DT](
            config.num_hidden_layers,
            config.num_key_value_heads,
            config.max_position_embeddings,
            config.head_dims,
        )

        self.v_cache.alloc(0.0)
        self.k_cache.alloc(0.0)

        self.position_index = 0
        self.number_of_cores = num_cores()


fn llama_forward_call[
    DT: DType, nelts: Int, cores: Int, parallelized: Bool
](
    input_ids: VariadicList[Int],
    position_ids: Array[DT],
    llama_state: LlamaState[DT],
    llama: LlamaWeights[DT],
    config: LlamaConfig,
    position_int: Int,
) raises -> Array[DT]:
    @parameter
    fn write_to_cahce(array: Array[DT], array_cache: Array[DT], layer_index: Int):
        let start_seq = position_ids[0].to_int()
        var end_seq = position_ids[position_ids.num_elements() - 1].to_int() + 1
        for s in range(start_seq, end_seq):
            for i in range(config.num_key_value_heads):

                @parameter
                fn rows(j: Int):
                    let cache_index: Int = array_cache.get_index(layer_index, i, s, j)
                    let array_index: Int = array.get_index(0, i, s - start_seq, j)
                    array_cache.store[1](cache_index, array.load[1](array_index))

                parallelize[rows](config.head_dims, cores)

    @parameter
    fn read_from_cache(array_cache: Array[DT], layer_index: Int) -> Array[DT]:
        var end_seq = position_ids[position_ids.num_elements() - 1].to_int() + 1
        var array: Array[DT] = Array[DT](
            1, config.num_key_value_heads, end_seq, config.head_dims
        )
        array.alloc(0.0)

        for s in range(0, end_seq):
            for i in range(config.num_key_value_heads):

                @parameter
                fn rows(j: Int):
                    let cache_index: Int = array_cache.get_index(layer_index, i, s, j)
                    let array_index: Int = array.get_index(0, i, s, j)
                    array.store[1](array_index, array_cache.load[1](cache_index))

                parallelize[rows](config.head_dims, cores)

        return array

    # Attention Forward

    @parameter
    fn attention_forward(
        hidden_state: Array[DT],
        layer_index: Int,
    ) raises -> Array[DT]:
        let query_sequence_length = position_ids.num_elements()

        let kv_sequence_length = position_ids[
            position_ids.num_elements() - 1
        ].to_int() + 1
        let q_index = layer_index * config.hidden_size * config.hidden_size
        let kv_index = layer_index * config.kv_dims * config.hidden_size

        let wq = llama.q_proj.data.offset(q_index)
        let wk = llama.k_proj.data.offset(kv_index)
        let wv = llama.v_proj.data.offset(kv_index)
        let wo = llama.o_proj.data.offset(q_index)

        var q = matmul[DT, nelts, cores, parallelized](
            hidden_state, Array[DT](wq, config.hidden_size, config.hidden_size)
        )
        var k = matmul[DT, nelts, cores, parallelized](
            hidden_state, Array[DT](wk, config.hidden_size, config.kv_dims)
        )
        var v = matmul[DT, nelts, cores, parallelized](
            hidden_state, Array[DT](wv, config.hidden_size, config.kv_dims)
        )

        q.reshape(
            1, query_sequence_length, config.num_attention_heads, config.head_dims
        )
        k.reshape(
            1, query_sequence_length, config.num_key_value_heads, config.head_dims
        )
        v.reshape(
            1, query_sequence_length, config.num_key_value_heads, config.head_dims
        )

        q = qkv_tranpose[DT, nelts, cores](q)
        k = qkv_tranpose[DT, nelts, cores](k)
        v = qkv_tranpose[DT, nelts, cores](v)

        q = rope[DT, nelts, cores](q, llama.fcr, llama.fci, position_ids)
        k = rope[DT, nelts, cores](k, llama.fcr, llama.fci, position_ids)

        write_to_cahce(k, llama_state.k_cache, layer_index)
        k = read_from_cache(llama_state.k_cache, layer_index)

        write_to_cahce(v, llama_state.v_cache, layer_index)
        v = read_from_cache(llama_state.v_cache, layer_index)

        if config.number_rep_kv != 1:
            k = repeat_kv[DT, nelts, cores](k, config.number_rep_kv)
            v = repeat_kv[DT, nelts, cores](v, config.number_rep_kv)

        k = k.T[nelts, cores]()

        var attention = matmul[DT, nelts, cores, parallelized](q, k)

        for i in range(attention.num_elements()):
            let x_rep: SIMD[DT, 1] = (
                attention.data.simd_load[1](i) / math.sqrt[DT, 1](config.head_dims)
            )
            attention.data.simd_store[1](i, x_rep)

        attention = softmax[DT, nelts, cores](attention, -1)
        var attention_weight = matmul[DT, nelts, cores, parallelized](attention, v)
        attention_weight = qkv_tranpose[DT, nelts, cores](attention_weight)

        attention_weight.reshape(1, query_sequence_length, config.hidden_size)
        let output = matmul[DT, nelts, cores, parallelized](
            attention_weight, Array[DT](wo, config.hidden_size, config.hidden_size)
        )
        return output

    @parameter
    fn mlp_forward(
        hidden_state: Array[DT],
        layer_index: Int,
    ) -> Array[DT]:
        let mlp_pad: Int = layer_index * config.hidden_size * config.intermediate_size

        let w1: DTypePointer[DT] = llama.w1.data.offset(mlp_pad)
        let w2: DTypePointer[DT] = llama.w2.data.offset(mlp_pad)
        let w3: DTypePointer[DT] = llama.w3.data.offset(mlp_pad)

        let w1_array: Array[DT] = Array[DT](
            w1, config.hidden_size, config.intermediate_size
        )
        let w2_array: Array[DT] = Array[DT](
            w2, config.intermediate_size, config.hidden_size
        )
        let w3_array: Array[DT] = Array[DT](
            w3, config.hidden_size, config.intermediate_size
        )

        let w1_res: Array[DT] = matmul[DT, nelts, cores, parallelized](
            hidden_state, w1_array
        )
        let w3_res: Array[DT] = matmul[DT, nelts, cores, parallelized](
            hidden_state, w3_array
        )

        @parameter
        fn silu[_nelts: Int](i: Int):
            let inner_w1 = w1_res.load[_nelts](i)
            let w1_silu = inner_w1 * (1.0 / (1.0 + math.exp(-inner_w1)))
            w1_res.store[_nelts](i, w1_silu * w3_res.load[_nelts](i))

        vectorize[nelts, silu](w1_res.num_elements())
        let output: Array[DT] = matmul[DT, nelts, cores, parallelized](w1_res, w2_array)
        return output

    var hidden_state = embedding[DT, nelts, cores](input_ids, llama.wte)
    for layer_index in range(config.num_hidden_layers):
        var residual: Array[DT] = Array[DT](hidden_state)

        hidden_state = rms_layernorm_2d[DT, nelts, cores](
            hidden_state,
            Array[DT](
                llama.input_layernorm.data.offset(layer_index * config.hidden_size),
                config.hidden_size,
            ),
            1e-5,
        )

        let attention_out: Array[DT] = attention_forward(hidden_state, layer_index)

        @parameter
        fn cols_add_attn[_nelts: Int](i: Int):
            hidden_state.store[_nelts](
                i, attention_out.load[_nelts](i) + residual.load[_nelts](i)
            )

        vectorize[nelts, cols_add_attn](attention_out.num_elements())

        residual = Array[DT](hidden_state)

        hidden_state = rms_layernorm_2d[DT, nelts, cores](
            hidden_state,
            Array[DT](
                llama.post_layernorm.data.offset(layer_index * config.hidden_size),
                config.hidden_size,
            ),
            1e-5,
        )
        hidden_state = mlp_forward(hidden_state, layer_index)

        @parameter
        fn cols_add_mlp[_nelts: Int](i: Int):
            let res: SIMD[DT, _nelts] = hidden_state.load[_nelts](i) + residual.load[
                _nelts
            ](i)
            hidden_state.store[_nelts](i, res)

        vectorize[nelts, cols_add_mlp](hidden_state.num_elements())

    hidden_state = rms_layernorm_2d[DT, nelts, cores](hidden_state, llama.norm, 1e-5)

    let logits = matmul[DT, nelts, cores, parallelized](hidden_state, llama.lm_head)
    return logits
