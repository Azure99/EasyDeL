from ..utilities import FileBuffer
from ..array import Array, ArrayShape, array_add, matmul_row, over_all_softmax
from runtime.llcl import num_cores
import math
from algorithm.functional import vectorize, parallelize, parallelize
from ..linen import rope_rotation, rms_layernorm_1d


struct LlamaConfig:
    var hidden_size: Int
    var num_attention_heads: Int
    var num_hidden_layers: Int
    var vocab_size: Int
    var epsilon: Float32
    var number_rep_kv: Int
    var max_position_embeddings: Int
    var num_key_value_heads: Int
    var head_dims: Int
    var kv_dims: Int
    var intermediate_size: Int

    fn __init__(
        inout self: Self,
        hidden_size: Int,
        num_attention_heads: Int,
        num_hidden_layers: Int,
        vocab_size: Int,
        epsilon: Float32,
        number_rep_kv: Int,
        max_position_embeddings: Int,
        num_key_value_heads: Int,
        intermediate_size: Int,
    ) -> None:
        self.hidden_size = hidden_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.vocab_size = vocab_size
        self.epsilon = epsilon
        self.number_rep_kv = number_rep_kv
        self.num_key_value_heads = num_key_value_heads
        self.intermediate_size = intermediate_size
        self.head_dims = hidden_size // num_attention_heads
        self.kv_dims = hidden_size // num_key_value_heads

    fn __init__(inout self: Self) -> None:
        self.hidden_size = 512
        self.max_position_embeddings = 2048
        self.num_attention_heads = 8
        self.num_hidden_layers = 8
        self.vocab_size = 32000
        self.epsilon = 1e-5
        self.number_rep_kv = 1
        self.num_key_value_heads = 1
        self.intermediate_size = self.hidden_size * 4
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = self.hidden_size // self.num_key_value_heads

    fn __init__(inout self: Self, inout buffer: FileBuffer) raises -> None:
        self.hidden_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.intermediate_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_hidden_layers = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_attention_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_key_value_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)

        self.vocab_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.max_position_embeddings = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = (
            self.num_key_value_heads * self.hidden_size
        ) // self.num_attention_heads
        self.number_rep_kv = self.num_attention_heads // self.num_key_value_heads
        self.epsilon = 1e-5
        return None

    fn print_config(self: Self) -> None:
        print("\033[1;36mHidden Size             : ", self.hidden_size)
        print("Max Position Embeddings : ", self.max_position_embeddings)
        print("Num Attention Heads     : ", self.num_attention_heads)
        print("Num Hidden Layers       : ", self.num_hidden_layers)
        print("Vocab Size              : ", self.vocab_size)
        print("RMS Norm Epsilon        : ", self.epsilon)
        print("Number Repeat Key Value : ", self.number_rep_kv)
        print("Number Key Value Heads  : ", self.num_key_value_heads)
        print("Intermediate Size       : ", self.intermediate_size)
        print("HEAD DIMS               : ", self.head_dims)
        print("KV DIMS                 : ", self.kv_dims)
        print_no_newline("\033[1;0m")


struct LlamaWeights[DT: DType]:
    var wte: Array[DT]
    var fcr: Array[DT]
    var fci: Array[DT]
    var input_layernorm: Array[DT]
    var q_proj: Array[DT]
    var k_proj: Array[DT]
    var v_proj: Array[DT]
    var o_proj: Array[DT]
    var post_layernorm: Array[DT]
    var w1: Array[DT]
    var w3: Array[DT]
    var w2: Array[DT]
    var norm: Array[DT]
    var lm_head: Array[DT]

    fn __init__(
        inout self,
        config: LlamaConfig,
        shared_weights: Bool,
        inout buf: FileBuffer,
        ed_exported: Bool = False,
    ) raises:
        let size: Int = sizeof[DT]()
        self.wte = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.hidden_size * config.vocab_size
            ),
            config.vocab_size,
            config.hidden_size,
        )

        self.input_layernorm = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.hidden_size * config.num_hidden_layers
            ),
            config.hidden_size,
            config.num_hidden_layers,
        )

        self.q_proj = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.hidden_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.hidden_size,
        )

        self.k_proj = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.kv_dims * config.hidden_size
            ),
            config.num_hidden_layers,
            config.kv_dims,
            config.hidden_size,
        )

        self.v_proj = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.kv_dims * config.hidden_size
            ),
            config.num_hidden_layers,
            config.kv_dims,
            config.hidden_size,
        )

        self.o_proj = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.hidden_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.hidden_size,
        )

        self.post_layernorm = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.hidden_size * config.num_hidden_layers
            ),
            config.hidden_size,
            config.num_hidden_layers,
        )

        self.w1 = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.intermediate_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.intermediate_size,
        )

        self.w2 = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.intermediate_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.intermediate_size,
        )

        self.w3 = Array[DT](
            buf.read_numerical_value_dynamic[DT](
                config.num_hidden_layers * config.hidden_size * config.intermediate_size
            ),
            config.num_hidden_layers,
            config.hidden_size,
            config.intermediate_size,
        )

        self.norm = Array[DT](
            buf.read_numerical_value_dynamic[DT](config.hidden_size),
            1,
            config.hidden_size,
        )

        if ed_exported:
            self.fcr = Array[DT](
                buf.read_numerical_value_dynamic[DT](
                    config.max_position_embeddings * config.head_dims
                ),
                config.max_position_embeddings,
                config.head_dims,
            )

            self.fci = Array[DT](
                buf.read_numerical_value_dynamic[DT](
                    config.max_position_embeddings * config.head_dims
                ),
                config.max_position_embeddings,
                config.head_dims,
            )
        else:
            let half_dim = config.head_dims // 2
            self.fcr = Array[DT](
                buf.read_numerical_value_dynamic[DT](
                    config.max_position_embeddings * half_dim
                ),
                config.max_position_embeddings,
                half_dim,
            )

            self.fci = Array[DT](
                buf.read_numerical_value_dynamic[DT](
                    config.max_position_embeddings * half_dim
                ),
                config.max_position_embeddings,
                half_dim,
            )

        self.lm_head = Array[DT](config.hidden_size, config.vocab_size)
        if shared_weights:
            self.lm_head.set_data_from_buffer(self.wte.data)
        else:
            self.lm_head.set_data_from_buffer(
                buf.read_numerical_value_dynamic[DT](self.lm_head.num_elements())
            )


struct LlamaState[DT: DType]:
    var hidden_state: Array[DT]  # activation at current time stamp (dim,)
    var residual: Array[DT]  # same, but inside a residual branch (dim,)
    var residual_port: Array[DT]  # an additional buffer just for convenience (dim,)
    var w1: Array[DT]  # buffer for hidden dimension in the ffn (hidden_dim,)
    var w3: Array[DT]  # buffer for hidden dimension in the ffn (hidden_dim,)
    var q: Array[DT]  # query (dim,)
    var k: Array[DT]  # key (kv_dim,)
    var v: Array[DT]  # value (kv_dim,)
    var attention: Array[DT]  # buffer for scores/attention values (n_heads, seq_len)
    var logits: Array[DT]  # output logits
    var key_cache: Array[DT]  # (layer, seq_len, dim)
    var value_cache: Array[DT]  # (layer, seq_len, dim)

    var number_of_cores: Int

    fn __init__(inout self: Self, config: LlamaConfig):
        self.hidden_state = Array[DT](config.hidden_size)
        self.residual = Array[DT](config.hidden_size)
        self.residual_port = Array[DT](config.hidden_size)

        self.w1 = Array[DT](config.intermediate_size)
        self.w3 = Array[DT](config.intermediate_size)

        self.attention = Array[DT](
            config.num_attention_heads, config.max_position_embeddings
        )

        self.key_cache = Array[DT](
            config.num_hidden_layers, config.max_position_embeddings, config.kv_dims
        )
        self.value_cache = Array[DT](
            config.num_hidden_layers, config.max_position_embeddings, config.kv_dims
        )

        self.q = Array[DT](config.hidden_size)
        self.k = Array[DT](config.kv_dims)
        self.v = Array[DT](config.kv_dims)

        self.logits = Array[DT](config.vocab_size)

        self.hidden_state.alloc(0.0)
        self.residual.alloc(0.0)
        self.residual_port.alloc(0.0)
        self.w1.alloc(0.0)
        self.w3.alloc(0.0)
        self.attention.alloc(0.0)
        self.logits.alloc(0.0)
        self.key_cache.alloc(0.0)
        self.value_cache.alloc(0.0)
        self.q.alloc(0.0)

        self.number_of_cores = num_cores()


fn llama_forward_call[
    DT: DType, nelts: Int, cores: Int, parallelized: Bool
](
    input_id: Int,
    position_id: Int,
    inout llama_state: LlamaState[DT],
    llama: LlamaWeights[DT],
    config: LlamaConfig,
):
    memcpy[DT](
        llama_state.hidden_state.data,
        llama.wte.data.offset(input_id * config.hidden_size),
        config.hidden_size,
    )

    let freqs_cis_imag_row: Array[DT] = Array[DT](
        llama.fci.data.offset(position_id * config.head_dims), config.head_dims
    )

    let freqs_cis_real_row: Array[DT] = Array[DT](
        llama.fcr.data.offset(position_id * config.head_dims), config.head_dims
    )

    for layer_index in range(config.num_hidden_layers):
        let cache_stride = layer_index * config.max_position_embeddings * config.kv_dims

        rms_layernorm_1d[DT, nelts, cores](
            llama_state.residual,
            llama_state.hidden_state,
            Array[DT](
                llama.input_layernorm.data.offset(layer_index * config.hidden_size),
                config.hidden_size,
            ),
            1e-5,
        )

        llama_state.k.set_data_from_buffer(
            llama_state.key_cache.data.offset(
                layer_index * position_id * config.kv_dims
            )
        )

        llama_state.v.set_data_from_buffer(
            llama_state.value_cache.data.offset(
                layer_index * position_id * config.kv_dims
            )
        )
        let q_proj: Array[DT] = Array[DT](
            llama.q_proj.data.offset(
                layer_index * config.hidden_size * config.hidden_size
            ),
            config.hidden_size,
            config.hidden_size,
        )
        let k_proj: Array[DT] = Array[DT](
            llama.k_proj.data.offset(layer_index * config.hidden_size * config.kv_dims),
            config.hidden_size,
            config.kv_dims,
        )
        let v_proj: Array[DT] = Array[DT](
            llama.v_proj.data.offset(layer_index * config.hidden_size * config.kv_dims),
            config.hidden_size,
            config.kv_dims,
        )

        matmul_row[DT, nelts, cores](llama_state.q, llama_state.residual, q_proj)
        matmul_row[DT, nelts, cores](llama_state.k, llama_state.residual, k_proj)
        matmul_row[DT, nelts, cores](llama_state.v, llama_state.residual, v_proj)

        rope_rotation[DT, cores](
            llama_state.q,
            llama_state.k,
            freqs_cis_real_row,
            freqs_cis_imag_row,
            config.head_dims,
            config.num_attention_heads,
            config.num_key_value_heads,
        )

        llama_state.residual.fill(0)

        @parameter
        fn loop_over_head(h: Int):
            let attention_offset = h * config.max_position_embeddings
            let hidden_size_offset = h * config.head_dims

            for pos in range(position_id + 1):
                let k_offset = (
                    cache_stride
                    + pos * config.kv_dims
                    + (h // config.number_rep_kv) * config.head_dims
                )
                var score: SIMD[DT, 1] = 0.0

                @parameter
                fn score_fn[_nelts: Int](i: Int):
                    score += (
                        llama_state.q.data.simd_load[_nelts](hidden_size_offset + i)
                        * llama_state.key_cache.data.simd_load[_nelts](k_offset + i)
                    ).reduce_add()

                vectorize[nelts, score_fn](config.head_dims)
                score /= math.sqrt[DT, 1](config.head_dims)
                llama_state.attention.store[1](attention_offset + pos, score)

            over_all_softmax[DT, nelts, cores](
                llama_state.attention,
                attention_offset,
                attention_offset + position_id + 1,
            )

            for pos in range(position_id + 1):
                let v_offset = (
                    cache_stride
                    + pos * config.kv_dims
                    + (h // config.number_rep_kv) * config.head_dims
                )

                let attn = llama_state.attention.load[1](attention_offset + pos)

                @parameter
                fn attn_value_mul[_nelts: Int](i: Int):
                    let av = llama_state.residual.data.simd_load[_nelts](
                        hidden_size_offset + i
                    ) + attn * llama_state.v.data.simd_load[_nelts](v_offset + i)
                    llama_state.residual.data.simd_store[_nelts](
                        hidden_size_offset + i, av
                    )

                vectorize[nelts, attn_value_mul](config.head_dims)

        parallelize[loop_over_head](config.num_attention_heads, cores)

        matmul_row[DT, nelts, cores](
            llama_state.residual_port,
            llama_state.residual,
            Array[DT](
                llama.o_proj.data.offset(
                    layer_index * config.hidden_size * config.hidden_size
                ),
                config.hidden_size,
                config.hidden_size,
            ),
        )

        array_add[DT, nelts, cores](
            llama_state.hidden_state,
            llama_state.residual_port,
            llama_state.hidden_state,
        )
        rms_layernorm_1d[DT, nelts, cores](
            llama_state.residual,
            llama_state.hidden_state,
            Array[DT](
                llama.post_layernorm.data.offset(layer_index * config.hidden_size),
                config.hidden_size,
            ),
            1e-5,
        )

        # Its fine here

        matmul_row[DT, nelts, cores](
            llama_state.w1,
            llama_state.residual,
            Array[DT](
                llama.w1.data.offset(
                    layer_index * config.hidden_size * config.intermediate_size
                ),
                config.hidden_size,
                config.intermediate_size,
            ),
        )
        matmul_row[DT, nelts, cores](
            llama_state.w3,
            llama_state.residual,
            Array[DT](
                llama.w3.data.offset(
                    layer_index * config.hidden_size * config.intermediate_size
                ),
                config.hidden_size,
                config.intermediate_size,
            ),
        )

        @parameter
        fn silu[_nelts: Int](i: Int):
            let initial_w1 = llama_state.w1.data.simd_load[_nelts](i)
            let w1_rep = initial_w1 * (1.0 / (1.0 + math.exp(-initial_w1)))
            llama_state.w1.data.simd_store[_nelts](
                i, w1_rep * llama_state.w3.data.simd_load[_nelts](i)
            )

        vectorize[nelts, silu](config.intermediate_size)

        llama_state.residual.fill(0)
        matmul_row[DT, nelts, cores](
            llama_state.residual,
            llama_state.w1,
            Array[DT](
                llama.w2.data.offset(
                    layer_index * config.hidden_size * config.intermediate_size
                ),
                config.intermediate_size,
                config.hidden_size,
            ),
        )
        array_add[DT, nelts, cores](
            llama_state.hidden_state,
            llama_state.hidden_state,
            llama_state.residual,
        )

    rms_layernorm_1d[DT, nelts, cores](
        llama_state.hidden_state,
        llama_state.hidden_state,
        llama.norm,
        1e-5,
    )

    matmul_row[DT, nelts, cores](
        llama_state.logits, llama_state.hidden_state, llama.lm_head
    )
