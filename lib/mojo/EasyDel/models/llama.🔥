from math import math
from ..matrix_ import *
from runtime.llcl import Runtime, num_cores
from python import Python as Py
from memory.memory import memset_zero


struct Tokenizer[T: DType]:
    var vocab: Pointer[Pointer[UInt8]]
    var vocab_scores: DTypePointer[T]
    var max_token_length: Int
    var vocab_size: Int
    var sorted_vocab: Pointer[Pointer[UInt8]]
    var sorted_indices: DynamicVector[Int]

    fn __init__(inout self, vocab_size: Int, inout buffer: FileBuffer[T]):
        self.vocab_size = vocab_size
        self.max_token_length = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.offset += sizeof[Int]()
        self.vocab_scores = DTypePointer[T].alloc(self.vocab_size)
        self.vocab = Pointer[Pointer[UInt8]].alloc(self.vocab_size)

        self.sorted_vocab = Pointer[Pointer[UInt8]].alloc(0)
        self.sorted_indices = DynamicVector[Int](0)

        for i in range(0, self.vocab_size):
            self.vocab_scores.simd_store[1](i, read_numerical_value[T](buffer))
            let string_length = (
                buffer.data.offset(buffer.offset)
                .bitcast[DType.uint32]()
                .load(0)
                .to_int()
            )
            buffer.offset += sizeof[Int]()
            self.vocab.store(i, read_string_value[T](buffer, string_length))

    fn sort(inout self) -> None:
        if len(self.sorted_indices) < self.vocab_size:
            self.sorted_indices = DynamicVector[Int](self.vocab_size)
            self.sorted_vocab = Pointer[Pointer[UInt8]].alloc(self.vocab_size)
            for ii in range(self.vocab_size):
                self.sorted_vocab.store(ii, self.vocab[ii])
                self.sorted_indices.push_back(ii)

        let n = self.vocab_size
        loop_sort(self.sorted_vocab, self.sorted_indices, 0, n - 1)

    fn find(inout self, token: Pointer[UInt8]) -> Int:
        let n = self.vocab_size
        if len(self.sorted_indices) < n:
            self.sort()
        var left = 0
        var right = n - 1
        while left <= right:
            let mid = left + (right - left) // 2
            let comparison = dif_string(self.sorted_vocab[mid], token)

            if comparison == 0:
                return self.sorted_indices[mid]
            if comparison < 0:
                left = mid + 1
            else:
                right = mid - 1
        return -1


struct Config[T: DType]:
    var hidden_size: Int
    var num_attention_heads: Int
    var num_hidden_layers: Int
    var vocab_size: Int
    var rms_norm_eps: SIMD[T, 1]
    var number_rep_kv: Int
    var max_position_embeddings: Int
    var num_key_value_heads: Int
    var head_dims: Int
    var kv_dims: Int
    var intermediate_size: Int

    fn __init__(
        inout self: Self,
        hidden_size: Int,
        num_attention_heads: Int,
        num_hidden_layers: Int,
        vocab_size: Int,
        rms_norm_eps: SIMD[T, 1],
        number_rep_kv: Int,
        max_position_embeddings: Int,
        num_key_value_heads: Int,
        intermediate_size: Int,
    ) -> None:
        self.hidden_size = hidden_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.number_rep_kv = number_rep_kv
        self.num_key_value_heads = num_key_value_heads
        self.intermediate_size = intermediate_size
        self.head_dims = hidden_size // num_attention_heads
        self.kv_dims = hidden_size // num_key_value_heads

    fn __init__(inout self: Self) -> None:
        self.hidden_size = 512
        self.intermediate_size = self.hidden_size * 4
        self.max_position_embeddings = 2048
        self.num_attention_heads = 8
        self.num_hidden_layers = 8
        self.vocab_size = 32000
        self.rms_norm_eps = 1e-5
        self.number_rep_kv = 1
        self.num_key_value_heads = 1
        self.intermediate_size = self.hidden_size * 4
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = self.hidden_size // self.num_key_value_heads

    fn __init__(inout self: Self, inout buffer: FileBuffer[T]) raises -> None:
        self.hidden_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.intermediate_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_hidden_layers = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_attention_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_key_value_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.vocab_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.max_position_embeddings = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = (
            self.num_key_value_heads * self.hidden_size
        ) // self.num_attention_heads
        self.number_rep_kv = self.num_attention_heads // self.num_attention_heads
        self.rms_norm_eps = 1e-5
        return None


struct RunState[T: DType]:
    var x: Matrix[T]  # hidden_size
    var x_norm: Matrix[T]  # normed state hidden_size
    var k: Matrix[T]  # Key Matrix 0,0
    var v: Matrix[T]  # Value Matrix 0,0
    var q: Matrix[T]  # Query Matrix hidden_size
    var o: Matrix[T]  # hidden_size ,hidden_size
    var attn_weights: Matrix[T]  # head_dims, max position embeddings
    var k_cache: Matrix[T]  # num_layers ,max position embeddings ,head_dims
    var v_cache: Matrix[T]  # num_layers ,max position embeddings ,head_dims
    var ffn_w1: Matrix[T]  # i size
    var ffn_w3: Matrix[T]  # i size
    var logits: Matrix[T]  # Matrix vocab size
    var rt: Runtime

    fn __init__(inout self: Self, config: Config[T]) -> None:
        self.x = Matrix[T](config.hidden_size)
        self.x.alloc_zero()
        self.x_norm = Matrix[T](config.hidden_size)
        self.x_norm.alloc_zero()
        self.q = Matrix[T](config.hidden_size)
        self.q.alloc_zero()
        self.k = Matrix[T](0, 0)
        self.k.alloc_zero()
        self.v = Matrix[T](0, 0)
        self.v.alloc_zero()
        self.o = Matrix[T](config.hidden_size, config.hidden_size)
        self.o.alloc_zero()
        self.attn_weights = Matrix[T](config.head_dims, config.max_position_embeddings)
        self.attn_weights.alloc_zero()
        self.v_cache = Matrix[T](
            config.num_hidden_layers, config.max_position_embeddings, config.kv_dims
        )
        self.v_cache.alloc_zero()
        self.k_cache = Matrix[T](
            config.num_hidden_layers, config.max_position_embeddings, config.kv_dims
        )
        self.k_cache.alloc_zero()

        self.ffn_w1 = Matrix[T](config.intermediate_size)
        self.ffn_w1.alloc_zero()
        self.ffn_w3 = Matrix[T](config.intermediate_size)
        self.ffn_w3.alloc_zero()

        self.logits = Matrix[T](config.vocab_size)
        self.logits.alloc_zero()
        self.rt = Runtime(num_cores() // 2)


struct LlamaWeights[T: DType]:
    var wte: Matrix[T]
    var fcr: Matrix[T]
    var fci: Matrix[T]
    var input_norm_attn_weigth: Matrix[T]
    var wq: Matrix[T]
    var wk: Matrix[T]
    var wv: Matrix[T]
    var wo: Matrix[T]
    var post_norm_weigth: Matrix[T]
    var w1: Matrix[T]
    var w3: Matrix[T]
    var w2: Matrix[T]
    var final_norm_weight: Matrix[T]
    var lm_head: Matrix[T]

    fn __init__(
        inout self, config: Config[T], shared_weights: Int, inout buf: FileBuffer[T]
    ) raises:
        let sla: Int = buf.offset

        self.wte = Matrix[T](config.vocab_size, config.hidden_size)
        self.wte.set_data_from_buffer(buf.read_values(self.wte.size()))

        self.input_norm_attn_weigth = Matrix[T](
            config.num_hidden_layers, config.hidden_size
        )
        self.input_norm_attn_weigth.set_data_from_buffer(
            buf.read_values(self.input_norm_attn_weigth.size())
        )
        self.wq = Matrix[T](
            config.num_hidden_layers, config.hidden_size, config.hidden_size
        )
        self.wq.set_data_from_buffer(buf.read_values(self.wq.size()))
        self.wk = Matrix[T](
            config.num_hidden_layers, config.hidden_size, config.kv_dims
        )
        self.wk.set_data_from_buffer(buf.read_values(self.wk.size()))
        self.wv = Matrix[T](
            config.num_hidden_layers, config.hidden_size, config.kv_dims
        )
        self.wv.set_data_from_buffer(buf.read_values(self.wv.size()))
        self.wo = Matrix[T](
            config.num_hidden_layers, config.hidden_size, config.hidden_size
        )
        self.wo.set_data_from_buffer(buf.read_values(self.wo.size()))
        self.post_norm_weigth = Matrix[T](config.num_hidden_layers, config.hidden_size)
        self.post_norm_weigth.set_data_from_buffer(
            buf.read_values(self.post_norm_weigth.size())
        )
        self.w1 = Matrix[T](
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w1.set_data_from_buffer(buf.read_values(self.w1.size()))
        self.w2 = Matrix[T](
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w2.set_data_from_buffer(buf.read_values(self.w2.size()))
        self.w3 = Matrix[T](
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w3.set_data_from_buffer(buf.read_values(self.w3.size()))
        self.final_norm_weight = Matrix[T](config.hidden_size)
        self.final_norm_weight.set_data_from_buffer(
            buf.read_values(self.final_norm_weight.size())
        )
        self.fcr = Matrix[T](
            config.max_position_embeddings,
            (config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fcr.set_data_from_buffer(buf.read_values(self.fcr.size()))
        self.fci = Matrix[T](
            config.max_position_embeddings,
            (config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fci.set_data_from_buffer(buf.read_values(self.fci.size()))
        self.lm_head = Matrix[T](config.vocab_size, config.hidden_size)
        self.lm_head.set_data_from_buffer(self.wte.data)


fn read_file[T: DType](inout buffer: FileBuffer[T], filename: StringRef) raises -> None:
    let _os = Py.import_module("os")
    let size: Int = atol(_os.path.getsize(filename).to_string())
    buffer.size = size
    let f = File(filename)
    let state = DTypePointer[DType.uint8].alloc(size)
    var reader = BufReader[4096](f ^)
    var bytes_read: Int = 1
    var offset = 0
    while bytes_read > 0:
        let buf = Buffer[4096, DType.uint8](state.offset(offset))
        bytes_read = reader.read(buf)
        offset += bytes_read
    buffer.data = state
    buffer.offset = 0
    return None


fn llama_forward[
    T: DType, nelts: Int
](
    input_id: Int,
    position: Int,
    weight: LlamaWeights[T],
    inout state: RunState[T],
    config: Config[T],
) raises -> None:
    let hidden_size: Int = config.hidden_size
    let intermediate_size: Int = config.intermediate_size
    let nl: Int = config.num_hidden_layers
    let nh: Int = config.num_attention_heads
    let hd: Int = config.head_dims
    let nkvh: Int = config.num_key_value_heads
    let kvd: Int = config.kv_dims
    let kv_mul: Int = config.number_rep_kv

    var x = state.x.data
    let wte_data = weight.wte.data.offset(input_id * hidden_size)
    memcpy[T](x, wte_data, hidden_size)

    var mrt: Matrix[T] = Matrix[T](0, 0)
    let fci = weight.fci.data.offset(position * hd // 2)
    let fcr = weight.fcr.data.offset(position * hd // 2)
    for layer_index in range(nl):
        # Padding is calculated by multiplying curreny layer index , max number of position embedding ( max number of position embeddin in padding will be used in
        # k v for seting offset and k and v values are like [max position embedding , kv_dims]), and kv_dims
        let padding = layer_index * config.max_position_embeddings * kvd
        rms_norm[T, nelts](
            state.x_norm.data,
            x,
            weight.input_norm_attn_weigth.data,
            1e-5,
            config.hidden_size,
            state.rt,
        )
        # ==================
        # Calculating Query
        mrt.set_data_from_buffer(
            weight.wq.data.offset(layer_index * hidden_size * hidden_size),
            hidden_size,
            hidden_size,
        )
        matmul_parallelize_with_parallelism_level[T, nelts](
            state.q, mrt, state.x_norm, state.rt
        )
        # ==================
        # Calculating Key
        mrt.set_data_from_buffer(
            weight.wk.data.offset(layer_index * hidden_size * hidden_size),
            hidden_size,
            hidden_size,
        )

        matmul_parallelize_with_parallelism_level[T, nelts](
            state.k, mrt, state.x_norm, state.rt
        )
        # ==================
        # Calculating Values
        mrt.set_data_from_buffer(
            weight.wv.data.offset(layer_index * hidden_size * hidden_size),
            hidden_size,
            hidden_size,
        )
        matmul_parallelize_with_parallelism_level[T, nelts](
            state.v, mrt, state.x_norm, state.rt
        )
        # ==================

        let q: DTypePointer[T] = state.q.data
        let k: DTypePointer[T] = state.k.data

        for i in range(0, hd * nkvh, 2):
            let half_dim: Int = i % hd // 2

            let cu_fci = fci.offset(half_dim).load(0)
            let cu_fcr = fcr.offset(half_dim).load(0)

            let q_0 = q.offset(half_dim).load(0)
            let q_1 = q.offset(half_dim + 1).load(0)

            let k_0 = k.offset(half_dim).load(0)
            let k_1 = k.offset(half_dim + 1).load(0)

            q.offset(half_dim).store(0, q_0 * cu_fcr - q_0 * cu_fci)
            q.offset(half_dim + 1).store(0, q_1 * cu_fci - q_1 * cu_fcr)

            k.offset(half_dim).store(0, k_0 * cu_fcr - k_0 * cu_fci)
            k.offset(half_dim + 1).store(0, k_1 * cu_fci - k_1 * cu_fcr)

        for i in range(hd * nkvh, hidden_size, 2):
            let half_dim: Int = i % hd // 2

            let cu_fci = fci.offset(half_dim).load(0)
            let cu_fcr = fcr.offset(half_dim).load(0)

            let q_0 = q.offset(half_dim).load(0)
            let q_1 = q.offset(half_dim + 1).load(0)

            q.offset(half_dim).store(0, q_0 * cu_fcr - q_0 * cu_fci)
            q.offset(half_dim + 1).store(0, q_1 * cu_fci - q_1 * cu_fcr)

        for head_inedx in range(config.num_attention_heads):
            let q = state.q.data.offset(head_inedx * hd)
            var attn = state.attn_weights.data.offset(
                head_inedx * config.max_position_embeddings
            )
            for cp in range(position + 1):
                let k = state.k_cache.data.offset(
                    padding + cp * kvd + (head_inedx // nkvh) * hd
                )
                var attn_w: SIMD[T, 1] = 0.0
                for ii in range(hd):
                    attn_w += q.offset(ii).load(0) * k.offset(ii).load(0)
                attn_w /= math.sqrt[T, 1](hd)
                attn.offset(cp).store(0, attn_w)
            softmax[T, nelts](attn, position + 1)
            var x_norm = state.x_norm.data.offset(head_inedx * hd)
            memset_zero[T](x_norm, hd)
            for cp in range(position + 1):
                let v = state.v_cache.data.offset(
                    padding + cp * kvd + (head_inedx // nkvh) * hd
                )
                var state_vw: SIMD[T, 1] = 0.0
                let aa = attn.offset(cp).load(0)
                for ii in range(hd):
                    let st = v.offset(ii).load(0) * aa + x_norm.offset(ii).load(0)
                    x_norm.offset(ii).store(0, st)

        mrt.set_data_from_buffer(
            weight.wo.data.offset(layer_index * hidden_size * hidden_size),
            hidden_size,
            hidden_size,
        )

        matmul_parallelize_with_parallelism_level[T, nelts](
            state.o, state.x_norm, mrt, state.rt
        )

        add_pointers[T, nelts](x, state.o.data, hidden_size)

        rms_norm[T, nelts](
            state.x_norm.data,
            x,
            weight.input_norm_attn_weigth.data,
            1e-5,
            hidden_size,
            state.rt,
        )
        mrt.set_data_from_buffer(
            weight.w1.data.offset(layer_index * intermediate_size * hidden_size),
            hidden_size,
            intermediate_size,
        )

        matmul_parallelize_with_parallelism_level[T, nelts](
            state.ffn_w1, mrt, state.x_norm, state.rt
        )

        mrt.set_data_from_buffer(
            weight.w3.data.offset(layer_index * intermediate_size * hidden_size),
            hidden_size,
            intermediate_size,
        )

        matmul_parallelize_with_parallelism_level[T, nelts](
            state.ffn_w3, mrt, state.x_norm, state.rt
        )

        for i in range(intermediate_size):
            let __x = state.ffn_w1[i]
            state.ffn_w1[i] = __x * (1.0 / (1.0 + math.exp[T, 1](-__x)))

        for i in range(intermediate_size):
            state.ffn_w1[i] = state.ffn_w1[i] * state.ffn_w3[i]
        mrt.set_data_from_buffer(
            weight.w2.data.offset(layer_index * intermediate_size * hidden_size),
            intermediate_size,
            hidden_size,
        )
        matmul_parallelize_with_parallelism_level[T, nelts](
            state.x_norm, mrt, state.ffn_w1, state.rt
        )
        add_pointers[T, nelts](x, state.x_norm.data, hidden_size)

    rms_norm[T, nelts](
        state.x_norm.data, x, weight.final_norm_weight.data, 1e-5, hidden_size, state.rt
    )
    mrt.set_data_from_buffer(weight.lm_head.data, hidden_size, config.vocab_size)
    matmul_parallelize_with_parallelism_level[T, nelts](
        state.logits, mrt, state.x_norm, state.rt
    )
