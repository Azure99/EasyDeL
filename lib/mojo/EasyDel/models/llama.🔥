from ..utilities import FileBuffer
from runtime.llcl import num_cores
import math
from algorithm.functional import vectorize, parallelize, parallelize
from tensor import Tensor, TensorSpec


struct LlamaConfig:
    var hidden_size: Int
    var num_attention_heads: Int
    var num_hidden_layers: Int
    var vocab_size: Int
    var epsilon: Float32
    var number_rep_kv: Int
    var max_position_embeddings: Int
    var num_key_value_heads: Int
    var head_dims: Int
    var kv_dims: Int
    var intermediate_size: Int

    fn __init__(
        inout self: Self,
        hidden_size: Int,
        num_attention_heads: Int,
        num_hidden_layers: Int,
        vocab_size: Int,
        epsilon: Float32,
        number_rep_kv: Int,
        max_position_embeddings: Int,
        num_key_value_heads: Int,
        intermediate_size: Int,
    ) -> None:
        self.hidden_size = hidden_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.vocab_size = vocab_size
        self.epsilon = epsilon
        self.number_rep_kv = number_rep_kv
        self.num_key_value_heads = num_key_value_heads
        self.intermediate_size = intermediate_size
        self.head_dims = hidden_size // num_attention_heads
        self.kv_dims = hidden_size // num_key_value_heads

    fn __init__(inout self: Self) -> None:
        self.hidden_size = 512
        self.max_position_embeddings = 2048
        self.num_attention_heads = 8
        self.num_hidden_layers = 8
        self.vocab_size = 32000
        self.epsilon = 1e-5
        self.number_rep_kv = 1
        self.num_key_value_heads = 1
        self.intermediate_size = self.hidden_size * 4
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = self.hidden_size // self.num_key_value_heads

    fn __init__(inout self: Self, inout buffer: FileBuffer) raises -> None:
        self.hidden_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.intermediate_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_hidden_layers = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_attention_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_key_value_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)

        self.vocab_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.max_position_embeddings = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = (
            self.num_key_value_heads * self.hidden_size
        ) // self.num_attention_heads
        self.number_rep_kv = self.num_attention_heads // self.num_key_value_heads
        self.epsilon = 1e-5
        return None

    fn print_config(self: Self) -> None:
        print("\033[1;36mHidden Size             : ", self.hidden_size)
        print("Max Position Embeddings : ", self.max_position_embeddings)
        print("Num Attention Heads     : ", self.num_attention_heads)
        print("Num Hidden Layers       : ", self.num_hidden_layers)
        print("Vocab Size              : ", self.vocab_size)
        print("RMS Norm Epsilon        : ", self.epsilon)
        print("Number Repeat Key Value : ", self.number_rep_kv)
        print("Number Key Value Heads  : ", self.num_key_value_heads)
        print("Intermediate Size       : ", self.intermediate_size)
        print("HEAD DIMS               : ", self.head_dims)
        print("KV DIMS                 : ", self.kv_dims)
        print_no_newline("\033[1;0m")
