from ..utilities import FileBuffer
from ..array import Array, ArrayShape, PointerOperation
from ..linen import rms_norm, rope, dot_product_attention
from runtime.llcl import num_cores
import math
from algorithm.functional import parallelize


struct LlamaConfig:
    var hidden_size: Int
    var num_attention_heads: Int
    var num_hidden_layers: Int
    var vocab_size: Int
    var epsilon: Float32
    var number_rep_kv: Int
    var max_position_embeddings: Int
    var num_key_value_heads: Int
    var head_dims: Int
    var kv_dims: Int
    var intermediate_size: Int

    fn __init__(
        inout self: Self,
        hidden_size: Int,
        num_attention_heads: Int,
        num_hidden_layers: Int,
        vocab_size: Int,
        epsilon: Float32,
        number_rep_kv: Int,
        max_position_embeddings: Int,
        num_key_value_heads: Int,
        intermediate_size: Int,
    ) -> None:
        self.hidden_size = hidden_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.vocab_size = vocab_size
        self.epsilon = epsilon
        self.number_rep_kv = number_rep_kv
        self.num_key_value_heads = num_key_value_heads
        self.intermediate_size = intermediate_size
        self.head_dims = hidden_size // num_attention_heads
        self.kv_dims = hidden_size // num_key_value_heads

    fn __init__(inout self: Self) -> None:
        self.hidden_size = 512
        self.max_position_embeddings = 2048
        self.num_attention_heads = 8
        self.num_hidden_layers = 8
        self.vocab_size = 32000
        self.epsilon = 1e-5
        self.number_rep_kv = 1
        self.num_key_value_heads = 1
        self.intermediate_size = self.hidden_size * 4
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = self.hidden_size // self.num_key_value_heads

    fn __init__(inout self: Self, inout buffer: FileBuffer) raises -> None:
        self.hidden_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.intermediate_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_hidden_layers = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_attention_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.num_key_value_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)

        self.vocab_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.max_position_embeddings = (
            buffer.data.offset(buffer.offset).bitcast[DType.int32]().load(0).to_int()
        )
        buffer.move_offset(4)
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = (
            self.num_key_value_heads * self.hidden_size
        ) // self.num_attention_heads
        self.number_rep_kv = self.num_attention_heads // self.num_key_value_heads
        self.epsilon = 1e-5
        return None

    fn print_config(self: Self) -> None:
        print("\033[1;36mHidden Size             : ", self.hidden_size)
        print("Max Position Embeddings : ", self.max_position_embeddings)
        print("Num Attention Heads     : ", self.num_attention_heads)
        print("Num Hidden Layers       : ", self.num_hidden_layers)
        print("Vocab Size              : ", self.vocab_size)
        print("RMS Norm Epsilon        : ", self.epsilon)
        print("Number Repeat Key Value : ", self.number_rep_kv)
        print("Number Key Value Heads  : ", self.num_key_value_heads)
        print("Intermediate Size       : ", self.intermediate_size)
        print("HEAD DIMS               : ", self.head_dims)
        print("KV DIMS                 : ", self.kv_dims)
        print_no_newline("\033[1;0m")


struct LlamaWeights[T: DType]:
    var wte: Array[T]
    var fcr: Array[T]
    var fci: Array[T]
    var input_layernorm: Array[T]
    var q_proj: Array[T]
    var k_proj: Array[T]
    var v_proj: Array[T]
    var o_proj: Array[T]
    var post_layernorm: Array[T]
    var w1: Array[T]
    var w3: Array[T]
    var w2: Array[T]
    var norm: Array[T]
    var lm_head: Array[T]

    fn __init__(
        inout self,
        config: LlamaConfig,
        shared_weights: Bool,
        inout buf: FileBuffer,
    ) raises:
        let size: Int = sizeof[T]()
        self.wte = Array[T](config.hidden_size, config.vocab_size)

        self.wte.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.wte.num_elements())
        )

        self.input_layernorm = Array[T](config.hidden_size, config.num_hidden_layers)
        self.input_layernorm.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.input_layernorm.num_elements())
        )
        self.q_proj = Array[T](
            config.num_hidden_layers, config.hidden_size, config.hidden_size
        )
        self.q_proj.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.q_proj.num_elements())
        )
        self.k_proj = Array[T](
            config.num_hidden_layers, config.kv_dims, config.hidden_size
        )
        self.k_proj.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.k_proj.num_elements())
        )
        self.v_proj = Array[T](
            config.num_hidden_layers, config.kv_dims, config.hidden_size
        )

        self.v_proj.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.v_proj.num_elements())
        )

        self.o_proj = Array[T](
            config.num_hidden_layers, config.hidden_size, config.hidden_size
        )

        self.o_proj.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.o_proj.num_elements())
        )

        self.post_layernorm = Array[T](config.hidden_size, config.num_hidden_layers)
        self.post_layernorm.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.post_layernorm.num_elements())
        )

        self.w1 = Array[T](
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w1.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.w1.num_elements())
        )

        self.w2 = Array[T](
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w2.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.w2.num_elements())
        )

        self.w3 = Array[T](
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w3.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.w3.num_elements())
        )

        self.norm = Array[T](config.hidden_size)
        self.norm.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.norm.num_elements())
        )

        self.fcr = Array[T](config.max_position_embeddings, config.head_dims // 2)
        self.fcr.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.fcr.num_elements())
        )

        self.fci = Array[T](config.max_position_embeddings, config.head_dims // 2)
        self.fci.set_data_from_buffer(
            buf.read_numerical_value_dynamic[T](self.fci.num_elements())
        )
        self.lm_head = Array[T](config.vocab_size, config.hidden_size)
        if shared_weights:
            self.lm_head.set_data_from_buffer(self.wte.data)
        else:
            self.lm_head.set_data_from_buffer(
                buf.read_numerical_value_dynamic[T](self.lm_head.num_elements())
            )


struct State[T: DType]:
    # State
    var hidden_state: Array[T]
    var ffd_out: Array[T]
    var x: Array[T]

    # Norms
    var input_layernorm: Array[T]
    var post_layernorm: Array[T]
    var norm: Array[T]

    # Attention Layer
    var k_cache: Array[T]
    var v_cache: Array[T]
    var q: Array[T]
    var k: Array[T]
    var v: Array[T]
    var o: Array[T]
    var attention: Array[T]

    # FeedForward
    var w1_C: Array[T]
    var w3_C: Array[T]

    # LM Head
    var logits: Array[T]

    # Number of cores
    var number_of_cores: Int

    fn __init__(inout self: Self, config: LlamaConfig):
        self.k_cache = Array[T](
            config.num_hidden_layers, config.max_position_embeddings, config.kv_dims
        )
        self.v_cache = Array[T](
            config.num_hidden_layers, config.max_position_embeddings, config.kv_dims
        )

        self.input_layernorm = Array[T](config.hidden_size)
        self.post_layernorm = Array[T](config.hidden_size)
        self.norm = Array[T](config.hidden_size)
        self.q = Array[T](1, config.num_attention_heads * config.head_dims)
        self.k = Array[T](1, config.num_key_value_heads * config.head_dims)
        self.v = Array[T](1, config.num_key_value_heads * config.head_dims)
        self.o = Array[T](1, config.hidden_size)
        self.attention = Array[T](
            config.num_attention_heads, config.max_position_embeddings
        )
        self.w1_C = Array[T](config.intermediate_size)
        self.w3_C = Array[T](config.intermediate_size)
        self.ffd_out = Array[T](config.hidden_size)
        self.hidden_state = Array[T](config.hidden_size)
        self.logits = Array[T](config.hidden_size, config.vocab_size)
        self.x = Array[T](config.hidden_size)

        self.x.alloc(0.0)

        self.v_cache.alloc(0.0)
        self.k_cache.alloc(0.0)

        self.attention.alloc(0.0)

        self.w1_C.alloc(0.0)
        self.w3_C.alloc(0.0)
        self.ffd_out.alloc(0.0)

        self.hidden_state.alloc(0.0)
        self.logits.alloc(0.0)

        self.q.alloc(0.0)
        self.o.alloc(0.0)

        self.number_of_cores = num_cores()


fn llama_forward[
    T: DType, nelts: Int = 1
](
    input_id: Int,
    position: Int,
    weights: LlamaWeights[T],
    inout state: State[T],
    config: LlamaConfig,
) raises:
    let PO = PointerOperation[T]()
    let q_size: Int = config.hidden_size * (
        config.num_attention_heads * config.head_dims
    )
    let o_size: Int = config.hidden_size * config.hidden_size
    let kv_size: Int = config.hidden_size * (
        config.num_key_value_heads * config.head_dims
    )
    let ffd_size: Int = config.hidden_size * config.intermediate_size

    var x: Array[T] = state.x
    let context_row: DTypePointer[T] = weights.wte.data.offset(
        input_id * config.hidden_size
    )

    memcpy[T](x.data, context_row, config.hidden_size)

    let fci_row = weights.fci.data.offset(position * config.head_dims // 2)
    let fcr_row = weights.fcr.data.offset(position * config.head_dims // 2)

    # config.num_hidden_layers

    for layer_index in range(config.num_hidden_layers):
        let pad: Int = layer_index * config.max_position_embeddings * config.kv_dims
        let inner_norm_pad: Int = layer_index * config.hidden_size
        rms_norm[T, nelts](
            state.hidden_state,
            x,  # X IS RESIDUAL
            Array[T](
                weights.input_layernorm.data.offset(inner_norm_pad), config.hidden_size
            ),
            1e-5,
        )

        matmul[T, nelts](
            state.q,
            Array[T](
                weights.q_proj.data.offset(layer_index * q_size),
                config.hidden_size,
                config.num_attention_heads * config.head_dims,
            ),
            state.hidden_state,
            state.number_of_cores,
        )
        # print("PASS Query Calculation")

        state.k.set_data_from_buffer(
            state.k_cache.data.offset(pad + position * config.kv_dims)
        )

        matmul[T, nelts](
            state.k,
            Array[T](
                weights.k_proj.data.offset(layer_index * kv_size),
                config.hidden_size,
                config.num_key_value_heads * config.head_dims,
            ),
            state.hidden_state,
            state.number_of_cores,
        )
        # print("PASS Key Calculation")
        state.v.set_data_from_buffer(
            state.v_cache.data.offset(pad + position * config.kv_dims)
        )

        matmul[T, nelts](
            state.v,
            Array[T](
                weights.v_proj.data.offset(layer_index * kv_size),
                config.hidden_size,
                config.num_key_value_heads * config.head_dims,
            ),
            state.hidden_state,
            state.number_of_cores,
        )
        # print("PASS Value Calculation")

        rope[T](
            q_array=state.q,
            k_array=state.k,
            fcr_row=fcr_row,
            fci_row=fci_row,
            num_attention_heads=config.num_attention_heads,
            num_key_value_heads=config.num_key_value_heads,
            head_dims=config.head_dims,
            num_cores=state.number_of_cores,
        )

        dot_product_attention[T, nelts](
            C=state.hidden_state,
            attention=state.attention,
            query=state.q,
            key_cache=state.k_cache,
            value_cache=state.v_cache,
            padding=pad,
            head_dims=config.head_dims,
            number_rep_kv=config.number_rep_kv,
            max_position_embeddings=config.max_position_embeddings,
            position=position,
            num_key_value_heads=config.num_key_value_heads,
            num_attention_heads=config.num_attention_heads,
            num_cores=state.number_of_cores,
        )

        matmul[T, nelts](
            state.o,
            Array[T](
                weights.o_proj.data.offset(layer_index * o_size),
                config.hidden_size,
                config.hidden_size,
            ),
            state.hidden_state,
            state.number_of_cores,
        )

        # APPLY RESIDUAL FOR ATTN(NORM(X)) + X
        PO.add(x.data, state.o.data, config.hidden_size)
        # FeedForward or MLP Layer

        rms_norm[T, nelts](
            state.hidden_state,
            x,  # X IS RESIDUAL
            Array[T](
                weights.post_layernorm.data.offset(inner_norm_pad), config.hidden_size
            ),
            1e-5,
        )

        matmul[T, nelts](
            state.w1_C,
            Array[T](
                weights.w1.data.offset(layer_index * ffd_size),
                config.intermediate_size,
                config.hidden_size,
            ),
            state.hidden_state,
            state.number_of_cores,
        )

        matmul[T, nelts](
            state.w3_C,
            Array[T](
                weights.w3.data.offset(layer_index * ffd_size),
                config.intermediate_size,
                config.hidden_size,
            ),
            state.hidden_state,
            state.number_of_cores,
        )

        @parameter
        fn _element_wise_(I: Int):
            let dt = state.w1_C[I]
            let data_ = (dt * (1.0 / (1.0 + math.exp(-dt)))) * state.w3_C[I]
            state.w1_C[I] = data_

        parallelize[_element_wise_](config.intermediate_size, state.number_of_cores)

        matmul[T, nelts](
            state.ffd_out,
            Array[T](
                weights.w2.data.offset(layer_index * ffd_size),
                config.hidden_size,
                config.intermediate_size,
            ),
            state.w1_C,
            state.number_of_cores,
        )
        # APPLY RESIDUAL FOR MLP(NORM(X)) + X

        PO.add(x.data, state.ffd_out.data, config.hidden_size)

    rms_norm[T, nelts](x, x, Array[T](weights.norm.data, config.hidden_size), 1e-5)
    matmul[T, nelts](
        state.logits,
        Array[T](weights.lm_head.data, config.vocab_size, config.hidden_size),
        x,
        state.number_of_cores,
    )
