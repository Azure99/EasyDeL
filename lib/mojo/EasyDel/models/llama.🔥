from math import math
from ..matrix_ import *
from runtime.llcl import Runtime, num_cores
from python import Python as Py
from memory.memory import memset_zero


struct LlamaConfig:
    var hidden_size: Int
    var num_attention_heads: Int
    var num_hidden_layers: Int
    var vocab_size: Int
    var rms_norm_eps: Float32
    var number_rep_kv: Int
    var max_position_embeddings: Int
    var num_key_value_heads: Int
    var head_dims: Int
    var kv_dims: Int
    var intermediate_size: Int

    fn __init__(
        inout self: Self,
        hidden_size: Int,
        num_attention_heads: Int,
        num_hidden_layers: Int,
        vocab_size: Int,
        rms_norm_eps: Float32,
        number_rep_kv: Int,
        max_position_embeddings: Int,
        num_key_value_heads: Int,
        intermediate_size: Int,
    ) -> None:
        self.hidden_size = hidden_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.number_rep_kv = number_rep_kv
        self.num_key_value_heads = num_key_value_heads
        self.intermediate_size = intermediate_size
        self.head_dims = hidden_size // num_attention_heads
        self.kv_dims = hidden_size // num_key_value_heads

    fn __init__(inout self: Self) -> None:
        self.hidden_size = 512
        self.intermediate_size = self.hidden_size * 4
        self.max_position_embeddings = 2048
        self.num_attention_heads = 8
        self.num_hidden_layers = 8
        self.vocab_size = 32000
        self.rms_norm_eps = 1e-5
        self.number_rep_kv = 1
        self.num_key_value_heads = 1
        self.intermediate_size = self.hidden_size * 4
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = self.hidden_size // self.num_key_value_heads

    fn __init__(inout self: Self, inout buffer: FileBuffer) raises -> None:
        self.hidden_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.intermediate_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_hidden_layers = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_attention_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_key_value_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.vocab_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.max_position_embeddings = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = (
            self.num_key_value_heads * self.hidden_size
        ) // self.num_attention_heads
        self.number_rep_kv = self.num_attention_heads // self.num_attention_heads
        self.rms_norm_eps = 1e-5
        return None


struct LlamaRunStateF32:
    var x: MatrixF32
    var x_normed: MatrixF32
    var x_buffer: MatrixF32
    var ffn_w1: MatrixF32
    var ffn_w3: MatrixF32
    var q: MatrixF32
    var k: MatrixF32
    var v: MatrixF32
    var attn_weights: MatrixF32
    var logits: MatrixF32
    var key_cache: MatrixF32
    var value_cache: MatrixF32
    var rt: Runtime

    fn __init__(inout self, config: LlamaConfig):
        self.x = MatrixF32(config.hidden_size)
        self.x.alloc_zero()
        self.x_normed = MatrixF32(config.hidden_size)
        self.x_normed.alloc_zero()
        self.x_buffer = MatrixF32(config.hidden_size)
        self.x_buffer.alloc_zero()
        self.ffn_w1 = MatrixF32(config.intermediate_size)
        self.ffn_w1.alloc_zero()
        self.ffn_w3 = MatrixF32(config.intermediate_size)
        self.ffn_w3.alloc_zero()
        self.q = MatrixF32(config.hidden_size)
        self.q.alloc_zero()
        self.k = MatrixF32(0, 0)
        self.v = MatrixF32(0, 0)
        self.attn_weights = MatrixF32(
            config.max_position_embeddings, config.num_attention_heads
        )
        self.attn_weights.alloc_zero()
        self.logits = MatrixF32(config.vocab_size)
        self.logits.alloc_zero()
        self.key_cache = MatrixF32(
            config.num_hidden_layers, config.kv_dims, config.max_position_embeddings
        )
        self.key_cache.alloc_zero()
        self.value_cache = MatrixF32(
            config.num_hidden_layers, config.kv_dims, config.max_position_embeddings
        )
        self.value_cache.alloc_zero()
        self.rt = Runtime(num_cores() // 2)


struct LlamaRunStateBF16:
    var x: MatrixBF16
    var x_normed: MatrixBF16
    var x_buffer: MatrixBF16
    var ffn_w1: MatrixBF16
    var ffn_w3: MatrixBF16
    var q: MatrixBF16
    var k: MatrixBF16
    var v: MatrixBF16
    var attn_weights: MatrixBF16
    var logits: MatrixBF16
    var key_cache: MatrixBF16
    var value_cache: MatrixBF16
    var rt: Runtime

    fn __init__(inout self, config: LlamaConfig):
        self.x = MatrixBF16(config.hidden_size)
        self.x.alloc_zero()
        self.x_normed = MatrixBF16(config.hidden_size)
        self.x_normed.alloc_zero()
        self.x_buffer = MatrixBF16(config.hidden_size)
        self.x_buffer.alloc_zero()
        self.ffn_w1 = MatrixBF16(config.intermediate_size)
        self.ffn_w1.alloc_zero()
        self.ffn_w3 = MatrixBF16(config.intermediate_size)
        self.ffn_w3.alloc_zero()
        self.q = MatrixBF16(config.hidden_size)
        self.q.alloc_zero()
        self.k = MatrixBF16(0, 0)
        self.v = MatrixBF16(0, 0)
        self.attn_weights = MatrixBF16(
            config.max_position_embeddings, config.num_attention_heads
        )
        self.attn_weights.alloc_zero()
        self.logits = MatrixBF16(config.vocab_size)
        self.logits.alloc_zero()
        self.key_cache = MatrixBF16(
            config.num_hidden_layers, config.kv_dims, config.max_position_embeddings
        )
        self.key_cache.alloc_zero()
        self.value_cache = MatrixBF16(
            config.num_hidden_layers, config.kv_dims, config.max_position_embeddings
        )
        self.value_cache.alloc_zero()
        self.rt = Runtime(num_cores() // 2)


struct LlamaWeightsF32:
    var wte: MatrixF32
    var fcr: MatrixF32
    var fci: MatrixF32
    var input_norm_attn_weigth: MatrixF32
    var wq: MatrixF32
    var wk: MatrixF32
    var wv: MatrixF32
    var wo: MatrixF32
    var post_norm_weigth: MatrixF32
    var w1: MatrixF32
    var w3: MatrixF32
    var w2: MatrixF32
    var final_norm_weight: MatrixF32
    var lm_head: MatrixF32

    fn __init__(
        inout self, config: LlamaConfig, shared_weights: Bool, inout buf: FileBuffer
    ) raises:
        self.wte = MatrixF32(config.hidden_size, config.vocab_size)
        self.wte.set_data_from_buffer(buf.read_value_float32(self.wte.size()))

        self.input_norm_attn_weigth = MatrixF32(
            config.hidden_size, config.num_hidden_layers
        )
        self.input_norm_attn_weigth.set_data_from_buffer(
            buf.read_value_float32(self.input_norm_attn_weigth.size())
        )
        self.wq = MatrixF32(
            config.num_hidden_layers, config.hidden_size, config.hidden_size
        )
        self.wq.set_data_from_buffer(buf.read_value_float32(self.wq.size()))
        self.wk = MatrixF32(
            config.num_hidden_layers, config.kv_dims, config.hidden_size
        )
        self.wk.set_data_from_buffer(buf.read_value_float32(self.wk.size()))
        self.wv = MatrixF32(
            config.num_hidden_layers, config.kv_dims, config.hidden_size
        )
        self.wv.set_data_from_buffer(buf.read_value_float32(self.wv.size()))
        self.wo = MatrixF32(
            config.num_hidden_layers, config.hidden_size, config.hidden_size
        )
        self.wo.set_data_from_buffer(buf.read_value_float32(self.wo.size()))
        self.post_norm_weigth = MatrixF32(config.hidden_size, config.num_hidden_layers)
        self.post_norm_weigth.set_data_from_buffer(
            buf.read_value_float32(self.post_norm_weigth.size())
        )
        self.w1 = MatrixF32(
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w1.set_data_from_buffer(buf.read_value_float32(self.w1.size()))
        self.w2 = MatrixF32(
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w2.set_data_from_buffer(buf.read_value_float32(self.w2.size()))
        self.w3 = MatrixF32(
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w3.set_data_from_buffer(buf.read_value_float32(self.w3.size()))
        self.final_norm_weight = MatrixF32(config.hidden_size)
        self.final_norm_weight.set_data_from_buffer(
            buf.read_value_float32(self.final_norm_weight.size())
        )
        self.fcr = MatrixF32(
            config.max_position_embeddings,
            (config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fcr.set_data_from_buffer(buf.read_value_float32(self.fcr.size()))
        self.fci = MatrixF32(
            config.max_position_embeddings,
            (config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fci.set_data_from_buffer(buf.read_value_float32(self.fci.size()))
        self.lm_head = MatrixF32(config.vocab_size, config.hidden_size)
        self.lm_head.set_data_from_buffer(
            self.wte.data if shared_weights else buf.read_value_float32(
                self.lm_head.size()
            )
        )


struct LlamaWeightsBF16:
    var wte: MatrixBF16
    var fcr: MatrixBF16
    var fci: MatrixBF16
    var input_norm_attn_weigth: MatrixBF16
    var wq: MatrixBF16
    var wk: MatrixBF16
    var wv: MatrixBF16
    var wo: MatrixBF16
    var post_norm_weigth: MatrixBF16
    var w1: MatrixBF16
    var w3: MatrixBF16
    var w2: MatrixBF16
    var final_norm_weight: MatrixBF16
    var lm_head: MatrixBF16

    fn __init__(
        inout self, config: LlamaConfig, shared_weights: Bool, inout buf: FileBuffer
    ) raises:
        self.wte = MatrixBF16(config.hidden_size, config.vocab_size)
        self.wte.set_data_from_buffer(buf.read_value_bfloat16(self.wte.size()))

        self.input_norm_attn_weigth = MatrixBF16(
            config.hidden_size, config.num_hidden_layers
        )
        self.input_norm_attn_weigth.set_data_from_buffer(
            buf.read_value_bfloat16(self.input_norm_attn_weigth.size())
        )
        self.wq = MatrixBF16(
            config.num_hidden_layers, config.hidden_size, config.hidden_size
        )
        self.wq.set_data_from_buffer(buf.read_value_bfloat16(self.wq.size()))
        self.wk = MatrixBF16(
            config.num_hidden_layers, config.kv_dims, config.hidden_size
        )
        self.wk.set_data_from_buffer(buf.read_value_bfloat16(self.wk.size()))
        self.wv = MatrixBF16(
            config.num_hidden_layers, config.kv_dims, config.hidden_size
        )
        self.wv.set_data_from_buffer(buf.read_value_bfloat16(self.wv.size()))
        self.wo = MatrixBF16(
            config.num_hidden_layers, config.hidden_size, config.hidden_size
        )
        self.wo.set_data_from_buffer(buf.read_value_bfloat16(self.wo.size()))
        self.post_norm_weigth = MatrixBF16(config.hidden_size, config.num_hidden_layers)
        self.post_norm_weigth.set_data_from_buffer(
            buf.read_value_bfloat16(self.post_norm_weigth.size())
        )
        self.w1 = MatrixBF16(
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w1.set_data_from_buffer(buf.read_value_bfloat16(self.w1.size()))
        self.w2 = MatrixBF16(
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w2.set_data_from_buffer(buf.read_value_bfloat16(self.w2.size()))
        self.w3 = MatrixBF16(
            config.num_hidden_layers, config.hidden_size, config.intermediate_size
        )
        self.w3.set_data_from_buffer(buf.read_value_bfloat16(self.w3.size()))
        self.final_norm_weight = MatrixBF16(config.hidden_size)
        self.final_norm_weight.set_data_from_buffer(
            buf.read_value_bfloat16(self.final_norm_weight.size())
        )
        self.fcr = MatrixBF16(
            config.max_position_embeddings,
            (config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fcr.set_data_from_buffer(buf.read_value_bfloat16(self.fcr.size()))
        self.fci = MatrixBF16(
            config.max_position_embeddings,
            (config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fci.set_data_from_buffer(buf.read_value_bfloat16(self.fci.size()))
        self.lm_head = MatrixBF16(config.vocab_size, config.hidden_size)
        self.lm_head.set_data_from_buffer(
            self.wte.data if shared_weights else buf.read_value_bfloat16(
                self.lm_head.size()
            )
        )


fn read_file[T: DType](inout buffer: FileBuffer, filename: StringRef) raises -> None:
    let _os = Py.import_module("os")
    let size: Int = atol(_os.path.getsize(filename).to_string())
    buffer.size = size
    let f = File(filename)
    let state = DTypePointer[DType.uint8].alloc(size)
    var reader = BufReader[4096](f ^)
    var bytes_read: Int = 1
    var offset = 0
    while bytes_read > 0:
        let buf = Buffer[4096, DType.uint8](state.offset(offset))
        bytes_read = reader.read(buf)
        offset += bytes_read
    buffer.data = state
    buffer.offset = 0
    return None
