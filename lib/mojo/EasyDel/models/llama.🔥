from math import math
from ..matrix_ import *
from runtime.llcl import Runtime, num_cores
from python import Python as Py
from memory.memory import memset_zero


struct LlamaConfig:
    var hidden_size: Int
    var num_attention_heads: Int
    var num_hidden_layers: Int
    var vocab_size: Int
    var rms_norm_eps: Float32
    var number_rep_kv: Int
    var max_position_embeddings: Int
    var num_key_value_heads: Int
    var head_dims: Int
    var kv_dims: Int
    var intermediate_size: Int

    fn __init__(
        inout self: Self,
        hidden_size: Int,
        num_attention_heads: Int,
        num_hidden_layers: Int,
        vocab_size: Int,
        rms_norm_eps: Float32,
        number_rep_kv: Int,
        max_position_embeddings: Int,
        num_key_value_heads: Int,
        intermediate_size: Int,
    ) -> None:
        self.hidden_size = hidden_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.number_rep_kv = number_rep_kv
        self.num_key_value_heads = num_key_value_heads
        self.intermediate_size = intermediate_size
        self.head_dims = hidden_size // num_attention_heads
        self.kv_dims = hidden_size // num_key_value_heads

    fn __init__(inout self: Self) -> None:
        self.hidden_size = 512
        self.intermediate_size = self.hidden_size * 4
        self.max_position_embeddings = 2048
        self.num_attention_heads = 8
        self.num_hidden_layers = 8
        self.vocab_size = 32000
        self.rms_norm_eps = 1e-5
        self.number_rep_kv = 1
        self.num_key_value_heads = 1
        self.intermediate_size = self.hidden_size * 4
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = self.hidden_size // self.num_key_value_heads

    fn __init__(inout self: Self, inout buffer: FileBuffer) raises -> None:
        self.hidden_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.intermediate_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_hidden_layers = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_attention_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.num_key_value_heads = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.vocab_size = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.max_position_embeddings = (
            buffer.data.offset(buffer.offset).bitcast[DType.uint32]().load(0).to_int()
        )
        buffer.move_offset(sizeof[DType.uint32]())
        self.head_dims = self.hidden_size // self.num_attention_heads
        self.kv_dims = (
            self.num_key_value_heads * self.hidden_size
        ) // self.num_attention_heads
        self.number_rep_kv = self.num_attention_heads // self.num_attention_heads
        self.rms_norm_eps = 1e-5
        return None


struct LlamaRunStateF32:
    var x: MatrixF32
    var x_normed: MatrixF32
    var x_buffer: MatrixF32
    var ffn_w1: MatrixF32
    var ffn_w3: MatrixF32
    var q: MatrixF32
    var k: MatrixF32
    var v: MatrixF32
    var attn_weights: MatrixF32
    var logits: MatrixF32
    var key_cache: MatrixF32
    var value_cache: MatrixF32
    var rt: Runtime

    fn __init__(inout self, config: LlamaConfig):
        self.x = MatrixF32(cols=config.hidden_size)
        self.x.alloc_zero()
        self.x_normed = MatrixF32(cols=config.hidden_size)
        self.x_normed.alloc_zero()
        self.x_buffer = MatrixF32(cols=config.hidden_size)
        self.x_buffer.alloc_zero()
        self.ffn_w1 = MatrixF32(cols=config.intermediate_size)
        self.ffn_w1.alloc_zero()
        self.ffn_w3 = MatrixF32(cols=config.intermediate_size)
        self.ffn_w3.alloc_zero()
        self.q = MatrixF32(cols=config.hidden_size)
        self.q.alloc_zero()
        self.k = MatrixF32(0, 0)
        self.v = MatrixF32(0, 0)
        self.attn_weights = MatrixF32(
            cols=config.max_position_embeddings, rows=config.num_attention_heads
        )
        self.attn_weights.alloc_zero()
        self.logits = MatrixF32(cols=config.vocab_size)
        self.logits.alloc_zero()
        self.key_cache = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.kv_dims,
            rows=config.max_position_embeddings,
        )
        self.key_cache.alloc_zero()
        self.value_cache = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.kv_dims,
            rows=config.max_position_embeddings,
        )
        self.value_cache.alloc_zero()
        self.rt = Runtime(num_cores() // 2)


struct LlamaRunStateBF16:
    var x: MatrixBF16
    var x_normed: MatrixBF16
    var x_buffer: MatrixBF16
    var ffn_w1: MatrixBF16
    var ffn_w3: MatrixBF16
    var q: MatrixBF16
    var k: MatrixBF16
    var v: MatrixBF16
    var attn_weights: MatrixBF16
    var logits: MatrixBF16
    var key_cache: MatrixBF16
    var value_cache: MatrixBF16
    var rt: Runtime

    fn __init__(inout self, config: LlamaConfig):
        self.x = MatrixBF16(cols=config.hidden_size)
        self.x.alloc_zero()
        self.x_normed = MatrixBF16(cols=config.hidden_size)
        self.x_normed.alloc_zero()
        self.x_buffer = MatrixBF16(cols=config.hidden_size)
        self.x_buffer.alloc_zero()
        self.ffn_w1 = MatrixBF16(cols=config.intermediate_size)
        self.ffn_w1.alloc_zero()
        self.ffn_w3 = MatrixBF16(cols=config.intermediate_size)
        self.ffn_w3.alloc_zero()
        self.q = MatrixBF16(cols=config.hidden_size)
        self.q.alloc_zero()
        self.k = MatrixBF16(0, 0)
        self.v = MatrixBF16(0, 0)
        self.attn_weights = MatrixBF16(
            cols=config.max_position_embeddings, rows=config.num_attention_heads
        )
        self.attn_weights.alloc_zero()
        self.logits = MatrixBF16(cols=config.vocab_size)
        self.logits.alloc_zero()
        self.key_cache = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.kv_dims,
            rows=config.max_position_embeddings,
        )
        self.key_cache.alloc_zero()
        self.value_cache = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.kv_dims,
            rows=config.max_position_embeddings,
        )
        self.value_cache.alloc_zero()
        self.rt = Runtime(num_cores() // 2)


struct LlamaWeightsF32:
    var wte: MatrixF32
    var fcr: MatrixF32
    var fci: MatrixF32
    var input_norm_attn_weigth: MatrixF32
    var wq: MatrixF32
    var wk: MatrixF32
    var wv: MatrixF32
    var wo: MatrixF32
    var post_norm_weigth: MatrixF32
    var w1: MatrixF32
    var w3: MatrixF32
    var w2: MatrixF32
    var final_norm_weight: MatrixF32
    var lm_head: MatrixF32

    fn __init__(
        inout self, config: LlamaConfig, shared_weights: Bool, inout buf: FileBuffer
    ) raises:
        self.wte = MatrixF32(cols=config.hidden_size, rows=config.vocab_size)
        self.wte.set_data_from_buffer(buf.read_value_float32(self.wte.size()))

        self.input_norm_attn_weigth = MatrixF32(
            cols=config.hidden_size, rows=config.num_hidden_layers
        )
        self.input_norm_attn_weigth.set_data_from_buffer(
            buf.read_value_float32(self.input_norm_attn_weigth.size())
        )
        self.wq = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.hidden_size,
        )
        self.wq.set_data_from_buffer(buf.read_value_float32(self.wq.size()))
        self.wk = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.kv_dims,
            rows=config.hidden_size,
        )
        self.wk.set_data_from_buffer(buf.read_value_float32(self.wk.size()))
        self.wv = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.kv_dims,
            rows=config.hidden_size,
        )
        self.wv.set_data_from_buffer(buf.read_value_float32(self.wv.size()))
        self.wo = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.hidden_size,
        )
        self.wo.set_data_from_buffer(buf.read_value_float32(self.wo.size()))
        self.post_norm_weigth = MatrixF32(
            cols=config.hidden_size, rows=config.num_hidden_layers
        )
        self.post_norm_weigth.set_data_from_buffer(
            buf.read_value_float32(self.post_norm_weigth.size())
        )
        self.w1 = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.intermediate_size,
        )
        self.w1.set_data_from_buffer(buf.read_value_float32(self.w1.size()))
        self.w2 = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.intermediate_size,
        )
        self.w2.set_data_from_buffer(buf.read_value_float32(self.w2.size()))
        self.w3 = MatrixF32(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.intermediate_size,
        )
        self.w3.set_data_from_buffer(buf.read_value_float32(self.w3.size()))
        self.final_norm_weight = MatrixF32(cols=config.hidden_size)
        self.final_norm_weight.set_data_from_buffer(
            buf.read_value_float32(self.final_norm_weight.size())
        )
        self.fcr = MatrixF32(
            cols=config.max_position_embeddings,
            rows=(config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fcr.set_data_from_buffer(buf.read_value_float32(self.fcr.size()))
        self.fci = MatrixF32(
            cols=config.max_position_embeddings,
            rows=(config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fci.set_data_from_buffer(buf.read_value_float32(self.fci.size()))
        self.lm_head = MatrixF32(cols=config.vocab_size, rows=config.hidden_size)
        self.lm_head.set_data_from_buffer(
            self.wte.data if shared_weights else buf.read_value_float32(
                self.lm_head.size()
            )
        )


struct LlamaWeightsBF16:
    var wte: MatrixBF16
    var fcr: MatrixBF16
    var fci: MatrixBF16
    var input_norm_attn_weigth: MatrixBF16
    var wq: MatrixBF16
    var wk: MatrixBF16
    var wv: MatrixBF16
    var wo: MatrixBF16
    var post_norm_weigth: MatrixBF16
    var w1: MatrixBF16
    var w3: MatrixBF16
    var w2: MatrixBF16
    var final_norm_weight: MatrixBF16
    var lm_head: MatrixBF16

    fn __init__(
        inout self, config: LlamaConfig, shared_weights: Bool, inout buf: FileBuffer
    ) raises:
        self.wte = MatrixBF16(cols=config.hidden_size, rows=config.vocab_size)
        self.wte.set_data_from_buffer(buf.read_value_bfloat16(self.wte.size()))

        self.input_norm_attn_weigth = MatrixBF16(
            cols=config.hidden_size, rows=config.num_hidden_layers
        )
        self.input_norm_attn_weigth.set_data_from_buffer(
            buf.read_value_bfloat16(self.input_norm_attn_weigth.size())
        )
        self.wq = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.hidden_size,
        )
        self.wq.set_data_from_buffer(buf.read_value_bfloat16(self.wq.size()))
        self.wk = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.kv_dims,
            rows=config.hidden_size,
        )
        self.wk.set_data_from_buffer(buf.read_value_bfloat16(self.wk.size()))
        self.wv = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.kv_dims,
            rows=config.hidden_size,
        )
        self.wv.set_data_from_buffer(buf.read_value_bfloat16(self.wv.size()))
        self.wo = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.hidden_size,
        )
        self.wo.set_data_from_buffer(buf.read_value_bfloat16(self.wo.size()))
        self.post_norm_weigth = MatrixBF16(
            cols=config.hidden_size, rows=config.num_hidden_layers
        )
        self.post_norm_weigth.set_data_from_buffer(
            buf.read_value_bfloat16(self.post_norm_weigth.size())
        )
        self.w1 = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.intermediate_size,
        )
        self.w1.set_data_from_buffer(buf.read_value_bfloat16(self.w1.size()))
        self.w2 = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.intermediate_size,
        )
        self.w2.set_data_from_buffer(buf.read_value_bfloat16(self.w2.size()))
        self.w3 = MatrixBF16(
            layers=config.num_hidden_layers,
            cols=config.hidden_size,
            rows=config.intermediate_size,
        )
        self.w3.set_data_from_buffer(buf.read_value_bfloat16(self.w3.size()))
        self.final_norm_weight = MatrixBF16(cols=config.hidden_size)
        self.final_norm_weight.set_data_from_buffer(
            buf.read_value_bfloat16(self.final_norm_weight.size())
        )
        self.fcr = MatrixBF16(
            cols=config.max_position_embeddings,
            rows=(config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fcr.set_data_from_buffer(buf.read_value_bfloat16(self.fcr.size()))
        self.fci = MatrixBF16(
            cols=config.max_position_embeddings,
            rows=(config.hidden_size // config.num_attention_heads) // 2,
        )
        self.fci.set_data_from_buffer(buf.read_value_bfloat16(self.fci.size()))
        self.lm_head = MatrixBF16(cols=config.vocab_size, rows=config.hidden_size)
        self.lm_head.set_data_from_buffer(
            self.wte.data if shared_weights else buf.read_value_bfloat16(
                self.lm_head.size()
            )
        )


fn read_file[T: DType](inout buffer: FileBuffer, filename: StringRef) raises -> None:
    let _os = Py.import_module("os")
    let size: Int = atol(_os.path.getsize(filename).to_string())
    buffer.size = size
    let f = File(filename)
    let state = DTypePointer[DType.uint8].alloc(size)
    var reader = BufReader[4096](f ^)
    var bytes_read: Int = 1
    var offset = 0
    while bytes_read > 0:
        let buf = Buffer[4096, DType.uint8](state.offset(offset))
        bytes_read = reader.read(buf)
        offset += bytes_read
    buffer.data = state
    buffer.offset = 0
    return None

@always_inline
fn llama_forward_fp32[
    rope_rotation: fn ( 
        inout q: DTypePointer[DType.float32],
        inout k: DTypePointer[DType.float32],
        fcr_row: DTypePointer[DType.float32],
        fci_row: DTypePointer[DType.float32],
        num_attention_heads: Int,
        num_key_values_head: Int,
        head_dims: Int
    ) -> None,
    matmul:fn (
        inout r:MatrixF32,
        a:MatrixF32,
        b:MatrixF32,
        rt:Runtime
    )->None,
    nelts:Int
](
    input_id: Int,
    position: Int,
    config: LlamaConfig,
    inout state: LlamaRunStateF32,
    weights: LlamaWeightsF32,
) -> None:
    var x = state.x.data
    let hidden_size = config.hidden_size
    let intermediate_size = config.intermediate_size
    let kv_dims = config.kv_dims
    let number_rep_kv = config.number_rep_kv
    let head_dims = config.head_dims

    var matrix_state = MatrixF32(0, 0)
    let content_row = weights.wte.data.offset(input_id * hidden_size)
    memcpy[DType.float32](x, content_row, config.hidden_size)

    let freq_cis_real_row = weights.fcr.data.offset(position * head_dims // 2)
    let freq_cis_imag_row = weights.fci.data.offset(position * head_dims // 2)

    for l in range(config.num_hidden_layers):
        rms_norm[nelts](state.x_normed.data, x, weights.input_norm_attn_weigth.data.offset(l * hidden_size), config.rms_norm_eps ,hidden_size,state.rt)

        matrix_state.set_data_from_buffer(weights.wq.data.offset(l * hidden_size * hidden_size), rows=hidden_size, cols=hidden_size)
        matmul(state.q, state.x_normed, matrix_state, state.rt)

        let loff = l * config.max_position_embeddings * kv_dims
        state.k.set_data_from_buffer(state.key_cache.data.offset(loff + position * kv_dims), rows=1, cols=kv_dims)
        matrix_state.set_data_from_buffer(weights.wk.data.offset(l * hidden_size * kv_dims), rows=kv_dims, cols=hidden_size)
        matmul(state.k, state.x_normed, matrix_state, state.rt)

        state.v.set_data_from_buffer(
            state.value_cache.data.offset(loff + position * kv_dims), rows=1, cols=kv_dims
        )
        matrix_state.set_data_from_buffer(weights.wv.data.offset(l * hidden_size * kv_dims), rows=kv_dims, cols=hidden_size)
        matmul(state.v, state.x_normed, matrix_state, state.rt)

        rope_rotation(state.q.data,state.k.data, freq_cis_real_row, freq_cis_imag_row, config.num_attention_heads,config.num_key_value_heads,config.head_dims)

        for h in range(config.num_attention_heads):
            let q = state.q.data.offset(h * head_dims)

            var attn_weights = state.attn_weights.data.offset(h * config.max_position_embeddings)

            for t in range(position + 1):
                let k = state.key_cache.data.offset(
                    loff + t * kv_dims + (h // number_rep_kv) * head_dims
                )
                var score: Float32 = 0.0
                for i in range(head_dims):
                    score += q.offset(i).load(0) * k.offset(i).load(0)
                score /= math.sqrt[DType.float32, 1](head_dims)

                attn_weights.offset(t).store(0, score)

            softmax[nelts](attn_weights, position + 1)

            let x_normed = state.x_normed.data.offset(h * head_dims)
            memset_zero(x_normed, head_dims)
            for t in range(position + 1):
                let v = state.value_cache.data.offset(
                    loff + t * kv_dims + (h // number_rep_kv) * head_dims
                )
                let a = attn_weights.offset(t).load(0)
                for i in range(head_dims):
                    let xbi = x_normed.offset(i).load(0) + a * v.offset(i).load(0)
                    x_normed.offset(i).store(0, xbi)
        matrix_state.set_data_from_buffer(weights.wo.data.offset(l * hidden_size * hidden_size), rows=hidden_size,cols=hidden_size)
        matmul(state.x_buffer, state.x_normed, matrix_state, state.rt)

        add_pointers[nelts](x, state.x_buffer.data, hidden_size)

        rms_norm[nelts](state.x_normed.data, x, weights.post_norm_weigth.data.offset(l * hidden_size),config.rms_norm_eps, hidden_size,state.rt)

        matrix_state.set_data_from_buffer(weights.w1.data.offset(l * hidden_size * intermediate_size), rows=intermediate_size, cols=hidden_size)
        matmul(state.ffn_w1, state.x_normed, matrix_state, state.rt)

        matrix_state.set_data_from_buffer(weights.w3.data.offset(l * hidden_size * intermediate_size), rows=intermediate_size, cols=hidden_size)
        matmul(state.ffn_w3, state.x_normed, matrix_state, state.rt)

        for i in range(intermediate_size):
            let hbi = state.ffn_w1[i]
            state.ffn_w1[i] = hbi * (1.0 / (1.0 + math.exp(-hbi)))

        for i in range(intermediate_size):
            state.ffn_w1[i] = state.ffn_w1[i] * state.ffn_w3[i]

        matrix_state.set_data_from_buffer(weights.w2.data.offset(l * hidden_size * intermediate_size), rows=hidden_size, cols=intermediate_size)
        matmul(state.x_normed, state.ffn_w1, matrix_state, state.rt)

        add_pointers[nelts](x, state.x_normed.data, hidden_size)

    rms_norm[nelts](x, x, weights.final_norm_weight.data,config.rms_norm_eps, hidden_size,state.rt)

    matrix_state.set_data_from_buffer(weights.lm_head.data, rows=config.vocab_size,cols= hidden_size)
    matmul(state.logits, state.x, matrix_state, state.rt)

