from ..array import Array, matmul
from algorithm.functional import parallelize, vectorize
import math


@always_inline
fn linear[
    DT: DType, nelts: Int, cores: Int, parallelized: Bool
](inout output: Array[DT], input: Array[DT], weight: Array[DT]):
    matmul[DT, nelts, cores, parallelized](output, input, weight)


@always_inline
fn linear[DT: DType, cores: Int](input: Array[DT], weight: Array[DT]) -> Array[DT]:
    var output: Array[DT] = Array[DT](input, weight)
    linear[DT, output.nelts, cores, True](output, input, weight)
    return output


@always_inline
fn embedding[
    DT: DType, nelts: Int, cores: Int
](input_ids: VariadicList[Int], weight: Array[DT]) -> Array[DT]:
    let last_dim: Int = weight.dim(-1)
    let batch: Int = len(input_ids)

    var embed: Array[DT] = Array[DT](batch, last_dim)
    embed.alloc(0.0)

    @parameter
    fn _row(i: Int):
        let axis: Int = input_ids[i]

        @parameter
        fn _cols[_nelts: Int](j: Int):
            embed.store[_nelts](VariadicList[Int](i, j), weight.load[_nelts](axis, j))

        vectorize[nelts, _cols](last_dim)

    parallelize[_row](batch, cores)
    return embed


@always_inline
fn embedding[
    DT: DType, nelts: Int, cores: Int
](input_ids: DynamicVector[Int], weight: Array[DT]) -> Array[DT]:
    let last_dim: Int = weight.dim(-1)
    let batch: Int = len(input_ids)

    var embed: Array[DT] = Array[DT](batch, last_dim)
    embed.alloc(0.0)

    @parameter
    fn _row(i: Int):
        let axis: Int = input_ids[i]

        @parameter
        fn _cols[_nelts: Int](j: Int):
            embed.store[_nelts](VariadicList[Int](i, j), weight.load[_nelts](axis, j))

        vectorize[nelts, _cols](last_dim)

    parallelize[_row](batch, cores)
    return embed


@always_inline
fn rms_layernorm_3d[
    DT: DType, nelts: Int, cores: Int
](inout res: Array[DT], input: Array[DT], weight: Array[DT], epsilon: SIMD[DT, 1]):
    let num_elements: Int = input.dim(-1)
    let batch: Int = input.num_elements() // (input.dim(-1) * input.dim(-2))
    for b in range(batch):

        @parameter
        fn _parallelizer(i: Int):
            var sp: SIMD[DT, nelts] = SIMD[DT, nelts](0.0)

            @parameter
            fn _do_sum_pow[_nelts: Int](j: Int):
                if _nelts < nelts:
                    sp[0] += (input.load[_nelts](b, i, j) ** 2).reduce_add()
                else:
                    sp += input.load[nelts](b, i, j) ** 2

            vectorize[nelts, _do_sum_pow](num_elements)
            var normed: SIMD[DT, 1] = sp.reduce_add()
            normed = normed / num_elements + epsilon
            normed = 1.0 / math.sqrt(normed)

            @parameter
            fn _do_element_wise[_nelts: Int](j: Int):
                let val = weight.load[_nelts](j) * normed * input.load[_nelts](b, i, j)
                res.store[_nelts](VariadicList[Int](b, i, j), val)

            vectorize[nelts, _do_element_wise](num_elements)

        parallelize[_parallelizer](input.dim(-2), cores)


@always_inline
fn rms_layernorm_3d[
    DT: DType, nelts: Int, cores: Int
](input: Array[DT], weight: Array[DT], epsilon: SIMD[DT, 1]) -> Array[DT]:
    var res: Array[DT] = Array[DT](input)
    res.alloc(0.0)
    rms_layernorm_3d[DT, nelts, cores](res, input, weight, epsilon)
    return res


@always_inline
fn rms_layernorm_2d[
    DT: DType, nelts: Int, cores: Int
](inout res: Array[DT], input: Array[DT], weight: Array[DT], epsilon: SIMD[DT, 1]):
    let num_elements: Int = input.dim(-1)

    @parameter
    fn _parallelizer(i: Int):
        var sp: SIMD[DT, nelts] = SIMD[DT, nelts](0.0)

        @parameter
        fn _do_sum_pow[_nelts: Int](j: Int):
            if _nelts < nelts:
                sp[0] += (input.load[_nelts](i, j) ** 2).reduce_add()
            else:
                sp += input.load[nelts](i, j) ** 2

        vectorize[nelts, _do_sum_pow](num_elements)
        var normed: SIMD[DT, 1] = sp.reduce_add()
        normed = normed / num_elements + epsilon
        normed = 1.0 / math.sqrt(normed)

        @parameter
        fn _do_element_wise[_nelts: Int](j: Int):
            let val = weight.load[_nelts](j) * normed * input.load[_nelts](i, j)
            res.store[_nelts](VariadicList[Int](i, j), val)

        vectorize[nelts, _do_element_wise](num_elements)

    parallelize[_parallelizer](input.dim(-2), cores)


@always_inline
fn rms_layernorm_2d[
    DT: DType, nelts: Int, cores: Int
](input: Array[DT], weight: Array[DT], epsilon: SIMD[DT, 1]) -> Array[DT]:
    var res: Array[DT] = Array[DT](input)
    res.alloc(0.0)
    rms_layernorm_2d[DT, nelts, cores](res, input, weight, epsilon)
    return res


@always_inline
fn rms_layernorm_1d[
    DT: DType, nelts: Int, cores: Int
](inout res: Array[DT], input: Array[DT], weight: Array[DT], epsilon: SIMD[DT, 1]):
    let num_elements: Int = input.dim(-1)

    var sp: SIMD[DT, nelts] = SIMD[DT, nelts](0.0)

    @parameter
    fn _do_sum_pow[_nelts: Int](j: Int):
        if _nelts < nelts:
            sp[0] += (input.load[_nelts]( j) ** 2).reduce_add()
        else:
            sp += input.load[nelts](j) ** 2

    vectorize[nelts, _do_sum_pow](num_elements)
    var normed: SIMD[DT, 1] = sp.reduce_add()
    normed = normed / num_elements + epsilon
    normed = 1.0 / math.sqrt(normed)

    @parameter
    fn _do_element_wise[_nelts: Int](j: Int):
        let val = weight.load[_nelts](j) * normed * input.load[_nelts](j)
        res.store[_nelts](VariadicList[Int](j), val)

    vectorize[nelts, _do_element_wise](num_elements)



@always_inline
fn rms_layernorm_1d[
    DT: DType, nelts: Int, cores: Int
](input: Array[DT], weight: Array[DT], epsilon: SIMD[DT, 1]) -> Array[DT]:
    var res: Array[DT] = Array[DT](input.dim(-1))
    res.alloc(0.0)
    rms_layernorm_2d[DT, nelts, cores](res, input, weight, epsilon)
    return res


struct Linear[DT: DType, nelts: Int, cores: Int, parallelized: Bool]:
    var weight: Array[DT]
    var bias: Array[DT]
    var has_bias: Bool

    fn __init__(inout self: Self, weight: Array[DT], bias: Array[DT]):
        self.weight = weight
        self.bias = bias
        self.has_bias = True

    fn __init__(inout self: Self, weight: Array[DT]):
        self.weight = weight
        self.bias = Array[DT](0)
        self.has_bias = False

    fn __call__(self: Self, x: Array[DT]) -> Array[DT]:
        var out = matmul[DT, nelts, cores, parallelized](x, self.weight)
        if self.has_bias:
            out = out + self.bias
        return out


struct Embedding[DT: DType, nelts: Int, cores: Int]:
    var weight: Array[DT]

    fn __init__(inout self: Self, weight: Array[DT]):
        self.weight = weight

    fn __call__(self: Self, input_ids: VariadicList[Int]) -> Array[DT]:
        let embed = embedding[DT, nelts, cores](input_ids, self.weight)
        return embed


struct RMSLayernorm[DT: DType, nelts: Int, cores: Int]:
    var weight: Array[DT]
    var epsilon: SIMD[DT, 1]

    fn __init__(inout self: Self, weight: Array[DT], epsilon: SIMD[DT, 1]):
        self.weight = weight
        self.epsilon = epsilon

    fn __call__(self: Self, x: Array[DT]) -> Array[DT]:
        let output: Array[DT] = rms_layernorm_2d[DT, nelts, cores](
            x, self.weight, self.epsilon
        )

        return output
