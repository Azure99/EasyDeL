from ..array import Array, ArrayShape, rotate_half, matmul, softmax
from algorithm.functional import vectorize, parallelize
from math import math
from memory import memset_zero


fn rope[
    DT: DType, nelts: Int, cores: Int
](array: Array[DT], cos: Array[DT], sin: Array[DT], position: Int) -> Array[DT]:
    let head_dim = array.dim(-1)
    let chunk_cos: Array[DT] = Array[DT](cos.data.offset(position * head_dim), head_dim)
    let chunk_sin: Array[DT] = Array[DT](sin.data.offset(position * head_dim), head_dim)

    let _r: Array[DT] = rotate_half[DT, nelts, cores](array)
    var _embed: Array[DT] = Array[DT](array.array_shape)
    let _row_size: Int = array.num_elements() // head_dim

    _embed.alloc(0.0)

    @parameter
    fn _row(i: Int):
        @parameter
        fn _cols[_nelts: Int](j: Int):
            let xbi: SIMD[DT, _nelts] = (
                array.load[_nelts](i, 0, j) * chunk_cos.load[_nelts](j)
            ) + (_r.load[_nelts](i, 0, j) * chunk_sin.load[_nelts](j))
            _embed.store[_nelts](VariadicList[Int](i, 0, j), xbi)

        vectorize[nelts, _cols](head_dim)

    parallelize[_row](_row_size, cores)
    return _embed


fn rope[
    DT: DType, nelts: Int, cores: Int
](array: Array[DT], cos: Array[DT], sin: Array[DT], position_ids: Array[DT]) -> Array[
    DT
]:
    r"""
    Takes Array Like
    Shape(number attention heads, sequence length, head dim).
    """

    let batch: Int = array.dim(0)
    let seq_len: Int = array.dim(-2)
    let head_dim: Int = array.dim(-1)
    let num_heads: Int = array.dim(-3)
    let st0 = array.strides[0]
    let st1 = array.strides[1]
    let st2 = array.strides[2]
    let st3 = array.strides[3]
    let num_position_ids: Int = position_ids.__len__()
    let _r: Array[DT] = rotate_half[DT, nelts, cores](array)

    if seq_len != position_ids.__len__():
        print("K/Q Seq Length don't match with Position IDS.")

    var chunk_cos: Array[DT] = Array[DT](seq_len, head_dim)
    var chunk_sin: Array[DT] = Array[DT](seq_len, head_dim)
    var embed: Array[DT] = Array[DT](array)

    chunk_cos.alloc(0.0)
    chunk_sin.alloc(0.0)
    embed.alloc(0.0)

    let pst0 = chunk_cos.strides[0]

    @parameter
    fn _rows(position: Int):
        let position_id: Int = position_ids[position].to_int()

        @parameter
        fn _cols[_nelts: Int](i: Int):
            let index = position * head_dim + i
            chunk_cos.store[_nelts](index, cos.load[_nelts](position_id, i))
            chunk_sin.store[_nelts](index, sin.load[_nelts](position_id, i))

        vectorize[nelts, _cols](head_dim)

    parallelize[_rows](num_position_ids)

    for b in range(batch):
        for p in range(num_position_ids):
            for h in range(num_heads):
                for d in range(head_dim):
                    let index: VariadicList[Int] = VariadicList[Int](b, h, p, d)
                    let x_re: SIMD[DT, 1] = (
                        array.load[1](index) * chunk_cos.load[1](p, d)
                    ) + (_r.load[1](index) * chunk_sin.load[1](p, d))
                    embed.store[1](index, x_re)

    return embed


@always_inline
fn dot_product_attention_weights[
    DT: DType, nelts: Int
](
    attention: Array[DT],
    query: Array[DT],
    key_cache: Array[DT],
    head_dims: Int,
    number_rep_kv: Int,
    max_position_embeddings: Int,
    position: Int,
    num_key_value_heads: Int,
    num_attention_heads: Int,
    padding: Int = 0,
    cores: Int = 1,
):
    r"""
    Calculates Q@K/Sqrt(HeadDims).
    """

    @parameter
    fn _calculate_each_head(head_index: Int):
        let q_offset = head_index * head_dims
        let attn_offset = head_index * max_position_embeddings
        let kv_dims: Int = num_key_value_heads * head_dims
        var C_AT = attention.data.offset(attn_offset)
        for position_index in range(position + 1):
            let k_offset = padding + position_index * kv_dims + (
                head_index // number_rep_kv
            ) * head_dims
            var score: SIMD[DT, nelts] = SIMD[DT, nelts](0.0)

            @parameter
            fn _qk[_nelts: Int](idx: Int):
                if _nelts < nelts:
                    score[0] += (
                        query.data.simd_load[_nelts](q_offset + idx)
                        * key_cache.data.simd_load[_nelts](k_offset + idx)
                    ).reduce_add()
                else:
                    score += query.data.simd_load[nelts](
                        q_offset + idx
                    ) * key_cache.data.simd_load[nelts](k_offset + idx)

            vectorize[nelts, _qk](head_dims)
            score /= math.sqrt[DT, nelts](head_dims)

            C_AT.offset(position_index).store(0, score.reduce_add())
        softmax[DT, nelts](C_AT, position + 1)

    parallelize[_calculate_each_head](num_attention_heads, cores)


@always_inline
fn dot_product_attention_weights[
    DT: DType, nelts: Int, cores: Int
](inout query: Array[DT], inout key: Array[DT]) -> Array[DT]:
    r"""
    Takes in
    q Shape(sequence length, num_attention_heads, head_dims)
    k Shape(sequence length, num_key_value_heads, head_dims)
    Calculates Q@K/Sqrt(HeadDims).
    """
    let head_dims: Int = key.dim(-1)
    key = key.T[nelts, cores]()

    var A: Array[DT] = Array[DT](query, key)
    matmul[DT, nelts, cores](A, query, key)
    let SH: SIMD[DT, nelts] = SIMD[DT, nelts](math.sqrt(head_dims))

    @parameter
    fn _row[_nelts: Int](i: Int):
        if _nelts < nelts:
            let res = A.load[nelts](i).reduce_add() / SH.reduce_add()
            A.store[1](i, res)
        else:
            let res = A.load[nelts](i) / SH
            A.store[nelts](i, res)

    vectorize[nelts, _row](A.num_elements())
    A = softmax[DT, nelts, cores](A, -1)
    return A ^


@always_inline
fn dot_product_attention[
    DT: DType, nelts: Int, cores: Int
](inout query: Array[DT], inout key: Array[DT], inout value: Array[DT],) -> Array[DT]:
    r"""
    Takes In
    q Shape(sequence length, num_attention_heads, head_dims)
    k Shape(sequence length, num_key_value_heads, head_dims)
    v Shape(sequence length, num_key_value_heads, head_dims)
    Calculates (Q@K/Sqrt(HeadDims))@V.
    """
    ...
    var attention_weight: Array[DT] = dot_product_attention_weights[DT, nelts, cores](
        query, key
    )
    var attention: Array[DT] = Array[DT](value, attention_weight)
    matmul[DT, nelts, cores](attention, value, attention_weight)
    return attention ^


fn repeat_kv[DT: DType, nelts: Int, cores: Int](A: Array[DT], times: Int) -> Array[DT]:
    let A0: Int = A.dim(0)
    let A1: Int = A.dim(1)
    let A2: Int = A.dim(2)
    let A3: Int = A.dim(3)
    var B: Array[DT] = Array[DT](A0, A1, times, A2, A3)
    B.alloc(0.0)
    for b in range(A0):

        @parameter
        fn _row(i: Int):
            for j in range(times):
                for k in range(A2):

                    @parameter
                    fn _cols[_nelts: Int](l: Int):
                        B.store[_nelts](
                            VariadicList[Int](b, i, j, k, l), A.load[_nelts](b, i, k, l)
                        )

                    vectorize[nelts, _cols](A3)

        parallelize[_row](A1, cores)
    B.view(A0, -1, A2, A3)
    return B ^


fn qkv_tranpose[DT: DType, nelts: Int, cores: Int](owned array: Array[DT]) -> Array[DT]:
    var transposed_array: Array[DT] = Array[DT](
        array.dim(0), array.dim(2), array.dim(1), array.dim(3)
    )
    transposed_array.alloc()
    for i in range(array.dim(0)):

        @parameter
        fn _rows(j: Int):
            for k in range(array.dim(1)):

                @parameter
                fn _cols[_nelts: Int](l: Int):
                    transposed_array.store[_nelts](
                        VariadicList[Int](i, j, k, l), array.load[_nelts](i, k, j, l)
                    )

                vectorize[nelts, _cols](array.dim(3))

        parallelize[_rows](array.dim(2))
    return transposed_array


fn collect_index[DT: DType, nelts: Int](A: Array[DT], axis: Int) -> Array[DT]:
    var B: Array[DT] = Array[DT](1, A.dim(-1))

    B.alloc()

    @parameter
    fn cols[_nelts: Int](i: Int):
        B.store[_nelts](i, A.load[_nelts](axis, i))

    vectorize[nelts, cols](B.num_elements())
    return B
