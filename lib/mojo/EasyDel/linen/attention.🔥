from ..array import Array, ArrayShape, rotate_half, CAT_3D_AXIS_2
from algorithm.functional import vectorize, parallelize
from math import math
from memory import memset_zero


@always_inline
fn rms_norm[
    T: DType, nelts: Int
](
    inout C: Array[T],
    x: Array[T],
    w: Array[T],
    epsilon: SIMD[T, 1],
    num_elements: Int = 0,
) -> None:
    var sp: SIMD[T, nelts] = SIMD[T, nelts](0.0)
    let nml: Int = num_elements if num_elements > 0 else x.num_elements()

    @parameter
    fn _do_sum_pow[_nelts: Int](size: Int):
        if _nelts < nelts:
            sp[0] += (x.load[_nelts](size) ** 2).reduce_add()
        else:
            sp += x.load[nelts](size) ** 2

    vectorize[nelts, _do_sum_pow](nml)
    var normed: SIMD[T, 1] = sp.reduce_add()
    normed = normed / nml + epsilon
    normed = 1.0 / math.sqrt(normed)

    @parameter
    fn _do_element_wise[_nelts: Int](j: Int):
        let val = w.data.simd_load[_nelts](j) * normed * x.data.simd_load[_nelts](j)
        C.data.offset(j).simd_store[_nelts](0, val)

    vectorize[nelts, _do_element_wise](nml)


fn rope[
    T: DType
](
    inout q_array: Array[T],
    inout k_array: Array[T],
    fcr_row: DTypePointer[T],
    fci_row: DTypePointer[T],
    num_attention_heads: Int,
    num_key_value_heads: Int,
    head_dims: Int,
    num_cores: Int = 1,
) -> None:
    @parameter
    fn _row(i: Int):
        for j in range(0, head_dims, 2):
            let fcr = fcr_row.offset(j // 2).load(0)
            let fci = fci_row.offset(j // 2).load(0)
            let q0 = q_array.data.offset(i * head_dims + j).load(0)
            let q1 = q_array.data.offset(i * head_dims + j + 1).load(0)
            q_array.data.offset(i * head_dims + j).store(0, q0 * fcr - q1 * fci)
            q_array.data.offset(i * head_dims + j + 1).store(0, q0 * fci + q1 * fcr)
            if i < num_key_value_heads:
                let k0 = k_array.data.offset(i * head_dims + j).load(0)
                let k1 = k_array.data.offset(i * head_dims + j + 1).load(0)
                k_array.data.offset(i * head_dims + j).store(0, k0 * fcr - k1 * fci)
                k_array.data.offset(i * head_dims + j + 1).store(0, k0 * fci + k1 * fcr)

    parallelize[_row](num_attention_heads, num_cores)


fn q_rope[
    T: DType, num_cores: Int
](q: Array[T], cos: Array[T], sin: Array[T], position: Int,) -> Array[T]:
    let head_dim = q.dim(-1)
    let chunk_cos: Array[T] = Array[T](cos.data.offset(position * head_dim), head_dim)
    let chunk_sin: Array[T] = Array[T](sin.data.offset(position * head_dim), head_dim)

    let q_r: Array[T] = rotate_half[T, num_cores](q)
    var q_embed: Array[T] = Array[T](q.array_shape)
    let q_row_size: Int = q.num_elements() // head_dim

    q_embed.alloc(0.0)

    @parameter
    fn _row_q(i: Int):
        @parameter
        fn _cols_q[nelts: Int](j: Int):
            let xbi: SIMD[T, nelts] = (
                q.load[nelts](i, 0, j) * chunk_cos.load[nelts](j)
            ) + (q_r.load[nelts](i, 0, j) * chunk_sin.load[nelts](j))
            q_embed.store[nelts](i, 0, j, xbi)

        vectorize[Array[T].nelts, _cols_q](head_dim)

    parallelize[_row_q](q_row_size)
    return q_embed


fn k_rope[
    T: DType, num_cores: Int
](k: Array[T], cos: Array[T], sin: Array[T], position: Int) -> Array[T]:
    let head_dim = k.dim(-1)
    let chunk_cos: Array[T] = Array[T](cos.data.offset(position * head_dim), head_dim)
    let chunk_sin: Array[T] = Array[T](sin.data.offset(position * head_dim), head_dim)

    let k_r: Array[T] = rotate_half[T, num_cores](k)
    let k_row_size: Int = k.num_elements() // head_dim
    var k_embed: Array[T] = Array[T](k.array_shape)

    k_embed.alloc(0.0)

    @parameter
    fn _row_k(i: Int):
        @parameter
        fn _cols_k[nelts: Int](j: Int):
            let xbi: SIMD[T, nelts] = (
                k.load[nelts](i, 0, j) * chunk_cos.load[nelts](j)
            ) + (k_r.load[nelts](i, 0, j) * chunk_sin.load[nelts](j))
            k_embed.store[nelts](i, 0, j, xbi)

        vectorize[Array[T].nelts, _cols_k](head_dim)

    parallelize[_row_k](k_row_size)

    return k_embed


fn torch_rope[
    T: DType, num_cores: Int
](
    q: Array[T],
    k: Array[T],
    cos: Array[T],
    sin: Array[T],
    position: Int,
) -> Tuple[
    Array[T], Array[T]
]:
    debug_assert(k.rank() != 3 or q.rank() != 3, "Wrong input Shape for Q,K")
    let head_dim = q.dim(-1)

    let chunk_cos: Array[T] = Array[T](cos.data.offset(position * head_dim), head_dim)
    let chunk_sin: Array[T] = Array[T](sin.data.offset(position * head_dim), head_dim)

    var q_embed: Array[T] = Array[T](q.array_shape)
    var k_embed: Array[T] = Array[T](k.array_shape)

    k_embed.alloc(0.0)
    q_embed.alloc(0.0)

    let q_r: Array[T] = rotate_half[T, num_cores](q)
    let k_r: Array[T] = rotate_half[T, num_cores](k)

    let q_row_size: Int = q.num_elements() // head_dim
    let k_row_size: Int = k.num_elements() // head_dim

    @parameter
    fn _row_q(i: Int):
        @parameter
        fn _cols_q[nelts: Int](j: Int):
            let xbi: SIMD[T, nelts] = (
                q.load[nelts](i, 1, j) * chunk_cos.load[nelts](j)
            ) + (q_r.load[nelts](i, 1, j) * chunk_sin.load[nelts](j))
            q_embed.store[nelts](i, 1, j, xbi)

        vectorize[Array[T].nelts, _cols_q](head_dim)

    @parameter
    fn _row_k(i: Int):
        @parameter
        fn _cols_k[nelts: Int](j: Int):
            let xbi: SIMD[T, nelts] = (
                k.load[nelts](i, 1, j) * chunk_cos.load[nelts](j)
            ) + (k_r.load[nelts](i, 1, j) * chunk_sin.load[nelts](j))
            k_embed.store[nelts](i, 1, j, xbi)

        vectorize[Array[T].nelts, _cols_k](head_dim)

    parallelize[_row_q](q_row_size)
    parallelize[_row_k](k_row_size)
    return q_embed, k_embed


@always_inline
fn dot_product_attention_weights[
    T: DType, nelts: Int
](
    attention: Array[T],
    query: Array[T],
    key_cache: Array[T],
    head_dims: Int,
    number_rep_kv: Int,
    max_position_embeddings: Int,
    position: Int,
    num_key_value_heads: Int,
    num_attention_heads: Int,
    padding: Int = 0,
    num_cores: Int = 1,
):
    r"""
    Calculates Q@K/Sqrt(HeadDims).
    """

    @parameter
    fn _calculate_each_head(head_index: Int):
        let q_offset = head_index * head_dims
        let attn_offset = head_index * max_position_embeddings
        let kv_dims: Int = num_key_value_heads * head_dims
        var C_AT = attention.data.offset(attn_offset)
        for position_index in range(position + 1):
            let k_offset = padding + position_index * kv_dims + (
                head_index // number_rep_kv
            ) * head_dims
            var score: SIMD[T, nelts] = SIMD[T, nelts](0.0)

            @parameter
            fn _qk[_nelts: Int](idx: Int):
                if _nelts < nelts:
                    score[0] += (
                        query.data.simd_load[_nelts](q_offset + idx)
                        * key_cache.data.simd_load[_nelts](k_offset + idx)
                    ).reduce_add()
                else:
                    score += query.data.simd_load[nelts](
                        q_offset + idx
                    ) * key_cache.data.simd_load[nelts](k_offset + idx)

            vectorize[nelts, _qk](head_dims)
            score /= math.sqrt[T, nelts](head_dims)

            C_AT.offset(position_index).store(0, score.reduce_add())
        softmax[T, nelts](C_AT, position + 1)

    parallelize[_calculate_each_head](num_attention_heads, num_cores)


@always_inline
fn dot_product_attention_weights[
    T: DType, workers: Int
](
    inout q: Array[T],
    inout k: Array[T],
    num_attention_heads: Int,
    num_key_value_heads: Int,
    head_dims: Int,
) -> Array[T]:
    r"""
    Calculates Q@K/Sqrt(HeadDims).
    """
    alias nelts: Int = q.nelts
    k = T_AXIS_0_2_1[T, nelts](k, workers)

    var A: Array[T] = Array[T](q, k)
    matmul[T, nelts](A, q, k, workers)
    let SH: SIMD[T, nelts] = SIMD[T, nelts](math.sqrt(head_dims))

    @parameter
    fn _row[_nelts: Int](i: Int):
        if _nelts < nelts:
            let res = A.load[nelts](i).reduce_add() / SH.reduce_add()
            A.store[1](i, res)
        else:
            let res = A.load[nelts](i) / SH
            A.store[nelts](i, res)

    vectorize[nelts, _row](A.num_elements())
    softmax[T, nelts](A)
    return A


@always_inline
fn dot_product_attention[
    T: DType, nelts: Int
](
    inout C: Array[T],
    inout attention: Array[T],
    query: Array[T],
    key_cache: Array[T],
    value_cache: Array[T],
    head_dims: Int,
    number_rep_kv: Int,
    max_position_embeddings: Int,
    position: Int,
    num_key_value_heads: Int,
    num_attention_heads: Int,
    padding: Int = 0,
    num_cores: Int = 1,
) -> NoneType:
    r"""
    Calculates (Q@K/Sqrt(HeadDims))@V.
    """

    @parameter
    fn _calculate_each_head(head_index: Int):
        let q_offset = head_index * head_dims
        let attn_offset = head_index * max_position_embeddings
        let kv_dims: Int = num_key_value_heads * head_dims
        var C_AT = attention.data.offset(attn_offset)
        for position_index in range(position + 1):
            let k_offset = padding + position_index * kv_dims + (
                head_index // number_rep_kv
            ) * head_dims
            var score: SIMD[T, nelts] = SIMD[T, nelts](0.0)

            @parameter
            fn _qk[_nelts: Int](idx: Int):
                if _nelts < nelts:
                    score[0] += (
                        query.data.simd_load[_nelts](q_offset + idx)
                        * key_cache.data.simd_load[_nelts](k_offset + idx)
                    ).reduce_add()
                else:
                    score += query.data.simd_load[nelts](
                        q_offset + idx
                    ) * key_cache.data.simd_load[nelts](k_offset + idx)

            vectorize[nelts, _qk](head_dims)
            score /= math.sqrt[T, nelts](head_dims)

            C_AT.offset(position_index).store(0, score.reduce_add())
        softmax[T, nelts](C_AT, position + 1)

        let H_S = C.data.offset(head_index * head_dims)
        memset_zero(H_S, head_dims)
        for position_index in range(position + 1):
            let v_offset: Int = padding + position_index * kv_dims + (
                head_index // number_rep_kv
            ) * head_dims
            let v = value_cache.data.offset(v_offset)
            let A = C_AT.offset(position_index).load(0)

            @parameter
            fn _wv[_nelts: Int](i: Int):
                let xbi = H_S.simd_load[_nelts](i) + A * v.simd_load[_nelts](i)
                H_S.simd_store[_nelts](i, xbi)

            vectorize[nelts, _wv](head_dims)

    parallelize[_calculate_each_head](num_attention_heads, num_cores)


fn repeat_kv[T: DType, cores: Int](A: Array[T], times: Int) -> Array[T]:
    var B: Array[T] = Array[T](A.dim(-3), times, A.dim(-2), A.dim(-1))
    B.alloc(0.0)
    let A1: Int = A.dim(0)
    let A2: Int = A.dim(1)
    let A3: Int = A.dim(2)

    @parameter
    fn _row(i: Int):
        for j in range(times):
            for k in range(A2):

                @parameter
                fn _cols[nelts: Int](l: Int):
                    B.store[nelts](i, j, k, l, A.load[nelts](i, k, l))

                vectorize[Array[T].nelts, _cols](A3)

    parallelize[_row](A1, cores)
    B.view(-1, A.dim(-2), A.dim(-1))
    return B ^
