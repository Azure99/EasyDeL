from ..array import Array, ArrayShape, rotate_half, CAT_3D_AXIS_2, matmul, softmax
from algorithm.functional import vectorize, parallelize
from math import math
from memory import memset_zero



fn rope[
    DT: DType, num_cores: Int
](array: Array[DT], cos: Array[DT], sin: Array[DT], position: Int,) -> Array[DT]:
    let head_dim = array.dim(-1)
    let chunk_cos: Array[DT] = Array[DT](cos.data.offset(position * head_dim), head_dim)
    let chunk_sin: Array[DT] = Array[DT](sin.data.offset(position * head_dim), head_dim)

    let _r: Array[DT] = rotate_half[DT, num_cores](array)
    var _embed: Array[DT] = Array[DT](array.array_shape)
    let _row_size: Int = array.num_elements() // head_dim

    _embed.alloc(0.0)

    @parameter
    fn _row(i: Int):
        @parameter
        fn _cols[nelts: Int](j: Int):
            let xbi: SIMD[DT, nelts] = (
                array.load[nelts](i, 0, j) * chunk_cos.load[nelts](j)
            ) + (_r.load[nelts](i, 0, j) * chunk_sin.load[nelts](j))
            _embed.store[nelts](VariadicList[Int](i, 0, j), xbi)

        vectorize[Array[DT].nelts, _cols](head_dim)

    parallelize[_row](_row_size)
    return _embed


fn rope[
    DT: DType, num_cores: Int
](array: Array[DT], cos: Array[DT], sin: Array[DT], position_ids: Array[DT]) -> Array[
    DT
]:
    """
    Takes Array Like
    Shape(number attention heads, sequence length, head dim).
    """
    let head_dim: Int = array.dim(-1)
    let seq_len: Int = array.dim(-2)
    let num_heads: Int = array.dim(-3)

    if seq_len != position_ids.__len__():
        print("K/Q Seq Length don't match with Position IDS.")

    var chunk_cos: Array[DT] = Array[DT](seq_len, head_dim)
    var chunk_sin: Array[DT] = Array[DT](seq_len, head_dim)
    chunk_cos.alloc(0.0)
    chunk_sin.alloc(0.0)

    @parameter
    fn _rows(position: Int):
        let position_id: Int = position_ids[position].to_int()

        @parameter
        fn _cols[_nelts: Int](i: Int):
            chunk_cos.store[_nelts](
                VariadicList[Int](position, i), cos.load[_nelts](position_id, i)
            )
            chunk_sin.store[_nelts](
                VariadicList[Int](position, i), sin.load[_nelts](position_id, i)
            )

        vectorize[array.nelts, _cols](head_dim)

    parallelize[_rows](position_ids.__len__())

    let _r: Array[DT] = rotate_half[DT, num_cores](array)
    var _embed: Array[DT] = Array[DT](array.array_shape)

    _embed.alloc(0.0)
    for seq_id in range(position_ids.__len__()):

        @parameter
        fn _row(i: Int):
            @parameter
            fn _cols[nelts: Int](j: Int):
                let xbi: SIMD[DT, nelts] = (
                    array.load[nelts](i, seq_id, j) * chunk_cos.load[nelts](seq_id, j)
                ) + (_r.load[nelts](i, seq_id, j) * chunk_sin.load[nelts](seq_id, j))
                _embed.store[nelts](VariadicList[Int](i, seq_id, j), xbi)

            vectorize[Array[DT].nelts, _cols](head_dim)

        parallelize[_row](num_heads)
    return _embed ^



@always_inline
fn dot_product_attention_weights[
    DT: DType, nelts: Int
](
    attention: Array[DT],
    query: Array[DT],
    key_cache: Array[DT],
    head_dims: Int,
    number_rep_kv: Int,
    max_position_embeddings: Int,
    position: Int,
    num_key_value_heads: Int,
    num_attention_heads: Int,
    padding: Int = 0,
    num_cores: Int = 1,
):
    r"""
    Calculates Q@K/Sqrt(HeadDims).
    """

    @parameter
    fn _calculate_each_head(head_index: Int):
        let q_offset = head_index * head_dims
        let attn_offset = head_index * max_position_embeddings
        let kv_dims: Int = num_key_value_heads * head_dims
        var C_AT = attention.data.offset(attn_offset)
        for position_index in range(position + 1):
            let k_offset = padding + position_index * kv_dims + (
                head_index // number_rep_kv
            ) * head_dims
            var score: SIMD[DT, nelts] = SIMD[DT, nelts](0.0)

            @parameter
            fn _qk[_nelts: Int](idx: Int):
                if _nelts < nelts:
                    score[0] += (
                        query.data.simd_load[_nelts](q_offset + idx)
                        * key_cache.data.simd_load[_nelts](k_offset + idx)
                    ).reduce_add()
                else:
                    score += query.data.simd_load[nelts](
                        q_offset + idx
                    ) * key_cache.data.simd_load[nelts](k_offset + idx)

            vectorize[nelts, _qk](head_dims)
            score /= math.sqrt[DT, nelts](head_dims)

            C_AT.offset(position_index).store(0, score.reduce_add())
        softmax[DT, nelts](C_AT, position + 1)

    parallelize[_calculate_each_head](num_attention_heads, num_cores)


@always_inline
fn dot_product_attention_weights[
    DT: DType, nelts: Int, cores: Int
](inout query: Array[DT], inout key: Array[DT]) -> Array[DT]:
    r"""
    Takes in
    q Shape(sequence length, num_attention_heads, head_dims)
    k Shape(sequence length, num_key_value_heads, head_dims)
    Calculates Q@K/Sqrt(HeadDims).
    """
    let head_dims: Int = key.dim(-1)
    key = key.T[nelts, cores]()

    var A: Array[DT] = Array[DT](query, key)
    matmul[DT, nelts, cores](A, query, key)
    let SH: SIMD[DT, nelts] = SIMD[DT, nelts](math.sqrt(head_dims))

    @parameter
    fn _row[_nelts: Int](i: Int):
        if _nelts < nelts:
            let res = A.load[nelts](i).reduce_add() / SH.reduce_add()
            A.store[1](i, res)
        else:
            let res = A.load[nelts](i) / SH
            A.store[nelts](i, res)

    vectorize[nelts, _row](A.num_elements())
    A = softmax[DT](A, -1)
    return A ^


@always_inline
fn dot_product_attention[
    DT: DType, nelts: Int, cores: Int
](inout query: Array[DT], inout key: Array[DT], inout value: Array[DT],) -> Array[DT]:
    r"""
    Takes In
    q Shape(sequence length, num_attention_heads, head_dims)
    k Shape(sequence length, num_key_value_heads, head_dims)
    v Shape(sequence length, num_key_value_heads, head_dims)
    Calculates (Q@K/Sqrt(HeadDims))@V.
    """
    ...
    var attention_weight: Array[DT] = dot_product_attention_weights[DT, nelts, cores](
        query, key
    )
    var attention: Array[DT] = Array[DT](value, attention_weight)
    matmul[DT, nelts, cores](attention, value, attention_weight)
    return attention ^


fn repeat_kv[DT: DType, cores: Int](A: Array[DT], times: Int) -> Array[DT]:
    var B: Array[DT] = Array[DT](A.dim(-3), times, A.dim(-2), A.dim(-1))
    B.alloc(0.0)
    let A1: Int = A.dim(0)
    let A2: Int = A.dim(1)
    let A3: Int = A.dim(2)

    @parameter
    fn _row(i: Int):
        for j in range(times):
            for k in range(A2):

                @parameter
                fn _cols[nelts: Int](l: Int):
                    B.store[nelts](
                        VariadicList[Int](i, j, k, l), A.load[nelts](i, k, l)
                    )

                vectorize[Array[DT].nelts, _cols](A3)

    parallelize[_row](A1, cores)
    B.view(-1, A.dim(-2), A.dim(-1))
    return B ^
