Index: lib/python/EasyDel/modules/gpt2/modelling_gpt2_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># coding=utf-8\n# Copyright 2021 The Google Flax Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Any, Optional, Tuple\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze\nfrom flax.linen import combine_masks, make_causal_mask\nfrom flax.linen.attention import dot_product_attention_weights\nfrom flax.traverse_util import flatten_dict, unflatten_dict\nfrom jax import lax\n\nfrom transformers.modeling_flax_outputs import (\n    FlaxBaseModelOutputWithPastAndCrossAttentions,\n    FlaxCausalLMOutputWithCrossAttentions,\n)\nfrom transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel\nfrom ..flax_modelling_utils import ACT2FN, create_mesh, JaxBaseClassModel, with_sharding_constraint, \\\n    get_dot_general_by_bits\nfrom .gpt2_configuration import GPT2Config\n\n_CHECKPOINT_FOR_DOC = \"gpt2\"\n_CONFIG_FOR_DOC = \"GPT2Config\"\n\n\nclass FlaxConv1D(nn.Module):\n    features: int\n    use_bias: bool = True\n    dtype: Any = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n    dot_general: Optional[None] = None\n\n    @nn.compact\n    def __call__(self, inputs):\n        inputs = jnp.asarray(inputs, self.dtype)\n        kernel = self.param(\"kernel\", jax.nn.initializers.normal(stddev=0.02), (self.features, inputs.shape[-1]))\n        kernel = jnp.asarray(kernel.transpose(), self.dtype)\n        if self.dot_general is not None:\n            dot_general = self.dot_general\n        else:\n            dot_general = lax.dot_general\n\n        y = dot_general(inputs, kernel, (((inputs.ndim - 1,), (0,)), ((), ())), precision=self.precision)\n        if self.use_bias:\n            bias = self.param(\"bias\", jax.nn.initializers.zeros, (self.features,))\n            bias = jnp.asarray(bias, self.dtype)\n            y = y + bias\n        return y\n\n\nclass FlaxGPT2Attention(nn.Module):\n    config: GPT2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n    causal: bool = True\n    is_cross_attention: bool = False\n\n    def setup(self):\n        config = self.config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n\n        if self.is_cross_attention:\n            self.c_attn = FlaxConv1D(\n                2 * self.embed_dim,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n            )\n            self.q_attn = FlaxConv1D(\n                self.embed_dim,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n            )\n        else:\n            self.c_attn = FlaxConv1D(\n                3 * self.embed_dim,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n            )\n        self.c_proj = FlaxConv1D(\n            self.embed_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.resid_dropout = nn.Dropout(rate=config.resid_pdrop)\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n            self,\n            hidden_states,\n            key_value_states: Optional[jnp.ndarray] = None,\n            attention_mask=None,\n            casual_mask=None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n    ):\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        batch_size = hidden_states.shape[0]\n\n        if not is_cross_attention:\n            qkv_out = self.c_attn(hidden_states)\n            query, key, value = jnp.split(qkv_out, 3, axis=2)\n        else:\n            q_out = self.q_attn(hidden_states)\n            (query,) = jnp.split(q_out, 1, axis=2)\n            kv_out = self.c_attn(key_value_states)\n            key, value = jnp.split(kv_out, 2, axis=2)\n\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n        if self.config.use_pjit_attention_force:\n            query = with_sharding_constraint(query, jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\"))\n            key = with_sharding_constraint(key, jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\"))\n            value = with_sharding_constraint(value, jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\"))\n        query_length, key_length = query.shape[1], key.shape[1]\n\n        if self.causal:\n            if self.has_variable(\"cache\", \"cached_key\"):\n                mask_shift = self.variables[\"cache\"][\"cache_index\"]\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n                causal_mask = lax.dynamic_slice(\n                    casual_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n                )\n            else:\n                causal_mask = casual_mask[:, :, :query_length, :key_length]\n            causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n\n        # combine masks if needed\n        if attention_mask is not None and self.causal:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n            attention_mask = combine_masks(attention_mask, causal_mask)\n        elif self.causal:\n            attention_mask = causal_mask\n        elif attention_mask is not None:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n\n        # transform boolean mask into float mask\n        if attention_mask is not None:\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n        else:\n            attention_bias = None\n\n        # usual dot product attention\n        attn_weights = dot_product_attention_weights(\n            query,\n            key,\n            bias=attention_bias,\n            dropout_rng=dropout_rng,\n            dropout_rate=self.config.attn_pdrop,\n            deterministic=deterministic,\n            dtype=self.dtype,\n            precision=None,\n        )\n        if self.config.use_pjit_attention_force:\n            attn_weights = with_sharding_constraint(attn_weights,\n                                                    jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"tp\", None, None))\n        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.c_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n\n        outputs = (attn_output, attn_weights) if output_attentions else (attn_output,)\n        return outputs\n\n\nclass FlaxGPT2MLP(nn.Module):\n    config: GPT2Config\n    intermediate_size: int\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self):\n        embed_dim = self.config.hidden_size\n        self.c_fc = FlaxConv1D(\n            self.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.c_proj = FlaxConv1D(\n            embed_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.act = ACT2FN[self.config.activation_function]\n        self.dropout = nn.Dropout(rate=self.config.resid_pdrop)\n\n    def __call__(self, hidden_states, deterministic: bool = True):\n        hidden_states = self.c_fc(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.c_proj(hidden_states)\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n        return hidden_states\n\n\nclass FlaxGPT2Block(nn.Module):\n    config: GPT2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self):\n        hidden_size = self.config.hidden_size\n        inner_dim = self.config.n_inner if self.config.n_inner is not None else 4 * hidden_size\n\n        self.ln_1 = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n        self.attn = FlaxGPT2Attention(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.ln_2 = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n\n        if self.config.add_cross_attention:\n            self.crossattention = FlaxGPT2Attention(\n                config=self.config, dtype=self.dtype, causal=False, is_cross_attention=True\n            )\n            self.ln_cross_attn = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n\n        self.mlp = FlaxGPT2MLP(self.config, inner_dim, dtype=self.dtype)\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            casual_mask=None,\n            encoder_hidden_states: Optional[jnp.ndarray] = None,\n            encoder_attention_mask: Optional[jnp.ndarray] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n    ):\n        residual = hidden_states\n        hidden_states = self.ln_1(hidden_states)\n        attn_outputs = self.attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            deterministic=deterministic,\n            casual_mask=casual_mask,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n        )\n        attn_output = attn_outputs[0]\n        outputs = attn_outputs[1:]\n        hidden_states = attn_output + residual\n        if encoder_hidden_states is not None:\n            if not hasattr(self, \"crossattention\"):\n                raise ValueError(\n                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n                )\n            residual = hidden_states\n            hidden_states = self.ln_cross_attn(hidden_states)\n            cross_attn_outputs = self.crossattention(\n                hidden_states,\n                key_value_states=encoder_hidden_states,\n                casual_mask=casual_mask,\n                attention_mask=encoder_attention_mask,\n                deterministic=deterministic,\n                output_attentions=output_attentions,\n            )\n            attn_output = cross_attn_outputs[0]\n            # residual connection\n            hidden_states = residual + attn_output\n            outputs = outputs + cross_attn_outputs[1:]  # add cross attentions if we output attention weights\n\n        residual = hidden_states\n        hidden_states = self.ln_2(hidden_states)\n        feed_forward_hidden_states = self.mlp(hidden_states, deterministic=deterministic)\n        # residual connection\n        hidden_states = residual + feed_forward_hidden_states\n\n        outputs = (hidden_states,) + outputs\n\n        return outputs\n\n\nclass FlaxGPT2PreTrainedModel(FlaxPreTrainedModel):\n    config_class = GPT2Config\n    base_model_prefix = \"transformer\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: GPT2Config,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            param_dtype: jnp.dtype = jnp.float32,\n            precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\"),\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        module = self.module_class(config=config, dtype=dtype, param_dtype=param_dtype, precision=precision, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(input_shape + (self.config.n_embd,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        # init input variables to retrieve cache\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return unfreeze(init_variables[\"cache\"])\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask=None,\n            position_ids=None,\n            encoder_hidden_states: Optional[jnp.ndarray] = None,\n            encoder_attention_mask: Optional[jnp.ndarray] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        if encoder_hidden_states is not None and encoder_attention_mask is None:\n            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n            encoder_attention_mask = jnp.ones((batch_size, sequence_length))\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params}\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            encoder_hidden_states,\n            encoder_attention_mask,\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxGPT2BlockCollection(nn.Module):\n    config: GPT2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self):\n        self.blocks = [\n            FlaxGPT2Block(\n                self.config,\n                name=str(i),\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision\n            ) for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            casual_mask=None,\n            encoder_hidden_states: Optional[jnp.ndarray] = None,\n            encoder_attention_mask: Optional[jnp.ndarray] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states,\n                attention_mask,\n                casual_mask=casual_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_attention_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n\n                if encoder_hidden_states is not None:\n                    all_cross_attentions += (layer_outputs[2],)\n\n        outputs = (hidden_states, all_hidden_states, all_attentions, all_cross_attentions)\n\n        return outputs\n\n\nclass FlaxGPT2Module(nn.Module):\n    config: GPT2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self):\n        self.embed_dim = self.config.hidden_size\n\n        self.wte = nn.Embed(\n            self.config.vocab_size,\n            self.embed_dim,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dtype=self.dtype,\n        )\n        self.wpe = nn.Embed(\n            self.config.max_position_embeddings,\n            self.embed_dim,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dtype=self.dtype,\n        )\n        self.casual_mask = make_causal_mask(\n            jnp.ones((1, self.config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\"\n        )\n        self.dropout = nn.Dropout(rate=self.config.embd_pdrop)\n        self.h = FlaxGPT2BlockCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.ln_f = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states: Optional[jnp.ndarray] = None,\n            encoder_attention_mask: Optional[jnp.ndarray] = None,\n            deterministic=True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        inputs_embeds = self.wte(input_ids.astype(\"i4\"))\n        position_embeds = self.wpe(position_ids.astype(\"i4\"))\n\n        hidden_states = inputs_embeds + position_embeds\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n\n        outputs = self.h(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            casual_mask=self.casual_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.ln_f(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[2],\n            cross_attentions=outputs[3],\n        )\n\n\nclass FlaxGPT2Model(FlaxGPT2PreTrainedModel):\n    module_class = FlaxGPT2Module\n\n\nclass FlaxGPT2LMHeadModule(nn.Module):\n    config: GPT2Config\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self):\n        self.transformer = FlaxGPT2Module(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states: Optional[jnp.ndarray] = None,\n            encoder_attention_mask: Optional[jnp.ndarray] = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        outputs = self.transformer(\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutputWithCrossAttentions(\n            logits=lm_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n\nclass FlaxGPT2LMHeadModel(FlaxGPT2PreTrainedModel):\n    module_class = FlaxGPT2LMHeadModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jax.Array] = None):\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask.astype(\"i4\"), (0, 0)\n            )\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/gpt2/modelling_gpt2_flax.py b/lib/python/EasyDel/modules/gpt2/modelling_gpt2_flax.py
--- a/lib/python/EasyDel/modules/gpt2/modelling_gpt2_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/gpt2/modelling_gpt2_flax.py	(date 1703667945163)
@@ -28,10 +28,10 @@
     FlaxBaseModelOutputWithPastAndCrossAttentions,
     FlaxCausalLMOutputWithCrossAttentions,
 )
-from transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel
-from ..flax_modelling_utils import ACT2FN, create_mesh, JaxBaseClassModel, with_sharding_constraint, \
-    get_dot_general_by_bits
+from ..flax_modelling_utils import ACT2FN, with_sharding_constraint, \
+    get_dot_general_by_bits, ACT2FN
 from .gpt2_configuration import GPT2Config
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 
 _CHECKPOINT_FOR_DOC = "gpt2"
 _CONFIG_FOR_DOC = "GPT2Config"
@@ -353,7 +353,7 @@
         return outputs
 
 
-class FlaxGPT2PreTrainedModel(FlaxPreTrainedModel):
+class FlaxGPT2PreTrainedModel(EasyDelFlaxPretrainedModel):
     config_class = GPT2Config
     base_model_prefix = "transformer"
     module_class: nn.Module = None
@@ -641,6 +641,12 @@
 class FlaxGPT2Model(FlaxGPT2PreTrainedModel):
     module_class = FlaxGPT2Module
 
+    def get_input_embeddings(self):
+        return self.module.wte
+
+    def set_input_embeddings(self, value):
+        self.module.wte = value
+
 
 class FlaxGPT2LMHeadModule(nn.Module):
     config: GPT2Config
@@ -713,6 +719,24 @@
 class FlaxGPT2LMHeadModel(FlaxGPT2PreTrainedModel):
     module_class = FlaxGPT2LMHeadModule
 
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
+    def get_decoder(self):
+        return self.module.transformer
+
+    def get_input_embeddings(self):
+        return self.module.transformer.wte
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
+    def set_decoder(self, decoder):
+        self.module.transformer = decoder
+
+    def set_input_embeddings(self, value):
+        self.module.transformer.wte = value
+
     def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jax.Array] = None):
         batch_size, seq_length = input_ids.shape
 
Index: lib/python/EasyDel/modules/phi/modelling_phi_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from dataclasses import field, dataclass\nfrom typing import Optional, Tuple, Any, Union, Dict, Sequence, Callable\n\nimport chex\nimport jax.lax\nimport transformers\nfrom flax.core import FrozenDict, freeze, unfreeze\nfrom flax.linen.normalization import _compute_stats, _canonicalize_axes\nfrom flax.traverse_util import unflatten_dict, flatten_dict\nfrom jax import numpy as jnp\nfrom flax import linen as nn\nfrom chex import Array\nfrom ..flax_modelling_utils import ACT2FN, get_gradient_checkpoint_policy, canonicalize_dtype\nfrom einops import repeat, rearrange\nfrom transformers.modeling_flax_outputs import FlaxCausalLMOutput\nfrom .phi_configuration import PhiConfig\n\n\n@dataclass\nclass InferenceParams:\n    max_seq_len: int = field(\n        metadata={\"help\": \"Maximum sequence length.\"}\n    )\n\n    max_batch_size: int = field(\n        metadata={\"help\": \"Maximum batch size.\"}\n    )\n\n    seq_len_offset: int = field(\n        default=0,\n        metadata={\"help\": \"Sequence length offset.\"}\n    )\n\n    batch_size_offset: int = field(\n        default=0,\n        metadata={\"help\": \"Batch size offset.\"}\n    )\n\n    key_value_memory_dict: Dict[str, Any] = field(\n        default_factory=dict,\n        metadata={\"help\": \"Key value memory dictionary.\"}\n    )\n\n    lengths_per_sample: Union[Array, None] = field(\n        default=None,\n        metadata={\"help\": \"Lengths per sample.\"}\n    )\n\n\ndef _normalize(\n        mdl: nn.Module,\n        x: Array,\n        mean: Array,\n        var: Array,\n        reduction_axes: int,\n        feature_axes: int,\n        dtype: chex.ArrayDType,\n        param_dtype: chex.ArrayDType,\n        epsilon: float,\n        use_bias: bool,\n        use_scale: bool,\n        bias_init: Callable[[jax.random.PRNGKey, chex.Shape, chex.ArrayDType], Array],\n        scale_init: Callable[[jax.random.PRNGKey, chex.Shape, chex.ArrayDType], Array],\n):\n    \"\"\"Normalizes the input of a normalization layer and optionally applies a learned scale and bias.\n  \n    Arguments:\n      mdl: Module to apply the normalization in (normalization params will reside\n        in this module).\n      x: The input.\n      mean: Mean to use for normalization.\n      var: Variance to use for normalization.\n      reduction_axes: The axes in ``x`` to reduce.\n      feature_axes: int containing features. A separate bias and scale is learned\n        for each specified feature.\n      dtype: The dtype of the result (default: infer from input and params).\n      param_dtype: The dtype of the parameters.\n      epsilon: Normalization epsilon.\n      use_bias: If true, add a bias term to the output.\n      use_scale: If true, scale the output.\n      bias_init: Initialization function for the bias term.\n      scale_init: Initialization function for the scaling function.\n  \n    Returns:\n      The normalized input.\n    \"\"\"\n    reduction_axes = _canonicalize_axes(x.ndim, reduction_axes)\n    feature_axes = _canonicalize_axes(x.ndim, feature_axes)\n    feature_shape = [1] * x.ndim\n    reduced_feature_shape = []\n    for ax in feature_axes:\n        feature_shape[ax] = x.shape[ax]\n        reduced_feature_shape.append(x.shape[ax])\n\n    mean = jnp.expand_dims(mean, reduction_axes)\n    var = jnp.expand_dims(var, reduction_axes)\n    y = x - mean\n    mul = jax.lax.rsqrt(var + epsilon)\n    args = [x]\n    if use_scale:\n        scale = mdl.param(\n            'weight', scale_init, reduced_feature_shape, param_dtype\n        ).reshape(feature_shape)\n        mul *= scale\n        args.append(scale)\n    y *= mul\n    if use_bias:\n        bias = mdl.param(\n            'bias', bias_init, reduced_feature_shape, param_dtype\n        ).reshape(feature_shape)\n        y += bias\n        args.append(bias)\n    dtype = canonicalize_dtype(*args, dtype=dtype)\n    return jnp.asarray(y, dtype)\n\n\nclass LayerNorm(nn.Module):\n    \"\"\"Layer normalization (https://arxiv.org/abs/1607.06450).\n  \n    LayerNorm normalizes the activations of the layer for each given example in a\n    batch independently, rather than across a batch like Batch Normalization.\n    i.e. applies a transformation that maintains the mean activation within\n    each example close to 0 and the activation standard deviation close to 1.\n  \n    Attributes:\n      epsilon: A small float added to variance to avoid dividing by zero.\n      dtype: the dtype of the result (default: infer from input and params).\n      param_dtype: the dtype passed to parameter initializers (default: float32).\n      use_bias:  If True, bias (beta) is added.\n      use_scale: If True, multiply by scale (gamma). When the next layer is linear\n        (also e.g. nn.relu), this can be disabled since the scaling will be done\n        by the next layer.\n      bias_init: Initializer for bias, by default, zero.\n      scale_init: Initializer for scale, by default, one.\n      reduction_axes: int for computing normalization statistics.\n      feature_axes: Feature axes for learned bias and scaling.\n      axis_name: the axis name used to combine batch statistics from multiple\n        devices. See `jax.pmap` for a description of axis names (default: None).\n        This is only needed if the model is subdivided across devices, i.e. the\n        array being normalized is sharded across devices within a pmap or shard\n        map. For SPMD jit, you do not need to manually synchronize. Just make sure\n        that the axes are correctly annotated and XLA:SPMD will insert the\n        necessary collectives.\n      axis_index_groups: groups of axis indices within that named axis\n        representing subsets of devices to reduce over (default: None). For\n        example, `[[0, 1], [2, 3]]` would independently batch-normalize over the\n        examples on the first two and last two devices. See `jax.lax.psum` for\n        more details.\n      use_fast_variance: If true, use a faster, but less numerically stable,\n        calculation for the variance.\n    \"\"\"\n\n    epsilon: float = 1e-6\n    dtype: Optional[chex.ArrayDType] = None\n    param_dtype: chex.ArrayDType = jnp.float32\n    use_bias: bool = True\n    use_scale: bool = True\n    bias_init: Callable[[jax.random.PRNGKey, chex.Shape, chex.ArrayDType], Array] = nn.initializers.zeros\n    scale_init: Callable[[jax.random.PRNGKey, chex.Shape, chex.ArrayDType], Array] = nn.initializers.ones\n    reduction_axes: int = -1\n    feature_axes: int = -1\n    axis_name: Optional[str] = None\n    axis_index_groups: Any = None\n    use_fast_variance: bool = True\n\n    @nn.compact\n    def __call__(self, x):\n        \"\"\"Applies layer normalization on the input.\n    \n        Args:\n          x: the inputs\n    \n        Returns:\n          Normalized inputs (the same shape as inputs).\n        \"\"\"\n        mean, var = _compute_stats(\n            x,\n            self.reduction_axes,\n            self.dtype,\n            self.axis_name,\n            self.axis_index_groups,\n            use_fast_variance=self.use_fast_variance,\n        )\n\n        return _normalize(\n            self,\n            x,\n            mean,\n            var,\n            self.reduction_axes,\n            self.feature_axes,\n            self.dtype,\n            self.param_dtype,\n            self.epsilon,\n            self.use_bias,\n            self.use_scale,\n            self.bias_init,\n            self.scale_init,\n        )\n\n\nclass EmbeddingFlax(nn.Module):\n    config: PhiConfig\n\n    def setup(self) -> None:\n        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd)\n        self.drop = nn.Dropout(self.config.embd_pdrop)\n\n    def __call__(self, input_ids: Array, deterministic: bool = True) -> Array:\n        return self.drop(self.wte(input_ids.reshape(-1, input_ids.shape[-1])), deterministic=deterministic)\n\n\ndef _apply_rotary_emb(\n        x: Array,\n        cos: Array,\n        sin: Array,\n) -> Array:\n    _, seq_len, _, _ = x.shape\n    _, rotary_dim = cos.shape\n    rotary_dim *= 2\n\n    x_rot = x[:, :, :, :rotary_dim]\n    x_pass = x[:, :, :, rotary_dim:]\n\n    x1, x2 = x_rot.chunk(2, axis=-1)\n    c, s = cos[:seq_len][:, jnp.newaxis, :], sin[:seq_len][:, jnp.newaxis, :]\n    x1, x2, c, s = [t.astype(dtype=jnp.float32) for t in [x1, x2, c, s]]\n\n    x_rot = jnp.concatenate([x1 * c - x2 * s, x1 * s + x2 * c], axis=-1).astype(x.dtype)\n\n    return jnp.concatenate([x_rot, x_pass], axis=-1)\n\n\ndef _apply_rotary_emb_kv(\n        kv: Array,\n        cos: Array,\n        sin: Array,\n        cos_k: Optional[Array] = None,\n        sin_k: Optional[Array] = None,\n) -> Array:\n    _, seq_len, _, _, _ = kv.shape\n    _, rotary_dim = cos.shape\n    rotary_dim *= 2\n\n    k_rot = kv[:, :, 0, :, :rotary_dim]\n    k_pass = kv[:, :, 0, :, rotary_dim:]\n\n    k1, k2 = k_rot.chunk(2, axis=-1)\n    c, s = cos[:seq_len][:, jnp.newaxis, :], sin[:seq_len][:, jnp.newaxis, :]\n    k1, k2, c, s = [t.astype(dtype=jnp.float32) for t in [k1, k2, c, s]]\n\n    k_rot = jnp.concatenate([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).astype(kv.dtype)\n\n    return jnp.concatenate(\n        [\n            jnp.concatenate([k_rot, k_pass], axis=-1)[:, :, jnp.newaxis, :, :],\n            kv[:, :, 1:2, :, :],\n        ],\n        axis=2,\n    )\n\n\ndef _apply_rotary_emb_qkv(\n        qkv: Array,\n        cos: Array,\n        sin: Array,\n        cos_k: Optional[Array] = None,\n        sin_k: Optional[Array] = None,\n) -> Array:\n    _, seq_len, _, _, _ = qkv.shape\n    _, rotary_dim = cos.shape\n    rotary_dim *= 2\n\n    q_rot = qkv[:, :, 0, :, :rotary_dim]\n    q_pass = qkv[:, :, 0, :, rotary_dim:]\n\n    k_rot = qkv[:, :, 1, :, :rotary_dim]\n    k_pass = qkv[:, :, 1, :, rotary_dim:]\n\n    q1, q2 = jnp.split(q_rot, 2, axis=-1)\n    k1, k2 = jnp.split(k_rot, 2, axis=-1)\n    c, s = cos[:seq_len][:, jnp.newaxis, :], sin[:seq_len][:, jnp.newaxis, :]\n    q1, q2, k1, k2, c, s = [t.astype(dtype=jnp.float32) for t in [q1, q2, k1, k2, c, s]]\n\n    q_rot = jnp.concatenate([q1 * c - q2 * s, q1 * s + q2 * c], axis=-1).astype(qkv.dtype)\n    k_rot = jnp.concatenate([k1 * c - k2 * s, k1 * s + k2 * c], axis=-1).astype(qkv.dtype)\n\n    return jnp.concatenate(\n        [\n            jnp.concatenate([q_rot, q_pass], axis=-1)[:, :, jnp.newaxis, :, :],\n            jnp.concatenate([k_rot, k_pass], axis=-1)[:, :, jnp.newaxis, :, :],\n            qkv[:, :, 2:3, :, :],\n        ],\n        axis=2,\n    )\n\n\nclass RotaryEmbedding(nn.Module):\n    axis: int\n    base: int = 10000\n    scale_base: Optional[float] = None\n    pos_idx_in_fp32: bool = True\n    max_position_embeddings: int = 2048\n    \"\"\"Rotary positional embedding (RoPE).\n    Reference:\n        RoFormer: Enhanced Transformer with Rotary Position Embedding.\n        https://arxiv.org/pdf/2104.09864.pdf.\n    \"\"\"\n\n    def setup(\n            self\n    ) -> None:\n\n        if self.scale_base is not None:\n            raise NotImplementedError\n\n        inv_freq = self._compute_inv_freq()\n        self.inv_freq = inv_freq\n        scale = (\n            (jnp.arange(0, self.axis, 2, dtype=jnp.float32) + 0.4 * self.axis) / (1.4 * self.axis)\n            if self.scale_base is not None\n            else None\n        )\n        self.scale = scale\n        self._seq_len_cached = self.max_position_embeddings\n        seq_len = self.max_position_embeddings\n\n        if self.pos_idx_in_fp32:\n            t = jnp.arange(seq_len, dtype=jnp.float32)\n            if self.inv_freq.dtype != jnp.float32:\n                inv_freq = self._compute_inv_freq()\n            else:\n                inv_freq = self.inv_freq\n        else:\n            t = jnp.arange(seq_len, dtype=self.inv_freq.dtype)\n            inv_freq = self.inv_freq\n\n        freqs = jnp.outer(t, inv_freq)\n        if self.scale is None:\n            self._cos_cached = jnp.cos(freqs).astype(jnp.float32)\n            self._sin_cached = jnp.sin(freqs).astype(jnp.float32)\n        else:\n            power = (\n                            jnp.arange(seq_len, dtype=self.scale.dtype) - seq_len // 2\n                    ) / self.scale_base\n            scale = self.scale ** power[:, jnp.newaxis]\n\n            self._cos_cached = (jnp.cos(freqs) * scale).astype(jnp.float32)\n            self._sin_cached = (jnp.sin(freqs) * scale).astype(jnp.float32)\n            self._cos_k_cached = (jnp.cos(freqs) / scale).astype(jnp.float32)\n            self._sin_k_cached = (jnp.sin(freqs) / scale).astype(jnp.float32)\n\n    def _compute_inv_freq(self) -> Array:\n        return 1.0 / (self.base ** (jnp.arange(0, self.axis, 2, dtype=jnp.float32) / self.axis))\n\n    def __call__(\n            self,\n            qkv: Array,\n            kv: Optional[Array] = None,\n            seq_len_offset: int = 0,\n    ) -> Tuple[Array, Array]:\n        seq_start = seq_len_offset\n        seq_end = seq_start + qkv.shape[1]\n\n        if kv is None:\n            return _apply_rotary_emb_qkv(\n                qkv,\n                self._cos_cached[seq_start:seq_end],\n                self._sin_cached[seq_start:seq_end],\n            )\n        else:\n            q = _apply_rotary_emb(\n                qkv,\n                self._cos_cached[seq_start:seq_end],\n                self._sin_cached[seq_start:seq_end],\n            )\n            kv = _apply_rotary_emb_kv(\n                kv,\n                self._cos_cached[seq_start:seq_end],\n                self._sin_cached[seq_start:seq_end],\n            )\n\n            return q, kv\n\n\nclass MLP(nn.Module):\n    config: PhiConfig\n    n_inner: Optional[int] = None\n    act_fn: Optional[str] = None\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    \"\"\"Multi-Layer Perceptron.\n    Reference:\n        Attention Is All You Need.\n        https://arxiv.org/pdf/1706.03762.pdf.\n    \"\"\"\n\n    def setup(\n            self\n    ) -> None:\n        act_fn = self.config.activation_function if self.act_fn is None else self.act_fn\n\n        n_inner = getattr(self.config, \"n_inner\", None) if self.n_inner is None else self.n_inner\n        n_inner = n_inner if n_inner is not None else 4 * self.config.n_embd\n\n        self.fc1 = nn.Dense(\n            n_inner,\n            kernel_init=nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.fc2 = nn.Dense(\n            self.config.n_embd,\n            kernel_init=nn.initializers.normal(self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.act = ACT2FN[act_fn]\n\n    def __call__(self, hidden_states: Array) -> Array:\n        return self.fc2(self.act(self.fc1(hidden_states)))\n\n\nclass SelfAttention(nn.Module):\n    causal: bool = True\n    softmax_scale: Optional[float] = None\n    attention_dropout: float = 0.0\n\n    \"\"\"\n    Self-attention layer (compatible with JAX/FLAX).\n    \"\"\"\n\n    def setup(\n            self\n    ) -> None:\n        self.drop = nn.Dropout(self.attention_dropout)\n\n    def __call__(\n            self,\n            qkv: Array,\n            causal: bool = None,\n            key_padding_mask: Optional[Array] = None,\n            deterministic: bool = True,\n            **kwargs,\n    ) -> Array:\n        batch_size, seq_len = qkv.shape[0], qkv.shape[1]\n        q, k, v = jnp.split(qkv, 3, axis=2)\n        q, k, v = map(lambda x: x.squeeze(2), [q, k, v])\n        q = q.astype(jnp.float32)\n        k = k.astype(jnp.float32)\n\n        causal = self.causal if causal is None else causal\n        softmax_scale = self.softmax_scale or jax.lax.rsqrt(jnp.array(q.shape[-1], dtype=jnp.float32))\n\n        scores = jnp.einsum(\"b t h d,b s h d->b h t s\", q, k * softmax_scale)\n\n        if key_padding_mask is not None:\n            padding_mask = jnp.where(\n                key_padding_mask.astype(jnp.bool_), 0.0, -10000.0\n            )[:, jnp.newaxis, jnp.newaxis, :]\n\n            scores = scores + padding_mask\n\n        if causal:\n            causal_mask = jnp.triu(jnp.full((seq_len, seq_len), -10000.0), 1)\n            scores = scores + causal_mask.astype(dtype=scores.dtype)\n\n        attention = jax.nn.softmax(scores, axis=-1).astype(v.dtype)\n        attention = self.drop(attention, deterministic=deterministic)\n        output = jnp.einsum(\"b h t s,b s h d->b t h d\", attention, v)\n        return output\n\n\nclass CrossAttention(nn.Module):\n    causal: bool = True\n    softmax_scale: Optional[float] = None\n    attention_dropout: float = 0.0\n    \"\"\"\n    Cross-attention layer (compatible with JAX/FLAX).\n    \"\"\"\n\n    def setup(\n            self\n    ) -> None:\n        self.drop = nn.Dropout(self.attention_dropout)\n\n    def __call__(\n            self,\n            q: Array,\n            kv: Array,\n            causal: bool = None,\n            key_padding_mask: Optional[Array] = None,\n            deterministic: bool = True,\n            **kwargs,\n    ) -> Array:\n        batch_size, seq_len_q = q.shape[0], q.shape[1]\n        seq_len_k = kv.shape[1]\n\n        if kv.shape[3] != q.shape[2]:\n            kv = repeat(kv, \"... hkv d -> ... (hkv g) d\", g=q.shape[2] // kv.shape[3])\n        k, v = kv.unbind(axis=2)\n\n        q = q.astype(jnp.float32)\n        k = k.astype(jnp.float32)\n\n        causal = self.causal if causal is None else causal\n        softmax_scale = self.softmax_scale or jax.lax.rsqrt(q.shape[-1])\n\n        scores = jnp.einsum(\"bthd,bshd->bhts\", q, k * softmax_scale)\n\n        if key_padding_mask is not None:\n            padding_mask = jax.lax.select(\n                key_padding_mask.astype(jnp.bool_), 0.0, -10000.0\n            )[:, jnp.newaxis, jnp.newaxis, :]\n\n            scores = scores + padding_mask\n\n        if causal:\n            rows = jnp.arange(seq_len_q, dtype=jnp.int32)[:, jnp.newaxis]\n            cols = jnp.arange(seq_len_k, dtype=jnp.int32)\n            causal_mask = cols > rows + seq_len_k - seq_len_q\n\n            scores = jax.lax.select(\n                causal_mask, scores, -10000.0\n            )\n\n        attention = jax.nn.softmax(scores, axis=-1).astype(v.dtype)\n        attention = self.drop(attention, deterministic=deterministic)\n\n        output = jnp.einsum(\"bhts,bshd->bthd\", attention, v)\n\n        return output\n\n\ndef _find_mha_dims(\n        config: PhiConfig,\n        n_head: Optional[int] = None,\n        n_head_kv: Optional[int] = None,\n        head_dim: Optional[int] = None,\n) -> Tuple[Union[int, Any], Union[int, None, Any], Union[int, Any]]:\n    if n_head is None and head_dim is None:\n        head_dim = config.n_embd // config.n_head\n        n_head = config.n_head\n    elif n_head is None or head_dim is None:\n        raise ValueError(\"`n_head` and `head_dim` must be both specified or `None`.\")\n    if n_head_kv is None:\n        n_head_kv = getattr(config, \"n_head_kv\", None) or n_head\n\n    return n_head, n_head_kv, head_dim\n\n\nclass MHA(nn.Module):\n    config: PhiConfig\n    dtype: Optional[jnp.dtype] = jnp.float32\n    param_dtype: Optional[jnp.dtype] = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n    rotary_dim_: Optional[int] = None\n    rotary_base_: float = 10000.0\n    rotary_scale_base_: Optional[float] = None\n    n_head_: Optional[int] = None\n    n_head_kv_: Optional[int] = None\n    head_dim_: Optional[int] = None\n    bias_: bool = True\n    causal_: bool = True\n    softmax_scale_: Optional[float] = None\n    layer_idx_: Optional[int] = None\n    return_residual_: bool = False\n\n    def setup(\n            self\n    ) -> None:\n\n        self.bias = self.bias_\n        self.causal = self.causal_\n        self.softmax_scale = self.softmax_scale_\n        self.layer_idx = self.layer_idx_\n        self.rotary_dim = self.rotary_dim_ if self.rotary_dim_ is not None else getattr(self.config, \"rotary_dim\", 0)\n        self.rotary_base = self.rotary_base_\n        self.rotary_scale_base = self.rotary_scale_base_\n\n        if self.rotary_dim > 0:\n            self.rotary_emb = RotaryEmbedding(\n                self.rotary_dim,\n                base=self.rotary_base,\n                scale_base=self.rotary_scale_base,\n                max_position_embeddings=self.config.n_positions\n            )\n\n        # MLP\n        self.n_head, self.n_head_kv, self.head_dim = _find_mha_dims(\n            self.config,\n            n_head=self.n_head_,\n            n_head_kv=self.n_head_kv_,\n            head_dim=self.head_dim_\n        )\n        op_size = self.head_dim * (self.n_head + 2 * self.n_head_kv)\n        hidden_size = self.config.n_embd\n\n        self.Wqkv = nn.Dense(\n            op_size,\n            use_bias=self.bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(self.config.initializer_range)\n        )\n        self.out_proj = nn.Dense(\n            hidden_size,\n            use_bias=self.bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(self.config.initializer_range)\n        )\n\n        self.inner_attn = SelfAttention(\n            causal=self.causal,\n            softmax_scale=self.softmax_scale,\n            attention_dropout=self.config.attn_pdrop,\n        )\n        self.inner_cross_attn = CrossAttention(\n            causal=self.causal,\n            softmax_scale=self.softmax_scale,\n            attention_dropout=self.config.attn_pdrop,\n        )\n        self.flash_attn = False\n\n    def _forward_self_attn(\n            self,\n            x: Array,\n            key_padding_mask: Optional[Array],\n            deterministic: bool = True\n    ) -> Array:\n        qkv = self.Wqkv(x)\n        qkv = rearrange(qkv, \"... (three h d) -> ... three h d\", three=3, d=self.head_dim)\n\n        if self.rotary_dim > 0:\n            qkv = self.rotary_emb(qkv)\n\n        return self.inner_attn(\n            qkv,\n            key_padding_mask=key_padding_mask,\n            deterministic=deterministic\n        )\n\n    def _forward_cross_attn(\n            self,\n            x: Array,\n            key_padding_mask: Optional[Array],\n            position_ids: Optional[Array] = None,\n            deterministic: bool = True\n    ) -> Array:\n\n        # TODO: adding past_key_values\n        past_key_values = None\n\n        batch_size = x.shape[0]\n\n        qkv = self.Wqkv(x)\n\n        q = qkv[..., : self.n_head * self.head_dim]\n        q = rearrange(q, \"... (h d) -> ... h d\", d=self.head_dim)\n\n        kv = qkv[..., self.n_head * self.head_dim:]\n        kv = rearrange(kv, \"... (two hkv d) -> ... two hkv d\", two=2, d=self.head_dim)\n\n        seq_len_offset = position_ids[batch_size - 1, 0] if position_ids is not None else 0\n        causal = None if seq_len_offset == 0 else False\n        if self.self.rotary_dim > 0:\n            q, kv = self.rotary_emb(q, kv=kv, seq_len_offset=seq_len_offset)\n\n        if past_key_values is not None:\n            raise NotImplementedError(\"TODO ?\")\n\n        return self.inner_cross_attn(\n            q,\n            kv,\n            key_padding_mask=key_padding_mask,\n            causal=causal,\n            deterministic=deterministic\n        )\n\n    def __call__(\n            self,\n            x: Array,\n            attention_mask: Array = None,\n            deterministic: bool = True,\n            **kwargs,\n    ) -> Tuple[Array, Array]:\n\n        attention_mask = attention_mask.astype(jnp.bool_)\n\n        # TODO: adding past_key_values\n        past_key_values = None\n        if self.n_head == self.n_head_kv:\n            if past_key_values is None:\n                attn_output = self._forward_self_attn(x, attention_mask, deterministic=deterministic)\n            else:\n                attn_output = self._forward_cross_attn(x, past_key_values, attention_mask, deterministic=deterministic)\n        else:\n            attn_output = self._forward_cross_attn(x, past_key_values, attention_mask, deterministic=deterministic)\n\n        output = rearrange(attn_output, \"... h d -> ... (h d)\")\n        output = self.out_proj(output)\n\n        return output if not self.return_residual_ else (output, x)\n\n\nclass ParallelBlock(nn.Module):\n    \"\"\"Parallel block.\n    This block applies parallel mixer and MLP layers to the input (used in GPT-J and CodeGen).\n    \"\"\"\n    config: PhiConfig\n    block_idx: Optional[int] = None\n    dtype: Optional[jnp.dtype] = jnp.float32\n    param_dtype: Optional[jnp.dtype] = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(\n            self,\n    ) -> None:\n        self.ln = LayerNorm(\n            epsilon=self.config.layer_norm_epsilon,\n            dtype=self.dtype\n        )\n        self.resid_dropout = nn.Dropout(self.config.resid_pdrop)\n\n        self.mixer = MHA(\n            self.config,\n            layer_idx_=self.block_idx,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.mlp = MLP(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(\n            self,\n            hidden_states: Array,\n            attention_mask: Optional[Array] = None,\n            deterministic: bool = True\n    ) -> Array:\n        residual = hidden_states\n        hidden_states = self.ln(hidden_states)\n\n        attn_outputs = self.mixer(\n            hidden_states,\n            attention_mask=attention_mask,\n            deterministic=deterministic\n        )\n        if isinstance(attn_outputs, tuple):\n            attn_outputs = attn_outputs[0]\n\n        attn_outputs = self.resid_dropout(attn_outputs, deterministic=deterministic)\n        feed_forward_hidden_states = self.resid_dropout(self.mlp(hidden_states), deterministic=deterministic)\n\n        hidden_states = attn_outputs + feed_forward_hidden_states + residual\n\n        return hidden_states\n\n\nclass CausalLMHead(nn.Module):\n    config: PhiConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n    \"\"\"Causal Language Modeling head.\n    Reference:\n        Improving Language Understanding by Generative Pre-Training.\n        https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf.\n    \"\"\"\n\n    def setup(self) -> None:\n        self.ln = LayerNorm(\n            epsilon=self.config.layer_norm_epsilon,\n            dtype=self.dtype\n        )\n        self.linear = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(self.config.initializer_range)\n        )\n\n    def __call__(self, hidden_states: Array) -> Array:\n        return self.linear(self.ln(hidden_states)).astype(jnp.float32)\n\n\nclass FlaxPhiPreTrainedModel(transformers.FlaxPreTrainedModel):\n    \"\"\"Phi pre-trained model.\"\"\"\n    module_class = None\n    config_class = PhiConfig\n    base_model_prefix = \"transformer\"\n\n    def __init__(self,\n                 config: PhiConfig,\n                 dtype: jnp.dtype = jnp.float32,\n                 param_dtype: jnp.dtype = jnp.float32,\n                 precision: jax.lax.Precision = jax.lax.Precision(\"fastest\"),\n                 input_shape=(1, 1),\n                 seed: int = 42,\n                 _do_init: bool = False\n                 ) -> None:\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision\n        )\n        super().__init__(\n            config=config,\n            module=module,\n            input_shape=input_shape,\n            _do_init=_do_init,\n            seed=seed\n        )\n\n    def prepare_inputs_for_generation(\n            self,\n            input_ids: Array,\n            attention_mask: Optional[Union[Array, Array]] = None,\n            **kwargs,\n    ) -> Dict[str, Any]:\n        # TODO: adding past_key_values\n        past_key_values = None\n\n        if input_ids.shape[1] > self.config.n_positions:\n            input_ids = input_ids[:, -self.config.n_positions:]\n            if attention_mask is not None:\n                attention_mask = attention_mask[:, -self.config.n_positions:]\n\n        if past_key_values is None or not (isinstance(past_key_values, InferenceParams)):\n            past_key_values = InferenceParams(\n                max_seq_len=self.config.n_positions,\n                max_batch_size=input_ids.shape[0],\n                seq_len_offset=0,\n                batch_size_offset=0,\n                key_value_memory_dict={},\n                lengths_per_sample=None,\n            )\n        else:\n            past_key_values.seq_len_offset = input_ids.shape[1] - 1\n            input_ids = input_ids[:, -1].unsqueeze(-1)\n\n        return {\n            \"input_ids\": input_ids,\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": attention_mask,\n        }\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        module_init_outputs = self.module.init(rngs, input_ids, attention_mask)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: Array,\n            attention_mask: Array = None,\n            params: dict = None,\n            deterministic: bool = True,\n            past_key_values: Array | Sequence[Array] = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length <= self.config.max_position_embeddings, (f'Position out of range '\n                                                                        f'(Model Support '\n                                                                        f'{self.config.max_position_embeddings} got'\n                                                                        f' {sequence_length})')\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            deterministic,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass ParallelBlockCollection(nn.Module):\n    config: PhiConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -> None:\n        block = ParallelBlock\n        if self.config.gradient_checkpointing != \"\":\n            policy = get_gradient_checkpoint_policy(self.config.gradient_checkpointing)\n            block = nn.remat(\n                block,\n                policy=policy,\n                static_argnums=(-1)\n            )\n        self.layers = [\n            block(\n                self.config,\n                block_idx=i,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            ) for i in range(self.config.n_layer)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: Array,\n            attention_mask: Optional[Array] = None,\n            deterministic: bool = True\n    ) -> Array:\n        for layer in self.layers:\n            hidden_states = layer(\n                hidden_states,\n                attention_mask=attention_mask,\n                deterministic=deterministic\n            )\n        return hidden_states\n\n\nclass FlaxPhiModule(nn.Module):\n    \"\"\"Phi model.\"\"\"\n    config: PhiConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -> None:\n        self.embd = EmbeddingFlax(\n            config=self.config\n        )\n        self.h = ParallelBlockCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(\n            self,\n            input_ids: Array,\n            attention_mask: Optional[Array] = None,\n            deterministic: bool = True\n    ) -> Array:\n        return self.h(\n            self.embd(input_ids, deterministic),\n            attention_mask=attention_mask,\n            deterministic=deterministic\n        )\n\n\nclass FlaxPhiForCausalLMModule(nn.Module):\n    \"\"\"Phi for Causal Language Modeling.\"\"\"\n    config: PhiConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -> None:\n        self.transformer = FlaxPhiModule(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.lm_head = CausalLMHead(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(\n            self,\n            input_ids: Array,\n            attention_mask: Optional[Array] = None,\n            deterministic: bool = True,\n            **kwargs,\n    ) -> FlaxCausalLMOutput:\n        hidden_states = self.transformer(input_ids, attention_mask=attention_mask, deterministic=deterministic)\n        lm_logits = self.lm_head(hidden_states)\n\n        return FlaxCausalLMOutput(logits=lm_logits)\n\n\nclass FlaxPhiForCausalLM(FlaxPhiPreTrainedModel):\n    module_class = FlaxPhiForCausalLMModule\n\n\nclass FlaxPhiModel(FlaxPhiPreTrainedModel):\n    module_class = FlaxPhiModule\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/phi/modelling_phi_flax.py b/lib/python/EasyDel/modules/phi/modelling_phi_flax.py
--- a/lib/python/EasyDel/modules/phi/modelling_phi_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/phi/modelling_phi_flax.py	(date 1703670641644)
@@ -14,6 +14,7 @@
 from einops import repeat, rearrange
 from transformers.modeling_flax_outputs import FlaxCausalLMOutput
 from .phi_configuration import PhiConfig
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 
 
 @dataclass
@@ -1051,6 +1052,30 @@
 class FlaxPhiForCausalLM(FlaxPhiPreTrainedModel):
     module_class = FlaxPhiForCausalLMModule
 
+    def get_input_embeddings(self):
+        return self.module.transformer.embd
+
+    def get_decoder(self):
+        return self.module.transformer
+
+    def set_input_embeddings(self, value):
+        self.module.transformer.embd = value
+
+    def set_decoder(self, decoder):
+        self.module.transformer = decoder
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
 
 class FlaxPhiModel(FlaxPhiPreTrainedModel):
     module_class = FlaxPhiModule
+
+    def get_input_embeddings(self):
+        return self.module.embd
+
+    def set_input_embeddings(self, value):
+        self.module.embd = value
Index: lib/python/EasyDel/modules/opt/modelling_opt_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># coding=utf-8\n# Copyright 2022 The Fairseq Authors and The Google Flax Team Authors And The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# THIS SCRIPT IS EDITED FROM ORIGINAL IMPLEMENTATION OF TRANSFORMERS OPT\n\"\"\" Flax OPT model.\"\"\"\n\nfrom functools import partial\nfrom typing import Optional, Tuple, Sequence\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze\nfrom flax.linen import combine_masks, make_causal_mask\nfrom flax.linen.attention import dot_product_attention_weights\nfrom flax.traverse_util import flatten_dict, unflatten_dict\nfrom jax import lax\nfrom jax.random import PRNGKey\nfrom transformers import PretrainedConfig\nfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxMaskedLMOutput\nfrom transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel\nfrom jax.sharding import PartitionSpec\nfrom transformers import logging\n\nfrom ..flax_modelling_utils import get_gradient_checkpoint_policy, \\\n    with_sharding_constraint, JaxBaseClassModel\n\nimport chex\n\nfrom .opt_configuration import OPTConfig\n\nlogger = logging.get_logger(__name__)\n\n\n# Copied from transformers.models.bart.modeling_flax_bart.FlaxBartAttention with Bart->OPT\nclass FlaxOPTAttention(nn.Module):\n    config: OPTConfig\n    embed_dim: int\n    num_heads: int\n    dropout: float = 0.0\n    causal: bool = False\n    bias: bool = True\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self) -> None:\n        self.head_dim = self.embed_dim // self.num_heads\n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(\n                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=self.bias,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n\n        self.dropout_layer = nn.Dropout(rate=self.dropout)\n\n        if self.causal:\n            self.causal_mask = make_causal_mask(\n                jnp.ones((1, self.config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\"\n            )\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: chex.Array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n            self,\n            hidden_states: jnp.ndarray,\n            key_value_states: Optional[jnp.ndarray] = None,\n            attention_mask: Optional[jnp.ndarray] = None,\n            init_cache: bool = False,\n            deterministic: bool = True,\n    ) -> Tuple[jnp.ndarray]:\n\n        is_cross_attention = key_value_states is not None\n        batch_size = hidden_states.shape[0]\n\n        query_states = self.q_proj(hidden_states)\n\n        if is_cross_attention:\n            key_states = self.k_proj(key_value_states)\n            value_states = self.v_proj(key_value_states)\n        else:\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        if self.config.use_pjit_attention_force:\n            value_states = with_sharding_constraint(value_states, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            key_states = with_sharding_constraint(key_states, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            query_states = with_sharding_constraint(query_states, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n        query_states = self._split_heads(query_states)\n        key_states = self._split_heads(key_states)\n        value_states = self._split_heads(value_states)\n\n        if self.causal:\n            query_length, key_length = query_states.shape[1], key_states.shape[1]\n            if self.has_variable(\"cache\", \"cached_key\"):\n                mask_shift = self.variables[\"cache\"][\"cache_index\"]\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n                causal_mask = lax.dynamic_slice(\n                    self.causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n                )\n            else:\n                causal_mask = self.causal_mask[:, :, :query_length, :key_length]\n            causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n\n        # combine masks if needed\n        if attention_mask is not None and self.causal:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n            attention_mask = combine_masks(attention_mask, causal_mask)\n        elif self.causal:\n            attention_mask = causal_mask\n        elif attention_mask is not None:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n            key_states, value_states, attention_mask = self._concatenate_to_cache(\n                key_states, value_states, query_states, attention_mask\n            )\n            if attention_mask is not None:\n                attention_bias = lax.select(\n                    attention_mask > 0,\n                    jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                    jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n                )\n            else:\n                attention_bias = None\n\n            dropout_rng = None\n            if not deterministic and self.dropout > 0.0:\n                dropout_rng = self.make_rng(\"dropout\")\n\n            attn_weights = dot_product_attention_weights(\n                query_states,\n                key_states,\n                bias=attention_bias,\n                dropout_rng=dropout_rng,\n                dropout_rate=self.dropout,\n                broadcast_dropout=True,\n                deterministic=deterministic,\n                dtype=self.dtype,\n                precision=None,\n            )\n            if self.config.use_pjit_attention_force:\n                attn_weights = with_sharding_constraint(attn_weights, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None))\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value_states)\n            attn_output = self._merge_heads(attn_output)\n            attn_output = self.out_proj(attn_output)\n\n        return attn_output, attn_weights\n\n\nclass FlaxOPTDecoderLayer(nn.Module):\n    config: OPTConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self) -> None:\n        self.embed_dim = self.config.hidden_size\n        self.self_attn = FlaxOPTAttention(\n            config=self.config,\n            embed_dim=self.embed_dim,\n            num_heads=self.config.num_attention_heads,\n            dropout=self.config.attention_dropout,\n            causal=True,\n            dtype=self.dtype,\n        )\n        self.do_layer_norm_before = self.config.do_layer_norm_before\n        self.dropout_layer = nn.Dropout(rate=self.config.dropout)\n        self.activation_fn = ACT2FN[self.config.activation_function]\n\n        self.self_attn_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n        self.fc1 = nn.Dense(\n            self.config.ffn_dim,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n        self.fc2 = nn.Dense(\n            self.embed_dim, dtype=self.dtype, kernel_init=jax.nn.initializers.normal(self.config.init_std)\n        )\n        self.final_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n\n    def __call__(\n            self,\n            hidden_states: jnp.ndarray,\n            attention_mask: jnp.ndarray,\n            init_cache: bool = False,\n            output_attentions: bool = True,\n            deterministic: bool = True,\n    ) -> Tuple[jnp.ndarray]:\n        residual = hidden_states\n\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n        if self.do_layer_norm_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            init_cache=init_cache,\n            deterministic=deterministic,\n        )\n        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n        hidden_states = residual + hidden_states\n        # 350m applies layer norm AFTER attention\n        if not self.do_layer_norm_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Fully Connected\n        hidden_states_shape = hidden_states.shape\n        hidden_states = hidden_states.reshape(-1, hidden_states.shape[-1])\n        residual = hidden_states\n\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n        if self.do_layer_norm_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.activation_fn(hidden_states)\n\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = self.dropout_layer(hidden_states, deterministic=deterministic)\n\n        hidden_states = (residual + hidden_states).reshape(hidden_states_shape)\n\n        # 350m applies layer norm AFTER attention\n        if not self.do_layer_norm_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        return outputs\n\n\nclass FlaxOPTDecoderLayerCollection(nn.Module):\n    config: OPTConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        block = FlaxOPTDecoderLayer\n        if self.config.gradient_checkpointing != '':\n            block = nn.remat(\n                block,\n                static_argnums=(3, 4),\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing)\n            )\n        self.layers = [\n            block(self.config, name=str(i), dtype=self.dtype)\n            for i in range(self.config.num_hidden_layers)\n        ]\n        self.layerdrop = self.config.layerdrop\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n    ):\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = decoder_layer(\n                hidden_states,\n                attention_mask=attention_mask,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n                deterministic=deterministic,\n            )\n\n            hidden_states = layer_outputs[0]\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        outputs = [hidden_states, all_hidden_states, all_self_attns]\n        return outputs\n\n\nclass FlaxOPTLearnedPositionalEmbedding(nn.Embed):\n\n    def setup(self):\n        self.offset = 2\n        self.embedding = self.param(\n            \"embedding\", self.embedding_init, (self.num_embeddings + self.offset, self.features), self.param_dtype\n        )\n\n    def __call__(self, positions):\n        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n\n        return super().__call__(positions + self.offset)\n\n\nclass FlaxOPTDecoder(nn.Module):\n    config: OPTConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n    offset: int = 2\n\n    def setup(self):\n        self.dropout_layer = nn.Dropout(rate=self.config.dropout)\n\n        embed_dim = self.config.hidden_size\n        self.padding_idx = self.config.pad_token_id\n        self.max_target_positions = self.config.max_position_embeddings\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.word_embed_proj_dim,\n            embedding_init=jax.nn.initializers.normal(self.config.init_std),\n            dtype=self.dtype,\n        )\n\n        self.embed_positions = FlaxOPTLearnedPositionalEmbedding(\n            self.config.max_position_embeddings,\n            embed_dim,\n            embedding_init=jax.nn.initializers.normal(self.config.init_std),\n            dtype=self.dtype,\n        )\n\n        if self.config.word_embed_proj_dim != self.config.hidden_size:\n            self.project_in = nn.Dense(self.config.hidden_size, use_bias=False)\n            self.project_out = nn.Dense(self.config.word_embed_proj_dim, use_bias=False)\n\n        else:\n            self.project_in = None\n            self.project_out = None\n\n        if self.config.do_layer_norm_before and not self.config._remove_final_layer_norm:\n            self.final_layer_norm = nn.LayerNorm(dtype=self.dtype, epsilon=1e-05)\n        else:\n            self.final_layer_norm = None\n\n        self.layers = FlaxOPTDecoderLayerCollection(self.config, self.dtype)\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask,\n            position_ids,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            deterministic: bool = True,\n    ):\n        input_shape = input_ids.shape\n        input_ids = input_ids.reshape(-1, input_shape[-1])\n\n        inputs_embeds = self.embed_tokens(input_ids)\n        if self.project_in is not None:\n            inputs_embeds = self.project_in(inputs_embeds)\n\n        positions = self.embed_positions(position_ids)\n\n        hidden_states = inputs_embeds + positions\n\n        hidden_state, all_hidden_states, attentions = self.layers(\n            hidden_states,\n            attention_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n        )\n\n        if self.final_layer_norm is not None:\n            hidden_state = self.final_layer_norm(hidden_state)\n\n        if self.project_out is not None:\n            hidden_state = self.project_out(hidden_state)\n\n        if output_hidden_states:\n            all_hidden_states += (hidden_state,)\n\n        outputs = [hidden_state, all_hidden_states, attentions]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_state,\n            hidden_states=all_hidden_states,\n            attentions=attentions,\n        )\n\n\nclass FlaxOPTPreTrainedModel(FlaxPreTrainedModel):\n    config_class = OPTConfig\n    base_model_prefix: str = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: OPTConfig,\n            input_shape: Tuple[int] = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # init input tensors\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n\n        batch_size, sequence_length = input_ids.shape\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            return_dict=False,\n        )\n\n        random_params = module_init_outputs[\"params\"]\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return unfreeze(init_variables[\"cache\"])\n\n    def __call__(\n            self,\n            input_ids: jnp.ndarray,\n            attention_mask: Optional[jnp.ndarray] = None,\n            position_ids: Optional[jnp.ndarray] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            dropout_rng: PRNGKey = None,\n            deterministic: bool = True,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n\n        if position_ids is None:\n            position_ids = (attention_mask.cumsum(axis=1) * attention_mask) - 1\n\n        # Handle any PRNG if needed\n        rngs = {\"dropout\": dropout_rng} if dropout_rng is not None else {}\n\n        inputs = {\"params\": params or self.params}\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n            position_ids=jnp.array(position_ids, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        # add updated cache to model output\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxOPTModule(nn.Module):\n    config: OPTConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n\n    def setup(self):\n        self.decoder = FlaxOPTDecoder(self.config, dtype=self.dtype)\n\n    def _get_decoder_module(self):\n        return self.decoder\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask,\n            position_ids,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            deterministic: bool = True,\n            init_cache=False,\n    ):\n        decoder_outputs = self.decoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n            init_cache=init_cache,\n        )\n\n        if not return_dict:\n            return decoder_outputs\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            hidden_states=decoder_outputs.hidden_states,\n            attentions=decoder_outputs.attentions,\n        )\n\n\n# Copied from transformers.models.bart.modeling_flax_bart.FlaxBartModel with Bart->OPT\nclass FlaxOPTModel(FlaxOPTPreTrainedModel):\n    config: OPTConfig\n    dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n    module_class = FlaxOPTModule\n\n\nclass FlaxOPTForCausalLMModule(nn.Module):\n    config: OPTConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.model = FlaxOPTModule(config=self.config, dtype=self.dtype)\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.init_std),\n        )\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask,\n            position_ids,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            deterministic: bool = True,\n    ):\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_embedding = self.model.variables[\"params\"][\"decoder\"][\"embed_tokens\"][\"embedding\"]\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxMaskedLMOutput(\n            logits=lm_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\nclass FlaxOPTForCausalLM(FlaxOPTPreTrainedModel):\n    module_class = FlaxOPTForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        # initializing the cache\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        # Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.\n        # But since the decoder uses a causal mask, those positions are masked anyway.\n        # Thus, we can create a single static attention_mask here, which is more efficient for compilation\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/opt/modelling_opt_flax.py b/lib/python/EasyDel/modules/opt/modelling_opt_flax.py
--- a/lib/python/EasyDel/modules/opt/modelling_opt_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/opt/modelling_opt_flax.py	(date 1703670192215)
@@ -27,18 +27,18 @@
 from flax.traverse_util import flatten_dict, unflatten_dict
 from jax import lax
 from jax.random import PRNGKey
-from transformers import PretrainedConfig
 from transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxMaskedLMOutput
-from transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel
+from transformers.modeling_flax_utils import ACT2FN
 from jax.sharding import PartitionSpec
 from transformers import logging
 
 from ..flax_modelling_utils import get_gradient_checkpoint_policy, \
-    with_sharding_constraint, JaxBaseClassModel
+    with_sharding_constraint
 
 import chex
 
 from .opt_configuration import OPTConfig
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 
 logger = logging.get_logger(__name__)
 
@@ -442,7 +442,7 @@
         )
 
 
-class FlaxOPTPreTrainedModel(FlaxPreTrainedModel):
+class FlaxOPTPreTrainedModel(EasyDelFlaxPretrainedModel):
     config_class = OPTConfig
     base_model_prefix: str = "model"
     module_class: nn.Module = None
@@ -603,12 +603,15 @@
         )
 
 
-# Copied from transformers.models.bart.modeling_flax_bart.FlaxBartModel with Bart->OPT
 class FlaxOPTModel(FlaxOPTPreTrainedModel):
-    config: OPTConfig
-    dtype: jnp.dtype = jnp.float32  # the dtype of the computation
     module_class = FlaxOPTModule
 
+    def set_input_embeddings(self, value):
+        self.module.embed_tokens = value
+
+    def get_input_embeddings(self):
+        return self.module.embed_tokens
+
 
 class FlaxOPTForCausalLMModule(nn.Module):
     config: OPTConfig
@@ -666,6 +669,24 @@
 class FlaxOPTForCausalLM(FlaxOPTPreTrainedModel):
     module_class = FlaxOPTForCausalLMModule
 
+    def set_input_embeddings(self, value):
+        self.module.model.embed_tokens = value
+
+    def get_input_embeddings(self):
+        return self.module.model.embed_tokens
+
+    def set_decoder(self, decoder):
+        self.module.model = decoder
+
+    def get_decoder(self):
+        return self.module.model
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
     def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):
         # initializing the cache
         batch_size, seq_length = input_ids.shape
Index: lib/python/EasyDel/modules/t5/modelling_t5_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># coding=utf-8\n# Copyright 2021 T5 Authors and HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# This model is copied from the Transformers and this script will apply pjit on them\n\"\"\" Flax T5 model.\"\"\"\n\nimport copy\nfrom typing import Callable, Optional, Tuple, Sequence\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze\nfrom flax.linen import combine_masks, make_causal_mask\nfrom flax.linen import partitioning as nn_partitioning\nfrom flax.linen.attention import dot_product_attention_weights\nfrom flax.traverse_util import flatten_dict, unflatten_dict\nfrom jax.random import PRNGKey\n\nfrom transformers.modeling_flax_outputs import (\n    FlaxBaseModelOutputWithPastAndCrossAttentions,\n    FlaxCausalLMOutputWithCrossAttentions,\n    FlaxSeq2SeqLMOutput,\n    FlaxSeq2SeqModelOutput,\n)\nfrom transformers.modeling_flax_utils import (\n    ACT2FN,\n    FlaxPreTrainedModel,\n)\n\nfrom transformers import PretrainedConfig\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import get_gradient_checkpoint_policy, \\\n    with_sharding_constraint, JaxBaseClassModel\n\nimport chex\nfrom .t5_configuration import T5Config\n\nremat = nn_partitioning.remat\n\n\ndef shift_tokens_right(input_ids: np.array, pad_token_id: int, decoder_start_token_id: int) -> np.ndarray:\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = jnp.zeros_like(input_ids)\n    shifted_input_ids = shifted_input_ids.at[:, 1:].set(input_ids[:, :-1])\n    shifted_input_ids = shifted_input_ids.at[:, 0].set(decoder_start_token_id)\n\n    shifted_input_ids = jnp.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n    return shifted_input_ids\n\n\nclass FlaxT5LayerNorm(nn.Module):\n    hidden_size: int\n    dtype: jnp.dtype = jnp.bfloat16\n    eps: float = 1e-6\n    weight_init: Callable[..., np.ndarray] = jax.nn.initializers.ones\n\n    def setup(self):\n        self.weight = self.param(\"weight\", self.weight_init, (self.hidden_size,))\n\n    def __call__(self, hidden_states):\n        variance = jnp.power(hidden_states.astype(\"f4\"), 2).mean(axis=-1, keepdims=True)\n        hidden_states = hidden_states / jnp.sqrt(variance + self.eps)\n\n        return self.weight * hidden_states\n\n\nclass FlaxT5DenseActDense(nn.Module):\n    config: T5Config\n    dtype: jnp.dtype = jnp.bfloat16\n\n    def setup(self):\n        wi_init_std = self.config.initializer_factor * (self.config.d_model ** -0.5)\n        wo_init_std = self.config.initializer_factor * (self.config.d_ff ** -0.5)\n\n        self.wi = nn.Dense(\n            self.config.d_ff,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(wi_init_std),\n            dtype=self.dtype,\n        )\n        self.wo = nn.Dense(\n            self.config.d_model,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(wo_init_std),\n            dtype=self.dtype,\n        )\n        self.dropout = nn.Dropout(self.config.dropout_rate)\n        self.act = ACT2FN[self.config.dense_act_fn]\n\n    def __call__(self, hidden_states, deterministic=True):\n        hidden_states = self.wi(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n        hidden_states = self.wo(hidden_states)\n        return hidden_states\n\n\nclass FlaxT5DenseGatedActDense(nn.Module):\n    config: T5Config\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        wi_init_std = self.config.initializer_factor * (self.config.d_model ** -0.5)\n        wo_init_std = self.config.initializer_factor * (self.config.d_ff ** -0.5)\n\n        self.wi_0 = nn.Dense(\n            self.config.d_ff,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(wi_init_std),\n            dtype=self.dtype,\n        )\n        self.wi_1 = nn.Dense(\n            self.config.d_ff,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(wi_init_std),\n            dtype=self.dtype,\n        )\n        self.wo = nn.Dense(\n            self.config.d_model,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(wo_init_std),\n            dtype=self.dtype,\n        )\n        self.dropout = nn.Dropout(self.config.dropout_rate)\n        self.act = ACT2FN[self.config.dense_act_fn]\n\n    def __call__(self, hidden_states, deterministic):\n        hidden_gelu = self.act(self.wi_0(hidden_states))\n        hidden_linear = self.wi_1(hidden_states)\n        hidden_states = hidden_gelu * hidden_linear\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n        hidden_states = self.wo(hidden_states)\n        return hidden_states\n\n\nclass FlaxT5LayerFF(nn.Module):\n    config: T5Config\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        if self.config.is_gated_act:\n            self.DenseReluDense = FlaxT5DenseGatedActDense(self.config, dtype=self.dtype)\n        else:\n            self.DenseReluDense = FlaxT5DenseActDense(self.config, dtype=self.dtype)\n\n        self.layer_norm = FlaxT5LayerNorm(self.config.d_model, eps=self.config.layer_norm_epsilon, dtype=self.dtype)\n        self.dropout = nn.Dropout(self.config.dropout_rate)\n\n    def __call__(self, hidden_states, deterministic=True):\n        forwarded_states = self.layer_norm(hidden_states)\n        forwarded_states = self.DenseReluDense(forwarded_states, deterministic=deterministic)\n        hidden_states = hidden_states + self.dropout(forwarded_states, deterministic=deterministic)\n        return hidden_states\n\n\nclass FlaxT5Attention(nn.Module):\n    config: T5Config\n    has_relative_attention_bias: bool = False\n    causal: bool = False\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        self.relative_attention_num_buckets = self.config.relative_attention_num_buckets\n        self.relative_attention_max_distance = self.config.relative_attention_max_distance\n        self.d_model = self.config.d_model\n        self.key_value_proj_dim = self.config.d_kv\n        self.n_heads = self.config.num_heads\n        self.dropout = self.config.dropout_rate\n        self.inner_dim = self.n_heads * self.key_value_proj_dim\n\n        q_init_std = self.config.initializer_factor * ((self.inner_dim * self.key_value_proj_dim) ** -0.5)\n        kv_init_std = self.config.initializer_factor * (self.inner_dim ** -0.5)\n        o_init_std = self.config.initializer_factor * (self.inner_dim ** -0.5)\n\n        self.q = nn.Dense(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(q_init_std),\n            dtype=self.dtype,\n        )\n        self.k = nn.Dense(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(kv_init_std),\n            dtype=self.dtype,\n        )\n        self.v = nn.Dense(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(kv_init_std),\n            dtype=self.dtype,\n        )\n        self.o = nn.Dense(\n            self.d_model,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(o_init_std),\n            dtype=self.dtype,\n        )\n\n        if self.has_relative_attention_bias:\n            self.relative_attention_bias = nn.Embed(\n                self.relative_attention_num_buckets,\n                self.n_heads,\n                embedding_init=jax.nn.initializers.normal(kv_init_std),\n                dtype=self.dtype,\n            )\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n\n        relative_buckets = 0\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (relative_position > 0) * num_buckets\n            relative_position = jnp.abs(relative_position)\n        else:\n            relative_position = -jnp.clip(relative_position, a_max=0)\n        # now relative_position is in the range [0, inf)\n\n        # half of the buckets are for exact increments in positions\n        max_exact = num_buckets // 2\n        is_small = relative_position < max_exact\n\n        relative_position_if_large = max_exact + (\n                jnp.log(relative_position / max_exact) / jnp.log(max_distance / max_exact) * (num_buckets - max_exact)\n        )\n        relative_position_if_large = jnp.clip(relative_position_if_large, a_max=num_buckets - 1)\n\n        relative_buckets += jnp.where(is_small, relative_position, relative_position_if_large)\n\n        return relative_buckets.astype(\"i4\")\n\n    def compute_bias(self, query_length, key_length):\n        \"\"\"Compute binned relative position bias\"\"\"\n        context_position = jnp.arange(query_length, dtype=\"i4\")[:, None]\n        memory_position = jnp.arange(key_length, dtype=\"i4\")[None, :]\n\n        relative_position = memory_position - context_position\n        relative_position_bucket = self._relative_position_bucket(\n            relative_position,\n            bidirectional=(not self.causal),\n            num_buckets=self.relative_attention_num_buckets,\n            max_distance=self.relative_attention_max_distance,\n        )\n\n        values = self.relative_attention_bias(relative_position_bucket)\n        values = values.transpose((2, 0, 1))[None, :, :, :]\n        return values\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.n_heads, self.key_value_proj_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.inner_dim,))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jax.Array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = jax.lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = jax.lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions\n            # that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def _create_position_bias(\n            self, key_states, query_states, attention_mask, init_cache, seq_length, causal_attention_mask_shift\n    ):\n        cache_is_filled = self.causal and self.has_variable(\"cache\", \"cached_key\") and (not init_cache)\n        key_length = key_states.shape[1]\n        query_length = key_length if cache_is_filled else query_states.shape[1]\n\n        if self.has_relative_attention_bias:\n            position_bias = self.compute_bias(query_length, key_length)\n        elif attention_mask is not None:\n            position_bias = jnp.zeros_like(attention_mask)\n        else:\n            position_bias = jnp.zeros((1, self.n_heads, query_length, key_length), dtype=self.dtype)\n\n        # if key and values are already calculated, only the last query position bias should be taken\n        if cache_is_filled:\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            position_bias = jax.lax.dynamic_slice(\n                position_bias,\n                (0, 0, causal_attention_mask_shift, 0),\n                (1, self.n_heads, seq_length, max_decoder_length),\n            )\n        return position_bias\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            key_value_states=None,\n            position_bias=None,\n            use_cache=False,\n            output_attentions=False,\n            deterministic=True,\n            init_cache=False,\n    ):\n\n        batch_size, seq_length = hidden_states.shape[:2]\n\n        # q, k, v projections\n        query_states = self.q(hidden_states)  # (batch_size, n_heads, seq_length, dim_per_head)\n        key_states = self.k(hidden_states) if key_value_states is None else self.k(key_value_states)\n        value_states = self.v(hidden_states) if key_value_states is None else self.v(key_value_states)\n        if self.config.use_pjit_attention_force:\n            query_states = with_sharding_constraint(query_states, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            key_states = with_sharding_constraint(key_states, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            value_states = with_sharding_constraint(value_states, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n\n        # reshape to (batch_size, seq_length, n_heads, head_dim)\n        query_states = self._split_heads(query_states)\n        key_states = self._split_heads(key_states)\n        value_states = self._split_heads(value_states)\n\n        # counter-act scaling in dot_product_attention_weights function\n        query_states *= jnp.sqrt(query_states.shape[-1])\n\n        # for fast decoding causal attention mask should be shifted\n        causal_attention_mask_shift = (\n            self.variables[\"cache\"][\"cache_index\"] if (self.has_variable(\"cache\", \"cached_key\") and self.causal) else 0\n        )\n        # create causal attention_mask; attention_mask has to be defined when model is causal\n        if self.causal:\n            causal_attention_mask = make_causal_mask(attention_mask, dtype=\"bool\")\n\n            # fast decoding for generate requires special attention_mask\n            if self.has_variable(\"cache\", \"cached_key\"):\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n                causal_attention_mask = jax.lax.dynamic_slice(\n                    causal_attention_mask,\n                    (0, 0, causal_attention_mask_shift, 0),\n                    (1, 1, seq_length, max_decoder_length),\n                )\n\n            # broadcast causal attention mask & attention mask to fit for merge\n            causal_attention_mask = jnp.broadcast_to(\n                causal_attention_mask, (batch_size,) + causal_attention_mask.shape[1:]\n            )\n            attention_mask = jnp.broadcast_to(\n                jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_attention_mask.shape\n            )\n            attention_mask = combine_masks(attention_mask, causal_attention_mask)\n        elif attention_mask is not None:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n            key_states, value_states, attention_attention_mask = self._concatenate_to_cache(\n                key_states, value_states, query_states, attention_mask\n            )\n\n        # replace masked positions with -10_000\n        if attention_mask is not None:\n            mask_value = jnp.finfo(self.dtype).min\n            attention_mask = jax.lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, mask_value).astype(self.dtype),\n            )\n\n        if position_bias is None:\n            # compute position bias (only for first layer)\n            position_bias = self._create_position_bias(\n                key_states, query_states, attention_mask, init_cache, seq_length, causal_attention_mask_shift\n            )\n\n            if attention_mask is not None:\n                position_bias = position_bias + attention_mask\n\n        # create dropout rng\n        dropout_rng = None\n        if not deterministic and self.dropout > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        # Softmax(QK^T)\n        attn_weights = dot_product_attention_weights(\n            query_states,\n            key_states,\n            bias=position_bias,\n            dropout_rng=dropout_rng,\n            dropout_rate=self.dropout,\n            broadcast_dropout=True,\n            deterministic=deterministic,\n            dtype=self.dtype,\n        )\n\n        if self.config.use_pjit_attention_force:\n            attn_weights = with_sharding_constraint(attn_weights, PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", None, None\n            ))\n\n        # multiply with value states\n        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value_states)\n\n        # bring back to (batch_size, seq_length, d_model)\n        attn_output = self._merge_heads(attn_output)\n\n        # apply output matrix\n        attn_output = self.o(attn_output)\n\n        outputs = (attn_output, position_bias)\n\n        if output_attentions:\n            outputs = outputs + (attn_weights,)\n\n        return outputs\n\n\nclass FlaxT5LayerSelfAttention(nn.Module):\n    config: T5Config\n    has_relative_attention_bias: bool = False\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        self.SelfAttention = FlaxT5Attention(\n            self.config,\n            has_relative_attention_bias=self.has_relative_attention_bias,\n            causal=self.config.causal,\n            dtype=self.dtype,\n        )\n        self.layer_norm = FlaxT5LayerNorm(self.config.d_model, eps=self.config.layer_norm_epsilon, dtype=self.dtype)\n        self.dropout = nn.Dropout(self.config.dropout_rate)\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            position_bias=None,\n            output_attentions=False,\n            deterministic=True,\n            init_cache=False,\n    ):\n        normed_hidden_states = self.layer_norm(hidden_states)\n        attention_output = self.SelfAttention(\n            normed_hidden_states,\n            attention_mask=attention_mask,\n            position_bias=position_bias,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            init_cache=init_cache,\n        )\n        hidden_states = hidden_states + self.dropout(attention_output[0], deterministic=deterministic)\n        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n        return outputs\n\n\nclass FlaxT5LayerCrossAttention(nn.Module):\n    config: T5Config\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        self.EncDecAttention = FlaxT5Attention(\n            self.config, has_relative_attention_bias=False, causal=False, dtype=self.dtype\n        )\n        self.layer_norm = FlaxT5LayerNorm(self.config.d_model, eps=self.config.layer_norm_epsilon, dtype=self.dtype)\n        self.dropout = nn.Dropout(self.config.dropout_rate)\n\n    def __call__(\n            self,\n            hidden_states,\n            key_value_states,\n            attention_mask=None,\n            position_bias=None,\n            output_attentions=False,\n            deterministic=True,\n    ):\n        normed_hidden_states = self.layer_norm(hidden_states)\n        attention_output = self.EncDecAttention(\n            normed_hidden_states,\n            attention_mask=attention_mask,\n            key_value_states=key_value_states,\n            position_bias=position_bias,\n            output_attentions=output_attentions,\n        )\n        hidden_states = hidden_states + self.dropout(attention_output[0], deterministic=deterministic)\n        outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n        return outputs\n\n\nclass FlaxT5Block(nn.Module):\n    config: T5Config\n    has_relative_attention_bias: bool = False\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        self.causal = self.config.causal\n        self.layer = (\n            FlaxT5LayerSelfAttention(\n                self.config,\n                has_relative_attention_bias=self.has_relative_attention_bias,\n                name=str(0),\n                dtype=self.dtype,\n            ),\n        )\n        feed_forward_index = 1\n        if self.causal:\n            self.layer += (FlaxT5LayerCrossAttention(self.config, name=str(1), dtype=self.dtype),)\n            feed_forward_index += 1\n\n        self.layer += (FlaxT5LayerFF(self.config, name=str(feed_forward_index), dtype=self.dtype),)\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            position_bias=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            encoder_decoder_position_bias=None,\n            output_attentions=False,\n            return_dict=True,\n            deterministic=True,\n            init_cache=False,\n    ):\n        self_attention_outputs = self.layer[0](\n            hidden_states,\n            attention_mask=attention_mask,\n            position_bias=position_bias,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            init_cache=init_cache,\n        )\n        hidden_states = self_attention_outputs[0]\n        attention_outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n\n        do_cross_attention = self.causal and encoder_hidden_states is not None\n        if do_cross_attention:\n            cross_attention_outputs = self.layer[1](\n                hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                position_bias=encoder_decoder_position_bias,\n                output_attentions=output_attentions,\n                deterministic=deterministic,\n            )\n            hidden_states = cross_attention_outputs[0]\n\n            # Keep cross-attention outputs and relative position weights\n            attention_outputs = attention_outputs + cross_attention_outputs[1:]\n\n        # Apply Feed Forward layer\n        hidden_states = self.layer[-1](hidden_states, deterministic=deterministic)\n\n        outputs = (hidden_states,)\n\n        outputs = outputs + attention_outputs\n\n        # returns hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights),\n        # (cross-attention position bias), (cross-attention weights)\n        return outputs\n\n\nclass FlaxT5LayerCollection(nn.Module):\n    config: T5Config\n    has_relative_attention_bias: bool\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        block = FlaxT5Block\n        if self.config.gradient_checkpointing != '':\n            block = remat(\n                block, static_argnums=(5, 6, 7, 8, 9),\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing)\n            )\n        self.layer = block(\n            self.config, has_relative_attention_bias=self.has_relative_attention_bias, dtype=self.dtype\n        )\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            position_bias=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            encoder_decoder_position_bias=None,\n            output_attentions=False,\n            deterministic=True,\n            init_cache=False,\n    ):\n        return self.layer(\n            hidden_states,\n            attention_mask=attention_mask,\n            position_bias=position_bias,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            encoder_decoder_position_bias=encoder_decoder_position_bias,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            init_cache=init_cache,\n        )\n\n\ndef get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'checkpoint_dots': jax.checkpoint_policies.checkpoint_dots,\n        'checkpoint_dots_with_no_batch_dims': jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims,\n    }[name]\n\n\nclass FlaxT5BlockCollection(nn.Module):\n    config: T5Config\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n    gradient_checkpointing: bool = False\n\n    def setup(self):\n        self.causal = self.config.causal\n        block = FlaxT5LayerCollection\n        if self.config.gradient_checkpointing != '':\n            block = remat(block,\n                          policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                          static_argnums=(6, 7, 8))\n        self.blocks = [\n            block(\n                self.config,\n                has_relative_attention_bias=(i == 0),\n                dtype=self.dtype,\n                name=str(i),\n            )\n            for i in range(self.config.num_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states=None,\n            attention_mask=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            deterministic: bool = True,\n            init_cache: bool = False,\n    ):\n        # Prepare head mask if needed\n        all_hidden_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and self.causal) else None\n        position_bias = None\n        encoder_decoder_position_bias = None\n\n        for i, layer_module in enumerate(self.blocks):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                position_bias,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                encoder_decoder_position_bias,\n                output_attentions,\n                deterministic,\n                init_cache,\n            )\n\n            hidden_states = layer_outputs[0]\n\n            position_bias = layer_outputs[1]\n\n            if self.causal and encoder_hidden_states is not None:\n                encoder_decoder_position_bias = layer_outputs[3 if output_attentions else 2]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[2],)\n                if self.causal:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n\n        return FlaxBaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            hidden_states=all_hidden_states,\n            attentions=all_attentions,\n            cross_attentions=all_cross_attentions,\n        )\n\n\nclass FlaxT5Stack(nn.Module):\n    config: T5Config\n    embed_tokens: nn.Embed\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n    gradient_checkpointing: bool = False\n\n    def setup(self):\n        self.causal = self.config.causal\n\n        self.block = FlaxT5BlockCollection(\n            self.config, dtype=self.dtype, gradient_checkpointing=self.gradient_checkpointing\n        )\n        self.final_layer_norm = FlaxT5LayerNorm(\n            self.config.d_model, eps=self.config.layer_norm_epsilon, dtype=self.dtype\n        )\n        self.dropout = nn.Dropout(self.config.dropout_rate)\n\n    def __call__(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            deterministic: bool = True,\n            init_cache: bool = False,\n    ):\n        hidden_states = self.embed_tokens(input_ids)\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n\n        outputs = self.block(\n            hidden_states,\n            attention_mask=attention_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            deterministic=deterministic,\n            init_cache=init_cache,\n        )\n\n        hidden_states = outputs[0]\n\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n\n        # Add last layer\n        all_hidden_states = None\n\n        if output_hidden_states:\n            all_hidden_states = outputs.hidden_states\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            if output_hidden_states:\n                return (\n                    hidden_states,\n                    all_hidden_states,\n                ) + outputs[2:]\n            return (hidden_states,) + outputs[1:]\n\n        return FlaxBaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            hidden_states=all_hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n\nclass FlaxT5PreTrainedModel(FlaxPreTrainedModel):\n    config_class = T5Config\n    base_model_prefix = \"transformer\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: T5Config,\n            input_shape: Tuple[int] = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.bfloat16,\n            _do_init: bool = True,\n            gradient_checkpointing: bool = False,\n            **kwargs,\n    ):\n        module = self.module_class(config=config, dtype=dtype, gradient_checkpointing=gradient_checkpointing, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def enable_gradient_checkpointing(self):\n        self._module = self.module_class(\n            config=self.config,\n            dtype=self.dtype,\n            gradient_checkpointing=True,\n        )\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # init input tensors\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n\n        attention_mask = jnp.ones_like(input_ids)\n        args = [input_ids, attention_mask]\n        if self.module_class not in [FlaxT5EncoderModule]:\n            decoder_input_ids = jnp.ones_like(input_ids)\n            decoder_attention_mask = jnp.ones_like(input_ids)\n            args.extend([decoder_input_ids, decoder_attention_mask])\n\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        random_params = self.module.init(\n            rngs,\n            *args,\n        )[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def __call__(\n            self,\n            input_ids: jnp.ndarray,\n            attention_mask: Optional[jnp.ndarray] = None,\n            decoder_input_ids: jnp.ndarray = None,\n            decoder_attention_mask: Optional[jnp.ndarray] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            train: bool = False,\n            params: dict = None,\n            dropout_rng: PRNGKey = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        if decoder_input_ids is None:\n            raise ValueError(\n                \"Make sure to provide both `input_ids` and `decoder_input_ids`. `decoder_input_ids` is not passed\"\n                \" here.\"\n            )\n\n        # prepare encoder inputs\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n\n        # prepare decoder inputs\n        if decoder_attention_mask is None:\n            decoder_attention_mask = jnp.ones_like(decoder_input_ids)\n\n        # Handle any PRNG if needed\n        rngs = {\"dropout\": dropout_rng} if dropout_rng is not None else {}\n\n        return self.module.apply(\n            {\"params\": params or self.params},\n            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),\n            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs,\n        )\n\n    def init_cache(self, batch_size, max_length, encoder_outputs):\n\n        decoder_input_ids = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        decoder_attention_mask = jnp.ones_like(decoder_input_ids)\n\n        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, **kwargs):\n            decoder_module = module._get_decoder_module()\n            return decoder_module(\n                decoder_input_ids,\n                decoder_attention_mask,\n                **kwargs,\n            )\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0),\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            init_cache=True,\n            method=_decoder_forward,  # we only need to call the decoder to init the cache\n        )\n        return unfreeze(init_variables[\"cache\"])\n\n    def encode(\n            self,\n            input_ids: jnp.ndarray,\n            attention_mask: Optional[jnp.ndarray] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            train: bool = False,\n            params: dict = None,\n            dropout_rng: PRNGKey = None,\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        def _encoder_forward(module, input_ids, attention_mask, **kwargs):\n            encode_module = module._get_encoder_module()\n            return encode_module(input_ids, attention_mask, **kwargs)\n\n        return self.module.apply(\n            {\"params\": params or self.params},\n            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs,\n            method=_encoder_forward,\n        )\n\n    def decode(\n            self,\n            decoder_input_ids,\n            encoder_outputs,\n            encoder_attention_mask: Optional[jnp.ndarray] = None,\n            decoder_attention_mask: Optional[jnp.ndarray] = None,\n            past_key_values: dict = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            train: bool = False,\n            params: dict = None,\n            dropout_rng: PRNGKey = None,\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        encoder_hidden_states = encoder_outputs[0]\n        if encoder_attention_mask is None:\n            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n            encoder_attention_mask = jnp.ones((batch_size, sequence_length))\n\n        batch_size, sequence_length = decoder_input_ids.shape\n        if decoder_attention_mask is None:\n            decoder_attention_mask = jnp.ones((batch_size, sequence_length))\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params}\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, **kwargs):\n            decoder_module = module._get_decoder_module()\n            return decoder_module(\n                decoder_input_ids,\n                decoder_attention_mask,\n                **kwargs,\n            )\n\n        outputs = self.module.apply(\n            inputs,\n            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),\n            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=jnp.array(encoder_attention_mask, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs,\n            mutable=mutable,\n            method=_decoder_forward,\n        )\n\n        # add updated cache to model output\n        if past_key_values is not None and return_dict:\n            outputs, past = outputs\n            outputs[\"past_key_values\"] = unfreeze(past[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past = outputs\n            outputs = outputs[:1] + (unfreeze(past[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxT5Module(nn.Module):\n    config: T5Config\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n    gradient_checkpointing: bool = False\n\n    def _get_encoder_module(self):\n        return self.encoder\n\n    def _get_decoder_module(self):\n        return self.decoder\n\n    def setup(self):\n        self.shared = nn.Embed(\n            self.config.vocab_size,\n            self.config.d_model,\n            embedding_init=jax.nn.initializers.normal(self.config.initializer_factor * 1.0),\n            dtype=self.dtype,\n        )\n\n        encoder_config = copy.deepcopy(self.config)\n        encoder_config.causal = False\n        self.encoder = FlaxT5Stack(\n            encoder_config,\n            embed_tokens=self.shared,\n            dtype=self.dtype,\n            gradient_checkpointing=self.gradient_checkpointing,\n        )\n\n        decoder_config = copy.deepcopy(self.config)\n        decoder_config.causal = True\n        decoder_config.num_layers = self.config.num_decoder_layers\n        self.decoder = FlaxT5Stack(\n            decoder_config,\n            embed_tokens=self.shared,\n            dtype=self.dtype,\n            gradient_checkpointing=self.gradient_checkpointing,\n        )\n\n    def __call__(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            encoder_outputs=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            deterministic: bool = True,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # Encode if needed (training, first prediction pass)\n        encoder_outputs = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        # Decode\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return FlaxSeq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n\nclass FlaxT5Model(FlaxT5PreTrainedModel):\n    module_class = FlaxT5Module\n\n\nclass FlaxT5EncoderModule(nn.Module):\n    config: T5Config\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n    gradient_checkpointing: bool = False\n\n    def setup(self):\n        self.shared = nn.Embed(\n            self.config.vocab_size,\n            self.config.d_model,\n            embedding_init=jax.nn.initializers.normal(self.config.initializer_factor * 1.0),\n            dtype=self.dtype,\n        )\n\n        encoder_config = copy.deepcopy(self.config)\n        encoder_config.is_decoder = False\n        encoder_config.is_encoder_decoder = False\n        encoder_config.causal = False\n        self.encoder = FlaxT5Stack(\n            encoder_config,\n            embed_tokens=self.shared,\n            dtype=self.dtype,\n            gradient_checkpointing=self.gradient_checkpointing,\n        )\n\n    def __call__(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            output_attentions=False,\n            output_hidden_states=False,\n            return_dict: bool = True,\n            deterministic: bool = True,\n    ):\n        # Encode if needed (training, first prediction pass)\n        encoder_outputs = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        return encoder_outputs\n\n\nclass FlaxT5EncoderModel(FlaxT5PreTrainedModel):\n    module_class = FlaxT5EncoderModule\n\n    def __call__(\n            self,\n            input_ids: jnp.ndarray,\n            attention_mask: Optional[jnp.ndarray] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            train: bool = False,\n            params: dict = None,\n            dropout_rng: PRNGKey = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        # prepare encoder inputs\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n\n        # Handle any PRNG if needed\n        rngs = {\"dropout\": dropout_rng} if dropout_rng is not None else {}\n\n        return self.module.apply(\n            {\"params\": params or self.params},\n            input_ids=jnp.array(input_ids, dtype=\"i4\"),\n            attention_mask=jnp.array(attention_mask, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs,\n        )\n\n\nclass FlaxT5ForConditionalGenerationModule(nn.Module):\n    config: T5Config\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n    gradient_checkpointing: bool = False\n\n    def _get_encoder_module(self):\n        return self.encoder\n\n    def _get_decoder_module(self):\n        return self.decoder\n\n    def setup(self):\n        self.model_dim = self.config.d_model\n\n        self.shared = nn.Embed(\n            self.config.vocab_size,\n            self.config.d_model,\n            embedding_init=jax.nn.initializers.normal(self.config.initializer_factor),\n            dtype=self.dtype,\n        )\n\n        encoder_config = copy.deepcopy(self.config)\n        encoder_config.causal = False\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = FlaxT5Stack(\n            encoder_config, self.shared, dtype=self.dtype, gradient_checkpointing=self.gradient_checkpointing\n        )\n\n        decoder_config = copy.deepcopy(self.config)\n        decoder_config.causal = True\n        decoder_config.is_encoder_decoder = False\n        decoder_config.num_layers = self.config.num_decoder_layers\n        self.decoder = FlaxT5Stack(\n            decoder_config, self.shared, dtype=self.dtype, gradient_checkpointing=self.gradient_checkpointing\n        )\n\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_factor),\n            dtype=self.dtype,\n        )\n\n    def __call__(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            decoder_input_ids=None,\n            decoder_attention_mask=None,\n            encoder_outputs=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            deterministic: bool = True,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # Encode\n        encoder_outputs = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        hidden_states = encoder_outputs[0]\n\n        # Decode\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=hidden_states,\n            encoder_attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=deterministic,\n        )\n\n        sequence_output = decoder_outputs[0]\n\n        if self.config.tie_word_embeddings:\n            sequence_output = sequence_output * (self.model_dim ** -0.5)\n\n        if self.config.tie_word_embeddings:\n            shared_embedding = self.shared.variables[\"params\"][\"embedding\"]\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, sequence_output)\n        else:\n            lm_logits = self.lm_head(sequence_output)\n\n        if not return_dict:\n            return (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n\n        return FlaxSeq2SeqLMOutput(\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n\nclass FlaxT5ForConditionalGeneration(FlaxT5PreTrainedModel):\n    module_class = FlaxT5ForConditionalGenerationModule\n\n    def decode(\n            self,\n            decoder_input_ids,\n            encoder_outputs,\n            encoder_attention_mask: Optional[jnp.ndarray] = None,\n            decoder_attention_mask: Optional[jnp.ndarray] = None,\n            past_key_values: dict = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            train: bool = False,\n            params: dict = None,\n            dropout_rng: PRNGKey = None,\n    ):\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        encoder_hidden_states = encoder_outputs[0]\n        if encoder_attention_mask is None:\n            batch_size, sequence_length = encoder_hidden_states.shape[:2]\n            encoder_attention_mask = jnp.ones((batch_size, sequence_length))\n\n        batch_size, sequence_length = decoder_input_ids.shape\n        if decoder_attention_mask is None:\n            decoder_attention_mask = jnp.ones((batch_size, sequence_length))\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params}\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        def _decoder_forward(module, decoder_input_ids, decoder_attention_mask, **kwargs):\n            decoder_module = module._get_decoder_module()\n            decoder_outputs = decoder_module(\n                decoder_input_ids,\n                decoder_attention_mask,\n                **kwargs,\n            )\n\n            sequence_output = decoder_outputs[0]\n\n            if self.config.tie_word_embeddings:\n                sequence_output = sequence_output * (self.config.d_model ** -0.5)\n\n            if self.config.tie_word_embeddings:\n                shared_embedding = module.shared.variables[\"params\"][\"embedding\"]\n                lm_logits = module.lm_head.apply({\"params\": {\"kernel\": shared_embedding.T}}, sequence_output)\n            else:\n                lm_logits = module.lm_head(sequence_output)\n\n            return lm_logits, decoder_outputs\n\n        outputs = self.module.apply(\n            inputs,\n            decoder_input_ids=jnp.array(decoder_input_ids, dtype=\"i4\"),\n            decoder_attention_mask=jnp.array(decoder_attention_mask, dtype=\"i4\"),\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=jnp.array(encoder_attention_mask, dtype=\"i4\"),\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            deterministic=not train,\n            rngs=rngs,\n            mutable=mutable,\n            method=_decoder_forward,\n        )\n\n        if past_key_values is None:\n            lm_logits, decoder_outputs = outputs\n        else:\n            (lm_logits, decoder_outputs), past = outputs\n\n        if return_dict:\n            outputs = FlaxCausalLMOutputWithCrossAttentions(\n                logits=lm_logits,\n                hidden_states=decoder_outputs.hidden_states,\n                attentions=decoder_outputs.attentions,\n                cross_attentions=decoder_outputs.cross_attentions,\n            )\n        else:\n            outputs = (lm_logits,) + decoder_outputs[1:]\n\n        # add updated cache to model output\n        if past_key_values is not None and return_dict:\n            outputs[\"past_key_values\"] = unfreeze(past[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs = outputs[:1] + (unfreeze(past[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n    def prepare_inputs_for_generation(\n            self,\n            decoder_input_ids,\n            max_length,\n            attention_mask: Optional[chex.Array] = None,\n            decoder_attention_mask: Optional[chex.Array] = None,\n            encoder_outputs=None,\n            **kwargs,\n    ):\n        # initializing the cache\n        batch_size, seq_length = decoder_input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length, encoder_outputs)\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if decoder_attention_mask is not None:\n            extended_attention_mask = jax.lax.dynamic_update_slice(\n                extended_attention_mask, decoder_attention_mask, (0, 0)\n            )\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"encoder_outputs\": encoder_outputs,\n            \"encoder_attention_mask\": attention_mask,\n            \"decoder_attention_mask\": extended_attention_mask,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        return model_kwargs\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/t5/modelling_t5_flax.py b/lib/python/EasyDel/modules/t5/modelling_t5_flax.py
--- a/lib/python/EasyDel/modules/t5/modelling_t5_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/t5/modelling_t5_flax.py	(date 1703671032681)
@@ -16,7 +16,7 @@
 """ Flax T5 model."""
 
 import copy
-from typing import Callable, Optional, Tuple, Sequence
+from typing import Callable, Optional, Tuple
 
 import flax.linen as nn
 import jax
@@ -40,15 +40,14 @@
     FlaxPreTrainedModel,
 )
 
-from transformers import PretrainedConfig
 from jax.sharding import PartitionSpec
 
 from ..flax_modelling_utils import get_gradient_checkpoint_policy, \
-    with_sharding_constraint, JaxBaseClassModel
+    with_sharding_constraint
 
 import chex
 from .t5_configuration import T5Config
-
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 remat = nn_partitioning.remat
 
 
@@ -785,7 +784,7 @@
         )
 
 
-class FlaxT5PreTrainedModel(FlaxPreTrainedModel):
+class FlaxT5PreTrainedModel(EasyDelFlaxPretrainedModel):
     config_class = T5Config
     base_model_prefix = "transformer"
     module_class: nn.Module = None
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"AutoImportSettings\">\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\n  </component>\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"e9058a88-3ea4-4b63-a9f8-ea56f3a88630\" name=\"Changes\" comment=\"\">\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/AvailableModels.md\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/docs/AvailableModels.md\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-configs-configs.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-data_preprocessing-_processor.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-erros-errors.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-eval-lm_eval.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-linen-bits.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-linen-utils.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-auto_models.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-falcon-modelling_falcon_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-flax_modelling_utils.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-gpt2-modelling_gpt2_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-gpt_j-modelling_gpt_j_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-gpt_neo_x-modelling_gpt_neo_x_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-llama-modelling_llama_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-lucid_transformer-modelling_lt_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-mistral-modelling_mistral_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-mixtral-modelling_mixtral_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-opt-modelling_opt_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-palm-modelling_palm_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-phi-modelling_phi_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-modules-t5-modelling_t5_flax.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-partitioning-partitioner.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-core.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-models-modelling_base.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-models-modelling_value_head.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-trainer-base.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-trainer-ppo_config.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-trainer-ppo_trainer.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rlhf-ppo.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rlhf-reward.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rlhf-trainer.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-rlhf-utils.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-serve-jax_serve.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-serve-torch_serve.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-serve-utils.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-smi-smi.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-trainer-config.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-trainer-fsdp_train.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-trainer-tf_dataset.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-trainer-training_utils.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-transform-easydel_transform.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-transform-falcon.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-transform-llama.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-transform-mistral.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-transform-mpt.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-transform-utils.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-utils-checker.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-utils-prompters.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-utils-tensor_utils.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/docs/lib-python-EasyDel-utils-utils.md\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/generate_documentations.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/generate_documentations.py\" afterDir=\"false\" />\n    </list>\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"FileTemplateManagerImpl\">\n    <option name=\"RECENT_TEMPLATES\">\n      <list>\n        <option value=\"Python Script\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\n      <map>\n        <entry key=\"$PROJECT_DIR$\" value=\"main\" />\n      </map>\n    </option>\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n  </component>\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProblemsViewState\">\n    <option name=\"selectedTabId\" value=\"CurrentFile\" />\n  </component>\n  <component name=\"ProjectColorInfo\">{\n  &quot;customColor&quot;: &quot;&quot;,\n  &quot;associatedIndex&quot;: 6\n}</component>\n  <component name=\"ProjectId\" id=\"2ZXHOtjPa3CdcVdlW1mCznNWdgB\" />\n  <component name=\"ProjectLevelVcsManager\">\n    <ConfirmationsSetting value=\"2\" id=\"Add\" />\n  </component>\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\"><![CDATA[{\n  \"keyToString\": {\n    \"RunOnceActivity.OpenProjectViewOnStart\": \"true\",\n    \"RunOnceActivity.ShowReadmeOnStart\": \"true\",\n    \"WebServerToolWindowFactoryState\": \"false\",\n    \"git-widget-placeholder\": \"main\",\n    \"last_opened_file_path\": \"/home/erfan/PycharmProjects/EasyDeL/lib/python/EasyDel/trainer\",\n    \"node.js.detected.package.eslint\": \"true\",\n    \"node.js.detected.package.tslint\": \"true\",\n    \"node.js.selected.package.eslint\": \"(autodetect)\",\n    \"node.js.selected.package.tslint\": \"(autodetect)\",\n    \"nodejs_package_manager_path\": \"npm\",\n    \"vue.rearranger.settings.migration\": \"true\"\n  }\n}]]></component>\n  <component name=\"RecentsManager\">\n    <key name=\"CopyFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$/lib/python/EasyDel/trainer\" />\n      <recent name=\"$PROJECT_DIR$/lib/python/EasyDel\" />\n    </key>\n    <key name=\"MoveFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$\" />\n      <recent name=\"$PROJECT_DIR$/lib/python/EasyDel\" />\n    </key>\n  </component>\n  <component name=\"RunManager\" selected=\"Python.generate_documentations\">\n    <configuration name=\"env\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/lib/python/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/env.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"generate_documentations\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/generate_documentations.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"llama_compration\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/python_test\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/python_test/llama_compration.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"llama_flax\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/python_test\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/python_test/llama_flax.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"mixtral_flax\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/python_test\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/python_test/mixtral_flax.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <recent_temporary>\n      <list>\n        <item itemvalue=\"Python.generate_documentations\" />\n        <item itemvalue=\"Python.env\" />\n        <item itemvalue=\"Python.llama_flax\" />\n        <item itemvalue=\"Python.mixtral_flax\" />\n        <item itemvalue=\"Python.llama_compration\" />\n      </list>\n    </recent_temporary>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"e9058a88-3ea4-4b63-a9f8-ea56f3a88630\" name=\"Changes\" comment=\"\" />\n      <created>1702561475154</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1702561475154</updated>\n      <workItem from=\"1702561476600\" duration=\"2879000\" />\n      <workItem from=\"1702564997855\" duration=\"1585000\" />\n      <workItem from=\"1702570696725\" duration=\"5776000\" />\n      <workItem from=\"1702625797888\" duration=\"1904000\" />\n      <workItem from=\"1702627837344\" duration=\"3368000\" />\n      <workItem from=\"1702795436603\" duration=\"757000\" />\n      <workItem from=\"1702799337712\" duration=\"11633000\" />\n      <workItem from=\"1702811433240\" duration=\"3570000\" />\n      <workItem from=\"1702830071828\" duration=\"8103000\" />\n      <workItem from=\"1703583904578\" duration=\"15895000\" />\n    </task>\n    <servers />\n  </component>\n  <component name=\"TypeScriptGeneratedFilesManager\">\n    <option name=\"version\" value=\"3\" />\n  </component>\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\n    <SUITE FILE_PATH=\"coverage/EasyDeL$generate_documentations.coverage\" NAME=\"generate_documentations Coverage Results\" MODIFIED=\"1703602214900\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$llama_compration.coverage\" NAME=\"llama_compration Coverage Results\" MODIFIED=\"1701936876438\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$env.coverage\" NAME=\"env Coverage Results\" MODIFIED=\"1703599174198\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/lib/python/\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$mistral_flax.coverage\" NAME=\"mistral_flax Coverage Results\" MODIFIED=\"1701867908351\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$llama_flax.coverage\" NAME=\"llama_flax Coverage Results\" MODIFIED=\"1703585720752\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$mixtral_flax.coverage\" NAME=\"mixtral_flax Coverage Results\" MODIFIED=\"1702977128554\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$falcon_flax.coverage\" NAME=\"falcon_flax Coverage Results\" MODIFIED=\"1701860359932\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$phi_flax.coverage\" NAME=\"phi_flax Coverage Results\" MODIFIED=\"1701867632398\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/.idea/workspace.xml	(date 1703670652480)
@@ -5,59 +5,35 @@
   </component>
   <component name="ChangeListManager">
     <list default="true" id="e9058a88-3ea4-4b63-a9f8-ea56f3a88630" name="Changes" comment="">
+      <change afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/easydel_modelling_utils.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/AvailableModels.md" beforeDir="false" afterPath="$PROJECT_DIR$/docs/AvailableModels.md" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-configs-configs.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-data_preprocessing-_processor.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-erros-errors.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-eval-lm_eval.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-linen-bits.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-linen-utils.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-auto_models.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-falcon-modelling_falcon_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-flax_modelling_utils.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-gpt2-modelling_gpt2_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-gpt_j-modelling_gpt_j_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-gpt_neo_x-modelling_gpt_neo_x_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-llama-modelling_llama_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-lucid_transformer-modelling_lt_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-mistral-modelling_mistral_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-mixtral-modelling_mixtral_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-opt-modelling_opt_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-palm-modelling_palm_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-phi-modelling_phi_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-modules-t5-modelling_t5_flax.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-partitioning-partitioner.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-core.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-models-modelling_base.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-models-modelling_value_head.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-trainer-base.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-trainer-ppo_config.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rl_trainer-trainer-ppo_trainer.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rlhf-ppo.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rlhf-reward.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rlhf-trainer.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-rlhf-utils.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-serve-jax_serve.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-serve-torch_serve.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-serve-utils.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-smi-smi.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-trainer-config.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-trainer-fsdp_train.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-trainer-tf_dataset.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-trainer-training_utils.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-transform-easydel_transform.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-transform-falcon.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-transform-llama.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-transform-mistral.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-transform-mpt.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-transform-utils.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-utils-checker.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-utils-prompters.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-utils-tensor_utils.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/docs/lib-python-EasyDel-utils-utils.md" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/generate_documentations.py" beforeDir="false" afterPath="$PROJECT_DIR$/generate_documentations.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/falcon/falcon_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/falcon/falcon_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/flax_modelling_utils.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/flax_modelling_utils.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt2/gpt2_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt2/gpt2_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt2/modelling_gpt2_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt2/modelling_gpt2_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt_j/gpt_j_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt_j/gpt_j_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt_j/modelling_gpt_j_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt_j/modelling_gpt_j_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt_neo_x/gpt_neo_x_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt_neo_x/gpt_neo_x_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt_neo_x/modelling_gpt_neo_x_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/gpt_neo_x/modelling_gpt_neo_x_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/llama/llama_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/llama/llama_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/llama/modelling_llama_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/llama/modelling_llama_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/lucid_transformer/lt_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/lucid_transformer/lt_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/mistral/mistral_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mistral/mistral_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/mixtral/mixtral_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mixtral/mixtral_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/mosaic_mpt/mosaic_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mosaic_mpt/mosaic_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/opt/modelling_opt_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/opt/modelling_opt_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/opt/opt_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/opt/opt_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/palm/modelling_palm_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/palm/modelling_palm_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/palm/palm_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/palm/palm_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/phi/modelling_phi_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/phi/modelling_phi_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/phi/phi_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/phi/phi_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/t5/modelling_t5_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/t5/modelling_t5_flax.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/t5/t5_configuration.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/t5/t5_configuration.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/reinforcement_learning/models/modelling_base.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/reinforcement_learning/models/modelling_base.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -71,6 +47,17 @@
       </list>
     </option>
   </component>
+  <component name="FlaskConsoleOptions" custom-start-script="import sys&#10;sys.path.extend([WORKING_DIR_AND_PYTHON_PATHS])&#10;from flask.cli import ScriptInfo&#10;locals().update(ScriptInfo(create_app=None).load_app().make_shell_context())&#10;print(&quot;Python %s on %s\nApp: %s [%s]\nInstance: %s&quot; % (sys.version, sys.platform, app.import_name, app.env, app.instance_path))">
+    <envs>
+      <env key="FLASK_APP" value="app" />
+    </envs>
+    <option name="myCustomStartScript" value="import sys&#10;sys.path.extend([WORKING_DIR_AND_PYTHON_PATHS])&#10;from flask.cli import ScriptInfo&#10;locals().update(ScriptInfo(create_app=None).load_app().make_shell_context())&#10;print(&quot;Python %s on %s\nApp: %s [%s]\nInstance: %s&quot; % (sys.version, sys.platform, app.import_name, app.env, app.instance_path))" />
+    <option name="myEnvs">
+      <map>
+        <entry key="FLASK_APP" value="app" />
+      </map>
+    </option>
+  </component>
   <component name="Git.Settings">
     <option name="RECENT_BRANCH_BY_REPOSITORY">
       <map>
@@ -79,6 +66,9 @@
     </option>
     <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
   </component>
+  <component name="HighlightingSettingsPerFile">
+    <setting file="file://$USER_HOME$/venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py" root0="SKIP_INSPECTION" />
+  </component>
   <component name="MarkdownSettingsMigration">
     <option name="stateVersion" value="1" />
   </component>
@@ -109,6 +99,7 @@
     "node.js.selected.package.eslint": "(autodetect)",
     "node.js.selected.package.tslint": "(autodetect)",
     "nodejs_package_manager_path": "npm",
+    "settings.editor.selected.configurable": "preferences.pluginManager",
     "vue.rearranger.settings.migration": "true"
   }
 }]]></component>
@@ -235,11 +226,11 @@
     </configuration>
     <recent_temporary>
       <list>
-        <item itemvalue="Python.generate_documentations" />
-        <item itemvalue="Python.env" />
-        <item itemvalue="Python.llama_flax" />
         <item itemvalue="Python.mixtral_flax" />
+        <item itemvalue="Python.llama_flax" />
         <item itemvalue="Python.llama_compration" />
+        <item itemvalue="Python.generate_documentations" />
+        <item itemvalue="Python.env" />
       </list>
     </recent_temporary>
   </component>
@@ -260,7 +251,8 @@
       <workItem from="1702799337712" duration="11633000" />
       <workItem from="1702811433240" duration="3570000" />
       <workItem from="1702830071828" duration="8103000" />
-      <workItem from="1703583904578" duration="15895000" />
+      <workItem from="1703583904578" duration="16457000" />
+      <workItem from="1703665326086" duration="5239000" />
     </task>
     <servers />
   </component>
@@ -268,7 +260,7 @@
     <option name="version" value="3" />
   </component>
   <component name="com.intellij.coverage.CoverageDataManagerImpl">
-    <SUITE FILE_PATH="coverage/EasyDeL$generate_documentations.coverage" NAME="generate_documentations Coverage Results" MODIFIED="1703602214900" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/EasyDeL$generate_documentations.coverage" NAME="generate_documentations Coverage Results" MODIFIED="1703602463609" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/EasyDeL$llama_compration.coverage" NAME="llama_compration Coverage Results" MODIFIED="1701936876438" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/python_test" />
     <SUITE FILE_PATH="coverage/EasyDeL$env.coverage" NAME="env Coverage Results" MODIFIED="1703599174198" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/lib/python/" />
     <SUITE FILE_PATH="coverage/EasyDeL$mistral_flax.coverage" NAME="mistral_flax Coverage Results" MODIFIED="1701867908351" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/python_test" />
Index: lib/python/EasyDel/modules/easydel_modelling_utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/easydel_modelling_utils.py b/lib/python/EasyDel/modules/easydel_modelling_utils.py
new file mode 100644
--- /dev/null	(date 1703667020111)
+++ b/lib/python/EasyDel/modules/easydel_modelling_utils.py	(date 1703667020111)
@@ -0,0 +1,247 @@
+from jax.experimental.mesh_utils import create_device_mesh
+from transformers import PretrainedConfig, FlaxPreTrainedModel
+import jax
+from jax import numpy as jnp
+from typing import Sequence, Union, Optional
+from dataclasses import dataclass
+
+
+@dataclass
+class EasyMethod:
+    TRAIN: str = "train"
+    SERVE: str = "serve"
+    EVAL: str = "serve"
+    CONVERT: str = "convert"
+
+
+class EasyDelPretrainedConfig(PretrainedConfig):
+    """
+    It initializes all the attributes of an object, and it's called when you create a new instance of that class.
+    :param self: Refer to the instance of the class
+    :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis
+    :param axis_names: Sequence[str]: Set the names of the axes
+    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor
+    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix
+    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor
+    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec
+    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights
+    :param use_shard_map: bool: whenever to use shard_map for attention
+    :param backend: Optional[None]: Specify the backend to use
+    :param easy_method: EasyMethod: Specify the use of model to init the QDot Method for (e.q TRAIN,SERVE,...)
+    """
+
+    def __init__(
+            self,
+            axis_dims: Sequence[int] = (1, -1, 1, 1),
+            axis_names: Sequence[str] = ("dp", "fsdp", "tp", "sp"),
+            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp", "tp", None),
+            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp", "tp", None),
+            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp", "tp", None),
+            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), None, None, None),
+            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp", "tp", None),
+            use_shard_map: bool = False,
+            backend: Optional[None] = None,
+            easy_method: EasyMethod = EasyMethod.TRAIN,
+            **kwargs
+    ):
+        self.q_ps = q_ps
+        self.k_ps = k_ps
+        self.v_ps = v_ps
+        self.b_ps = b_ps
+        self.a_ps = a_ps
+        self.use_shard_map = use_shard_map
+        self.axis_dims = axis_dims
+        self.axis_names = axis_names
+        self.backend = backend if backend is not None else ""
+        self.easy_method = easy_method
+        super().__init__(**kwargs)
+
+    @staticmethod
+    def create_mesh(
+            axis_dims: Sequence[int] = (1, -1, 1, 1), axis_names: Sequence[str] = ("dp", "fsdp", "tp", "sp"), backend=""
+    ):
+        """
+        The create_mesh function creates a mesh object that can be used to shard arrays.
+
+        :param axis_dims: Sequence[int]: Specify the dimensions of the mesh
+        :param axis_names: Sequence[str]: Name the axes of the mesh
+        :param backend: Specify the backend to use
+        :return: A mesh object
+
+        """
+        array_devices = jax.numpy.ones(
+            (len(jax.devices() if backend == "" else jax.devices(backend)), 1))
+        resh = array_devices.reshape(axis_dims).shape
+
+        return jax.sharding.Mesh(
+            create_device_mesh(resh), axis_names
+        )
+
+    def jax_mesh(self) -> jax.sharding.Mesh:
+        """
+        The jax_mesh function is a helper function that creates a jax.sharding.Mesh object from the
+        axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively.
+        The backend attribute is also used if it exists.
+
+        :param self: Refer to the object itself
+        :return: A jaxMesh
+
+        """
+        return self.create_mesh(
+            axis_dims=self.axis_dims,
+            axis_names=self.axis_names,
+            backend=(self.backend if self.backend is not None else "") if hasattr(
+                self, 'backend') else ""
+        )
+
+    def get_partition_rules(self, fully_fsdp: bool = True):
+        if not fully_fsdp:
+            raise NotImplementedError
+        else:
+            return (
+                ('.*', jax.sharding.PartitionSpec(("fsdp", "sp")))
+            )
+
+    def get_axis_dims(self) -> Sequence[int]:
+        """
+        The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.
+
+        :param self: Represent the instance of the class
+        :return: The dimensions of the axes
+
+        """
+        return self.axis_dims
+
+    def get_axis_names(self) -> Sequence[str]:
+        """
+        The get_axis_names function returns a list of the names of the axes.
+
+        :param self: Represent the instance of the class
+        :return: A list of the names of all axes
+
+        """
+        return self.axis_names
+
+    def get_backend(self) -> str:
+        """
+        The get_backend function returns the backend that is currently being used.
+        If no backend has been set, it will return the default JAX backend.
+
+        :param self: Bind the method to an object
+        :return: The backend platform
+
+        """
+        return self.backend if not self.backend == "" else jax.lib.xla_bridge.get_backend().platform
+
+    def add_partitions(
+            self,
+            axis_dims: Sequence[int] = (1, -1, 1, 1),
+            axis_names: Sequence[str] = ("dp", "fsdp", "tp", "sp"),
+            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp", "tp", None
+            ),
+            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp", "tp", None
+            ),
+            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp", "tp", None
+            ),
+            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), None, None, None
+            ),
+            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp", "tp", None
+            ),
+            use_shard_map: bool = False,
+            backend: Optional[str] = None,
+    ):
+        """
+            It initializes all the attributes of an object, and it's called when you create a new instance of that class.
+            :param self: Refer to the instance of the class
+            :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis
+            :param axis_names: Sequence[str]: Set the names of the axes
+            :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor
+            :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix
+            :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor
+            :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec
+            :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights
+            :param use_shard_map: bool: whenever to use shard_map for attention
+            :param backend: Optional[None]: Specify the backend to use
+            """
+        self.axis_dims = axis_dims
+        self.axis_names = axis_names
+        self.q_ps = q_ps
+        self.k_ps = k_ps
+        self.v_ps = v_ps
+        self.b_ps = b_ps
+        self.a_ps = a_ps
+        self.backend = backend
+        self.use_shard_map = use_shard_map
+
+
+class EasyDelFlaxPretrainedModel(FlaxPreTrainedModel):
+
+    def get_input_embeddings(self):
+        """
+        The get_input_embeddings function returns the embedding layer of the model.
+
+        :param self: Refer to the current object
+        :return: The embedding layer of the model
+        """
+        raise NotImplementedError()
+
+    def set_input_embeddings(self, value):
+        """
+        The set_input_embeddings function is used to set the embedding module of the model.
+
+        :param self: Represent the instance of the class
+        :param value: Set the embeddings of the model
+        """
+        raise NotImplementedError()
+
+    def get_output_embeddings(self):
+        """
+        The get_output_embeddings function returns the output embeddings of a model.
+
+        :param self: Represent the instance of the class
+        :return: The output embeddings of the model
+        """
+        raise NotImplementedError()
+
+    def set_output_embeddings(self, new_embeddings):
+        """
+        The set_output_embeddings function is used to set the output embeddings of a model.
+        This function can be used to change the output embedding layer of a pretrained model in order to finetune it
+        to some downstream task. Changing this layer has an effect only if the model has already been fine-tuned on some
+        task (e.g., for classification). If you are training your own language models, you should call this function before
+        you start training.
+
+        :param self: Represent the instance of the class
+        :param new_embeddings: Set the embeddings of the output layer
+        :return: A new embedding layer
+        """
+        raise NotImplementedError()
+
+    def set_decoder(self, decoder):
+        """
+        The set_decoder function is used to set the decoder for a given encoder.
+
+        :param self: Refer to the object itself
+        :param decoder: Set the decoder for a given encoder
+        :return: A decoder
+        """
+        raise NotImplementedError()
+
+    def get_decoder(self):
+        """
+        The get_decoder function is used to create a decoder object.
+
+        :param self: Represent the instance of the class
+        :return: A decoder object
+        """
+        raise NotImplementedError()
Index: lib/python/EasyDel/modules/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from .llama import (\n    FlaxLlamaModel as FlaxLlamaModel,\n    FlaxLlamaForCausalLM as FlaxLlamaForCausalLM,\n    FlaxLlamaForSequenceClassification as FlaxLlamaForSequenceClassification,\n    LlamaConfig as LlamaConfig\n)\nfrom .gpt_j import (\n    FlaxGPTJModel as FlaxGPTJModel,\n    FlaxGPTJForCausalLM as FlaxGPTJForCausalLM,\n    GPTJConfig as GPTJConfig\n)\nfrom .gpt2 import (\n    FlaxGPT2Model as FlaxGPT2Model,\n    FlaxGPT2LMHeadModel as FlaxGPT2LMHeadModel,\n    GPT2Config as GPT2Config,\n)\nfrom .lucid_transformer import (\n    FlaxLTModel as FlaxLTModel,\n    FlaxLTForCausalLM as FlaxLTForCausalLM,\n    FlaxLTConfig as FlaxLTConfig,\n)\nfrom .mosaic_mpt import (\n    FlaxMptModel as FlaxMptModel,\n    FlaxMptForCausalLM as FlaxMptForCausalLM,\n    MptConfig as MptConfig,\n)\nfrom .falcon import (\n    FlaxFalconModel as FlaxFalconModel,\n    FlaxFalconForCausalLM as FlaxFalconForCausalLM,\n    FalconConfig as FalconConfig,\n)\nfrom .gpt_neo_x import (\n    FlaxGPTNeoXModel as FlaxGPTNeoXModel,\n    FlaxGPTNeoXForCausalLM as FlaxGPTNeoXForCausalLM,\n    GPTNeoXConfig as GPTNeoXConfig,\n)\nfrom .palm import (\n    FlaxPalmModel as FlaxPalmModel,\n    FlaxPalmForCausalLM as FlaxPalmForCausalLM,\n    PalmConfig as PalmConfig,\n)\nfrom .t5 import (\n    FlaxT5Model as FlaxT5Model,\n    FlaxT5ForConditionalGeneration as FlaxT5ForConditionalGeneration,\n    T5Config as T5Config,\n)\nfrom .opt import (\n    FlaxOPTModel as FlaxOPTModel,\n    FlaxOPTForCausalLM as FlaxOPTForCausalLM,\n    OPTConfig as OPTConfig,\n)\nfrom .mistral import (\n    FlaxMistralModel as FlaxMistralModel,\n    FlaxMistralForCausalLM as FlaxMistralForCausalLM,\n    MistralConfig as MistralConfig,\n)\nfrom .mixtral import (\n    FlaxMixtralModel as FlaxMixtralModel,\n    FlaxMixtralForCausalLM as FlaxMixtralForCausalLM,\n    MixtralConfig as MixtralConfig,\n)\nfrom .auto_models import AutoEasyDelModelForCausalLM\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/__init__.py b/lib/python/EasyDel/modules/__init__.py
--- a/lib/python/EasyDel/modules/__init__.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/__init__.py	(date 1703671032689)
@@ -59,4 +59,4 @@
     FlaxMixtralForCausalLM as FlaxMixtralForCausalLM,
     MixtralConfig as MixtralConfig,
 )
-from .auto_models import AutoEasyDelModelForCausalLM
+from .auto_easydel_model import AutoEasyDelModelForCausalLM
Index: lib/python/EasyDel/modules/flax_modelling_utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import dataclasses\nimport functools\n\nimport fjformer.attention\nimport transformers\nfrom fjformer.bits import config as q_config, q_flax\nfrom jax.interpreters import pxla\nfrom jax.experimental.pjit import with_sharding_constraint as wsc\nimport jax\nfrom flax import linen as nn\nfrom functools import partial\nimport chex\nfrom typing import Sequence, Optional\nfrom jax.experimental.mesh_utils import create_device_mesh\nfrom jax.experimental.shard_map import shard_map\n\nACT2FN = {\n    \"gelu\": partial(nn.gelu, approximate=False),\n    \"relu\": nn.relu,\n    \"silu\": nn.swish,\n    \"swish\": nn.swish,\n    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"tanh\": nn.tanh,\n    \"sigmoid\": nn.sigmoid,\n    \"leaky_relu\": partial(nn.leaky_relu, negative_slope=0.01),\n    \"glu\": nn.glu,\n    \"elu\": nn.elu,\n    \"softmax\": nn.softmax\n}\n\n\ndef canonicalize_dtype(\n        *args, dtype: Optional[chex.ArrayDType] = None, inexact: bool = True\n) -> chex.ArrayDType:\n    \"\"\"Canonicalize an optional dtype to the definitive dtype.\n\n    If the ``dtype`` is None this function will infer the dtype. If it is not\n    None it will be returned unmodified or an exceptions is raised if the dtype\n    is invalid.\n    from the input arguments using ``jnp.result_type``.\n\n    Args:\n      *args: JAX array compatible values. None values\n        are ignored.\n      dtype: Optional dtype override. If specified the arguments are cast to\n        the specified dtype instead and dtype inference is disabled.\n      inexact: When True, the output dtype must be a subdtype\n      of `jnp.inexact`. Inexact dtypes are real or complex floating points. This\n      is useful when you want to apply operations that don't work directly on\n      integers like taking a mean for example.\n    Returns:\n      The dtype that *args should be cast to.\n    \"\"\"\n    if dtype is None:\n        args_filtered = [jax.numpy.asarray(x) for x in args if x is not None]\n        dtype = jax.numpy.result_type(*args_filtered)\n        if inexact and not jax.numpy.issubdtype(dtype, jax.numpy.inexact):\n            dtype = jax.numpy.promote_types(jax.numpy.float32, dtype)\n    if inexact and not jax.numpy.issubdtype(dtype, jax.numpy.inexact):\n        raise ValueError(f'Dtype must be inexact: {dtype}')\n    return dtype\n\n\ndef get_names_from_partition_spec(partition_specs):\n    \"\"\"\n    The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list.\n    If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:\n        If the item is None, continue (do nothing) and move on to next iteration of loop.\n        If the item is an instance of str (i.e., if it's just one string), add that string to names set and move\n        on to next iteration of loop.\n        Otherwise, (if not None or str), call get_names_from_partition_spec recurs\n\n    :param partition_specs: Define the partitioning of a table\n    :return: A list of the names of all partitions\n\n    \"\"\"\n    names = set()\n    if isinstance(partition_specs, dict):\n        partition_specs = partition_specs.values()\n    for item in partition_specs:\n        if item is None:\n            continue\n        elif isinstance(item, str):\n            names.add(item)\n        else:\n            names.update(get_names_from_partition_spec(item))\n\n    return list(names)\n\n\n@dataclasses.dataclass\nclass EasyMethod:\n    TRAIN: str = \"train\"\n    SERVE: str = \"serve\"\n    EVAL: str = \"serve\"\n    CONVERT: str = \"convert\"\n\n\ndef names_in_mesh(*names):\n    \"\"\"\n    The names_in_mesh function is a decorator that can be used to check whether\n    the names of the axes passed into a function are valid.  It will raise an\n    exception if any of the axis names are not in the physical mesh.  For example,\n    if you have a function that takes two axes as arguments, and you want to make sure they're both in your mesh:\n\n    :param *names: Collect all the names passed to the function into a tuple\n    :return: A boolean indicating whether all the given\n\n    \"\"\"\n    return set(names) <= set(pxla.thread_resources.env.physical_mesh.axis_names)\n\n\ndef with_sharding_constraint(x, partition_specs):\n    \"\"\"\n    The with_sharding_constraint function is used to ensure that the sharding of a tensor\n    is consistent with the sharding of its inputs.  This function should be called on any\n    tensor which has been created by an operation which does not automatically handle this,\n    such as tf.concat or tf.split.\n\n    :param x: Define the tensor that will be sharded\n    :param partition_specs: Specify the partitioning of the data\n    :return: The same tensor with the\n\n    \"\"\"\n    axis_names = get_names_from_partition_spec(partition_specs)\n    if names_in_mesh(*axis_names):\n        x = wsc(x, partition_specs)\n    return x\n\n\ndef get_gradient_checkpoint_policy(name):\n    \"\"\"\n    The get_gradient_checkpoint_policy function is a helper function that returns the gradient checkpoint policy\n        specified by the name parameter.\n\n    :param name: Select the checkpoint policy from the dictionary\n    :return: A function that is used in the jax\n\n    \"\"\"\n    gradients = dict(\n        everything_saveable=jax.checkpoint_policies.everything_saveable,\n        nothing_saveable=jax.checkpoint_policies.nothing_saveable,\n        dots_saveable=jax.checkpoint_policies.dots_saveable,\n        checkpoint_dots=jax.checkpoint_policies.checkpoint_dots,\n        dots_with_no_batch_dims_saveable=jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n        checkpoint_dots_with_no_batch_dims=jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims,\n        save_anything_except_these_names=jax.checkpoint_policies.save_anything_except_these_names,\n        save_any_names_but_these=jax.checkpoint_policies.save_any_names_but_these,\n        save_only_these_names=jax.checkpoint_policies.save_only_these_names,\n        save_from_both_policies=jax.checkpoint_policies.save_from_both_policies\n    )\n    return gradients[name]\n\n\ndef repeat_kv_bnsh(x: chex.Array, n_rep: int) -> chex.Array:\n    \"\"\"\n    The repeat_kv_bnsh function is used to repeat the key and value vectors for each head in a multi-head attention\n    module. This function takes as input an array of shape (batch_size, n_heads, sequence_length, head_dim) and returns\n    an array of shape (batch_size, n_heads * nrep, sequence length, head dim). The reason this is necessary is because the\n    attention module expects keys/values/queries to be repeated across heads but not across batches. However we want our\n    keys/values/queries to be repeated both across heads AND batches so that we can use them\n\n    :param x: chex.Array: Pass in the input to the function\n    :param n_rep: int: Repeat the key and value heads\n    :return: A new array with the same shape as x, except for the second dimension which is n_kv_heads * n_rep\n\n    \"\"\"\n    bs, n_kv_heads, s, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    x = x[:, :, jax.numpy.newaxis, :, :]\n    x = jax.numpy.repeat(x, n_rep, axis=2)\n\n    return x.reshape(bs, n_kv_heads * n_rep, s, head_dim)\n\n\ndef repeat_kv_bsnh(x: chex.Array, n_rep: int) -> chex.Array:\n    \"\"\"\n    The repeat_kv_bsnh function is used to repeat the key and value vectors for each head.\n\n    :param x: chex.Array: Specify the input array\n    :param n_rep: int: Repeat the key-value attention heads n_rep times\n    :return: A new array with the same batch size, sequence length, and head dimension as the input array\n\n    \"\"\"\n    bs, s, n_kv_heads, head_dim = x.shape\n    x = x.transpose(0, 2, 1, 3)\n    if n_rep == 1:\n        return x\n    x = x[:, :, jax.numpy.newaxis, :, :]\n    x = jax.numpy.repeat(x, n_rep, axis=2)\n\n    x = x.transpose(0, 2, 1, 3)\n\n    return x.reshape(bs, s, n_kv_heads * n_rep, head_dim)\n\n\ndef precompute_freq_cis(\n        dim, max_position_embeddings=2048, base=10000, scaling_factor=1.0, rope_type: str | None = None\n):\n    if rope_type == \"none\":\n        rope_type = None\n    assert rope_type in [\n        \"linear\",\n        \"dynamic\",\n        None\n    ], \"wrong rope type has been given\"\n    t = jax.numpy.arange(max_position_embeddings)\n\n    if rope_type == \"linear\":\n        t = t / scaling_factor\n\n    if rope_type == \"dynamic\":\n        base = base * (\n                scaling_factor - (scaling_factor - 1)\n        ) ** (dim / (dim - 2))\n\n    inv_freq = 1.0 / (\n            base ** (jax.numpy.arange(0, dim, 2, dtype=jax.numpy.float32) / dim)\n    )\n    freq = jax.numpy.einsum(\n        \"i , j -> i j\", t, inv_freq\n    ).astype(\"float32\")\n\n    embed = jax.numpy.concatenate((freq, freq), axis=-1)\n    return jax.numpy.sin(embed)[:, :], jax.numpy.cos(embed)[:, :]\n\n\ndef rotate_half(x):\n    \"\"\"\n    The rotate_half function takes a complex-valued array and rotates the\n    phase of its second half by 180 degrees. This is equivalent to multiplying\n    the second half by -i, or equivalently rotating it 90 degrees counterclockwise.\n\n\n    :param x: Specify the input array\n    :return: A new array that is the same as the input\n\n    \"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return jax.numpy.concatenate((-x2, x1), axis=-1)\n\n\ndef apply_rotary_pos_emb(tensor, sin_, cos_):\n    \"\"\"\n    The apply_rotary_pos_emb function applies a rotary positional embedding to the input tensor.\n    b,h,s,d or pytorch style\n\n    :param tensor: Store the tensor that is passed into the function\n    :param sin_: Rotate the tensor by pi/2\n    :param cos_: Apply the cosine function to the tensor\n    :return: A tensor with the same shape as the input tensor\n\n    \"\"\"\n    b, h, s, d = tensor.shape\n    return (tensor * cos_[:, :, :s, :]) + (rotate_half(tensor) * sin_[:, :, :s, :])\n\n\ndef get_ranks_and_size(mesh):\n    \"\"\"\n    The get_ranks_and_size function is used to determine the number of MPI processes\n    (``mp_node_size``) and the number of devices per process (``dp_node_size``).\n    The ``mesh.shape[mp]`` determines how many MPI processes are needed,\n    and then we divide that by the local device count to get ``mp_node_size = max( 1, mp / jax.local )`.\n    This means that if there are more than enough devices for all MPI ranks on a node, each rank will only use one device; otherwise it will use\n\n    :param mesh: Get the shape of the mesh\n    :return: A dictionary with the following keys:\n\n    \"\"\"\n    out = dict(mesh=mesh)\n    total_process_size = mesh.shape[\"tp\"] * mesh.shape[\"sp\"]\n    mp_node_size = max(1, total_process_size // jax.local_device_count())\n    dp_node_size = jax.process_count() // mp_node_size\n    out.update(mp_node_size=mp_node_size,\n               dp_node_size=dp_node_size)\n\n    dp_node_rank = jax.process_index() // mp_node_size\n    mp_node_rank = jax.process_index() % mp_node_size\n    out.update(dp_node_rank=dp_node_rank,\n               mp_node_rank=mp_node_rank)\n    return out\n\n\ndef get_flash_attention():\n    \"\"\"\n    return: FlashAttention FN, Upcast Needed to float32,do_shard_map\n    \"\"\"\n    platform = jax.lib.xla_bridge.get_backend().platform\n    if platform == \"gpu\":\n        float32_logits = False\n        ring_attention_fn = fjformer.attention.ring_flash_attention_gpu\n        do_shard_map = True\n    elif platform == \"tpu\":\n        float32_logits = True\n        ring_attention_fn = fjformer.attention.tpu_flash_attention\n        do_shard_map = False\n    else:\n        raise ValueError(f\"Unsupported platform {platform}\")\n\n    return ring_attention_fn, float32_logits, do_shard_map\n\n\ndef smart_flash_attention(\n        q: chex.Array,\n        k: chex.Array,\n        v: chex.Array,\n        bias: chex.Array,\n        q_ps: jax.sharding.PartitionSpec,\n        k_ps: jax.sharding.PartitionSpec,\n        v_ps: jax.sharding.PartitionSpec,\n        b_ps: jax.sharding.PartitionSpec,\n        a_ps: jax.sharding.PartitionSpec,\n        block_k: int,\n        block_q: int,\n        block_b: int,\n        q_seq_len: int,\n        kv_seq_len: int,\n        num_attention_heads: int,\n        head_dims: int,\n        causal: bool,\n        attn_pdrop: float,\n        mesh: jax.sharding.Mesh = None,\n        dtype: jax.numpy.dtype = jax.numpy.float32,\n        precision: jax.lax.Precision = jax.lax.Precision('fastest'),\n        dropout_rng: jax.random.PRNGKey = None,\n        force_float32_tpu: bool = True,\n        deterministic: bool = False\n) -> chex.Array:\n    \"\"\"\n    Smart Flash Attention mechanism for efficient attention computation.\n\n    :param q: Query tensor with shape [batch_size, num_attention_heads, q_seq_len, head_dims].\n    :type q: tensor\n\n    :param k: Key tensor with shape [batch_size, num_attention_heads, kv_seq_len, head_dims].\n    :type k: tensor\n\n    :param v: Value tensor with shape [batch_size, num_attention_heads, kv_seq_len, head_dims].\n    :type v: tensor\n\n    :param bias: Bias tensor with shape [batch_size, num_attention_heads, q_seq_len, kv_seq_len].\n    :type bias: tensor\n\n    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n\n    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n\n    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n\n    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n\n    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n\n    :param block_k: Block size for key tensor reshaping.\n    :type block_k: int\n\n    :param block_q: Block size for query tensor reshaping.\n    :type block_q: int\n\n    :param block_b: Block size for bias tensor reshaping.\n    :type block_b: int\n\n    :param q_seq_len: Length of the query sequence.\n    :type q_seq_len: int\n\n    :param kv_seq_len: Length of the key-value sequence.\n    :type kv_seq_len: int\n\n    :param num_attention_heads: Number of attention heads.\n    :type num_attention_heads: int\n\n    :param head_dims: Dimensionality of each attention head.\n    :type head_dims: int\n\n    :param causal: If True, applies causal masking to the attention scores.\n    :type causal: bool\n\n    :param attn_pdrop: Dropout probability for attention weights.\n    :type attn_pdrop: float\n\n    :param mesh: Mesh specifying the data distribution for parallel computation.\n    :type mesh: mesh_type\n\n    :param dtype: Data type of the tensors.\n    :type dtype: data_type\n\n    :param precision: Precision mode for computation (default is 'fastest').\n    :type precision: str\n\n    :param dropout_rng: Random number generator key for dropout.\n    :type dropout_rng: rng_key\n\n    :param force_float32_tpu: If True, forces computation to use float32 on TPU.\n    :type force_float32_tpu: bool\n\n    :param deterministic: If True, ensures deterministic computation.\n    :type deterministic: bool\n\n    :return: chex.Array: Output tensor with the same shape as the input value tensor v.\n    :rtype: tensor\n\n    :raises ValueError: If the shapes of input tensors are not compatible for attention computation.\n    \"\"\"\n    assertion_mkv_err = \"\"\"\n    Q,K,V and bias shapes must be like\n    Q Shape : [batch_size, num_attention_heads, q_seq_len, head_dims]\n    K Shape : [batch_size, num_attention_heads, kv_seq_len, head_dims]\n    V Shape : [batch_size, num_attention_heads, kv_seq_len, head_dims]\n    bias Shape : [batch_size, num_attention_heads, q_seq_len, kv_seq_len]\n    \"\"\"\n    batch_size = q.shape[0]\n    assert batch_size == k.shape[0] == v.shape[0], 'Batch Size for q,k,v wont match'\n\n    assert q.shape == (batch_size, num_attention_heads,\n                       q_seq_len, head_dims), assertion_mkv_err\n    assert k.shape == (batch_size, num_attention_heads,\n                       kv_seq_len, head_dims), assertion_mkv_err\n    assert v.shape == (batch_size, num_attention_heads,\n                       kv_seq_len, head_dims), assertion_mkv_err\n    assert bias.shape == (batch_size, num_attention_heads,\n                          q_seq_len, kv_seq_len), assertion_mkv_err\n\n    flash_attn_fn, f32_upcast, do_shard_map = get_flash_attention()\n\n    if do_shard_map:\n        q, k, v = map(lambda x: jax.numpy.transpose(\n            x, (0, 2, 1, 3)), [q, k, v])\n        assert mesh is not None, 'For Using Shard Map on GPUs you have to pass Mesh'\n        ring_attention_sharded = shard_map(\n            partial(\n                flash_attn_fn,\n                axis_name=\"sp\",\n                float32_logits=f32_upcast,\n                blockwise_kwargs=dict(\n                    deterministic=deterministic,\n                    dropout_rng=dropout_rng,\n                    attn_pdrop=attn_pdrop,\n                    causal=causal,\n                    query_chunk_size=block_q,\n                    key_chunk_size=block_k,\n                    dtype=dtype,\n                    policy=jax.checkpoint_policies.nothing_saveable,\n                    precision=precision,\n                    prevent_cse=False,\n                )\n            ),\n            mesh=mesh,\n            in_specs=(\n                q_ps,\n                k_ps,\n                v_ps,\n                b_ps\n            ),\n            out_specs=a_ps,\n            check_rep=False\n        )\n        attn_output = ring_attention_sharded(q, k, v, bias)\n        attn_output = with_sharding_constraint(attn_output, a_ps)\n    else:\n        if force_float32_tpu or f32_upcast:\n            q, k, v = map(lambda x: x.astype(jax.numpy.float32), [q, k, v])\n        attn_output = fjformer.attention.jax_flash_attn_tpu.flash_attention(\n            q,\n            k,\n            v,\n            bias,\n            None,\n            causal=False,\n            sm_scale=1.0,\n            block_sizes=fjformer.attention.jax_flash_attn_tpu.BlockSizes(\n                block_b=block_b,\n                block_k=block_k,\n                block_q=block_q,\n                block_k_major=block_k\n            ),\n            debug=False,\n        )\n\n    attn_output = attn_output.astype(dtype)\n    return attn_output\n\n\ndef create_mesh(\n        axis_dims: Sequence[int] = (1, -1, 1, 1), axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"), backend=\"\"\n):\n    \"\"\"\n    The create_mesh function creates a mesh object that can be used to shard arrays.\n\n    :param axis_dims: Sequence[int]: Specify the dimensions of the mesh\n    :param axis_names: Sequence[str]: Name the axes of the mesh\n    :param backend: Specify the backend to use\n    :return: A mesh object\n\n    \"\"\"\n    array_devices = jax.numpy.ones(\n        (len(jax.devices() if backend == \"\" else jax.devices(backend)), 1))\n    resh = array_devices.reshape(axis_dims).shape\n\n    return jax.sharding.Mesh(\n        create_device_mesh(resh), axis_names\n    )\n\n\nclass JaxBaseClassModel(transformers.PretrainedConfig):\n    \"\"\"\n    It initializes all the attributes of an object, and it's called when you create a new instance of that class.\n    :param self: Refer to the instance of the class\n    :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis\n    :param axis_names: Sequence[str]: Set the names of the axes\n    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n    :param use_shard_map: bool: whenever to use shard_map for attention\n    :param backend: Optional[None]: Specify the backend to use\n    :param easy_method: EasyMethod: Specify the use of model to init the QDot Method for (e.q TRAIN,SERVE,...)\n    \"\"\"\n\n    def __init__(\n            self,\n            axis_dims: Sequence[int] = (1, -1, 1, 1),\n            axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), None, None, None),\n            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            use_shard_map: bool = False,\n            backend: Optional[None] = None,\n            easy_method: EasyMethod = EasyMethod.TRAIN,\n            **kwargs\n    ):\n        self.q_ps = q_ps\n        self.k_ps = k_ps\n        self.v_ps = v_ps\n        self.b_ps = b_ps\n        self.a_ps = a_ps\n        self.use_shard_map = use_shard_map\n        self.axis_dims = axis_dims\n        self.axis_names = axis_names\n        self.backend = backend if backend is not None else \"\"\n        self.easy_method = easy_method\n        super().__init__(**kwargs)\n\n    def jax_mesh(self) -> jax.sharding.Mesh:\n        \"\"\"\n        The jax_mesh function is a helper function that creates a jax.sharding.Mesh object from the\n        axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively.\n        The backend attribute is also used if it exists.\n\n        :param self: Refer to the object itself\n        :return: A jaxMesh\n\n        \"\"\"\n        return create_mesh(\n            axis_dims=self.axis_dims,\n            axis_names=self.axis_names,\n            backend=(self.backend if self.backend is not None else \"\") if hasattr(\n                self, 'backend') else \"\"\n        )\n\n    def get_partition_rules(self, fully_fsdp: bool = True):\n        if not fully_fsdp:\n            raise NotImplementedError\n        else:\n            return (\n                ('.*', jax.sharding.PartitionSpec((\"fsdp\", \"sp\")))\n            )\n\n    def get_axis_dims(self) -> Sequence[int]:\n        \"\"\"\n        The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.\n\n        :param self: Represent the instance of the class\n        :return: The dimensions of the axes\n\n        \"\"\"\n        return self.axis_dims\n\n    def get_axis_names(self) -> Sequence[str]:\n        \"\"\"\n        The get_axis_names function returns a list of the names of the axes.\n\n        :param self: Represent the instance of the class\n        :return: A list of the names of all axes\n\n        \"\"\"\n        return self.axis_names\n\n    def get_backend(self) -> str:\n        \"\"\"\n        The get_backend function returns the backend that is currently being used.\n        If no backend has been set, it will return the default JAX backend.\n\n        :param self: Bind the method to an object\n        :return: The backend platform\n\n        \"\"\"\n        return self.backend if not self.backend == \"\" else jax.lib.xla_bridge.get_backend().platform\n\n    @staticmethod\n    def get_flash_attention():\n        \"\"\"\n        The get_flash_attention function is used to get the flash attention value from the database.\n            :returns: The flash attention value from the database.\n\n        :return: A function\n\n        \"\"\"\n        return get_flash_attention()\n\n    def add_partitions(\n            self,\n            axis_dims: Sequence[int] = (1, -1, 1, 1),\n            axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), None, None, None),\n            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\n                (\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            use_shard_map: bool = False,\n            backend: Optional[str] = None,\n    ):\n        \"\"\"\n            It initializes all the attributes of an object, and it's called when you create a new instance of that class.\n            :param self: Refer to the instance of the class\n            :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis\n            :param axis_names: Sequence[str]: Set the names of the axes\n            :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n            :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n            :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n            :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n            :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n            :param use_shard_map: bool: whenever to use shard_map for attention\n            :param backend: Optional[None]: Specify the backend to use\n            \"\"\"\n        self.axis_dims = axis_dims\n        self.axis_names = axis_names\n        self.q_ps = q_ps\n        self.k_ps = k_ps\n        self.v_ps = v_ps\n        self.b_ps = b_ps\n        self.a_ps = a_ps\n        self.backend = backend\n        self.use_shard_map = use_shard_map\n\n\ndef add_start_docstrings(*docstr):\n    \"\"\"\n    The add_start_docstrings function is a decorator that adds the docstrings to the beginning of a function.\n    The add_start_docstrings function takes in an arbitrary number of strings and returns a decorator.\n    The returned decorator takes in one argument, fn, which is assumed to be a function. The docstring for fn is set equal to\n    the concatenation of all the strings passed into add_start_docstrings plus (if it exists) the original docstring for fn.\n\n    :param *docstr: Pass in a variable number of arguments to the function\n    :return: A decorator that adds the docstrings to the function\n\n    \"\"\"\n\n    def docstring_decorator(fn):\n        fn.__doc__ = \"\".join(docstr) + \\\n                     (fn.__doc__ if fn.__doc__ is not None else \"\")\n        return fn\n\n    return docstring_decorator\n\n\ndef get_dot_general_by_bits(\n        bits: Optional[int] = None,\n        mode: EasyMethod = EasyMethod.TRAIN\n) -> dict:\n    \"\"\"\n    The get_general_dot function is a helper function that returns a q_flax.QDotGeneral object\n    with the specified number of bits for forward and backward passes. If no bits are specified,\n    the function returns None.\n\n    :param bits: Optional[int]: Specify the number of bits for quantization\n    :param mode: EasyMethod: Specify the use of model to init the QDot Method for (e.q TRAIN,SERVE,...)\n    :return: A dict that contain dot_general_cls\n    \"\"\"\n    if mode == EasyMethod.TRAIN:\n        rhs_quant_mode = q_flax.QuantMode.TRAIN\n    elif mode == EasyMethod.EVAL or mode == EasyMethod.SERVE:\n        rhs_quant_mode = q_flax.QuantMode.SERVE\n    elif mode == EasyMethod.CONVERT:\n        rhs_quant_mode = q_flax.QuantMode.CONVERT\n    else:\n        raise ValueError(\"Unknown Quant Method for EasyMethod\")\n    if bits is not None:\n        return {\n            \"dot_general_cls\": functools.partial(\n                q_flax.QDotGeneral,\n                q_config.fully_quantized(\n                    fwd_bits=bits,\n                    bwd_bits=bits\n                ),\n                rhs_quant_mode=rhs_quant_mode\n            )\n        }\n    return {}  # empty just in case of not getting any error\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/flax_modelling_utils.py b/lib/python/EasyDel/modules/flax_modelling_utils.py
--- a/lib/python/EasyDel/modules/flax_modelling_utils.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/flax_modelling_utils.py	(date 1703665961901)
@@ -1,8 +1,6 @@
-import dataclasses
 import functools
 
 import fjformer.attention
-import transformers
 from fjformer.bits import config as q_config, q_flax
 from jax.interpreters import pxla
 from jax.experimental.pjit import with_sharding_constraint as wsc
@@ -13,6 +11,8 @@
 from typing import Sequence, Optional
 from jax.experimental.mesh_utils import create_device_mesh
 from jax.experimental.shard_map import shard_map
+from .easydel_modelling_utils import EasyMethod
+
 
 ACT2FN = {
     "gelu": partial(nn.gelu, approximate=False),
@@ -88,14 +88,6 @@
     return list(names)
 
 
-@dataclasses.dataclass
-class EasyMethod:
-    TRAIN: str = "train"
-    SERVE: str = "serve"
-    EVAL: str = "serve"
-    CONVERT: str = "convert"
-
-
 def names_in_mesh(*names):
     """
     The names_in_mesh function is a decorator that can be used to check whether
@@ -103,7 +95,7 @@
     exception if any of the axis names are not in the physical mesh.  For example,
     if you have a function that takes two axes as arguments, and you want to make sure they're both in your mesh:
 
-    :param *names: Collect all the names passed to the function into a tuple
+    :param names: Collect all the names passed to the function into a tuple
     :return: A boolean indicating whether all the given
 
     """
@@ -503,161 +495,6 @@
     )
 
 
-class JaxBaseClassModel(transformers.PretrainedConfig):
-    """
-    It initializes all the attributes of an object, and it's called when you create a new instance of that class.
-    :param self: Refer to the instance of the class
-    :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis
-    :param axis_names: Sequence[str]: Set the names of the axes
-    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor
-    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix
-    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor
-    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec
-    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights
-    :param use_shard_map: bool: whenever to use shard_map for attention
-    :param backend: Optional[None]: Specify the backend to use
-    :param easy_method: EasyMethod: Specify the use of model to init the QDot Method for (e.q TRAIN,SERVE,...)
-    """
-
-    def __init__(
-            self,
-            axis_dims: Sequence[int] = (1, -1, 1, 1),
-            axis_names: Sequence[str] = ("dp", "fsdp", "tp", "sp"),
-            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), "sp", "tp", None),
-            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), "sp", "tp", None),
-            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), "sp", "tp", None),
-            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), None, None, None),
-            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), "sp", "tp", None),
-            use_shard_map: bool = False,
-            backend: Optional[None] = None,
-            easy_method: EasyMethod = EasyMethod.TRAIN,
-            **kwargs
-    ):
-        self.q_ps = q_ps
-        self.k_ps = k_ps
-        self.v_ps = v_ps
-        self.b_ps = b_ps
-        self.a_ps = a_ps
-        self.use_shard_map = use_shard_map
-        self.axis_dims = axis_dims
-        self.axis_names = axis_names
-        self.backend = backend if backend is not None else ""
-        self.easy_method = easy_method
-        super().__init__(**kwargs)
-
-    def jax_mesh(self) -> jax.sharding.Mesh:
-        """
-        The jax_mesh function is a helper function that creates a jax.sharding.Mesh object from the
-        axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively.
-        The backend attribute is also used if it exists.
-
-        :param self: Refer to the object itself
-        :return: A jaxMesh
-
-        """
-        return create_mesh(
-            axis_dims=self.axis_dims,
-            axis_names=self.axis_names,
-            backend=(self.backend if self.backend is not None else "") if hasattr(
-                self, 'backend') else ""
-        )
-
-    def get_partition_rules(self, fully_fsdp: bool = True):
-        if not fully_fsdp:
-            raise NotImplementedError
-        else:
-            return (
-                ('.*', jax.sharding.PartitionSpec(("fsdp", "sp")))
-            )
-
-    def get_axis_dims(self) -> Sequence[int]:
-        """
-        The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.
-
-        :param self: Represent the instance of the class
-        :return: The dimensions of the axes
-
-        """
-        return self.axis_dims
-
-    def get_axis_names(self) -> Sequence[str]:
-        """
-        The get_axis_names function returns a list of the names of the axes.
-
-        :param self: Represent the instance of the class
-        :return: A list of the names of all axes
-
-        """
-        return self.axis_names
-
-    def get_backend(self) -> str:
-        """
-        The get_backend function returns the backend that is currently being used.
-        If no backend has been set, it will return the default JAX backend.
-
-        :param self: Bind the method to an object
-        :return: The backend platform
-
-        """
-        return self.backend if not self.backend == "" else jax.lib.xla_bridge.get_backend().platform
-
-    @staticmethod
-    def get_flash_attention():
-        """
-        The get_flash_attention function is used to get the flash attention value from the database.
-            :returns: The flash attention value from the database.
-
-        :return: A function
-
-        """
-        return get_flash_attention()
-
-    def add_partitions(
-            self,
-            axis_dims: Sequence[int] = (1, -1, 1, 1),
-            axis_names: Sequence[str] = ("dp", "fsdp", "tp", "sp"),
-            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), "sp", "tp", None),
-            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), "sp", "tp", None),
-            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), "sp", "tp", None),
-            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), None, None, None),
-            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
-                ("dp", "fsdp"), "sp", "tp", None),
-            use_shard_map: bool = False,
-            backend: Optional[str] = None,
-    ):
-        """
-            It initializes all the attributes of an object, and it's called when you create a new instance of that class.
-            :param self: Refer to the instance of the class
-            :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis
-            :param axis_names: Sequence[str]: Set the names of the axes
-            :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor
-            :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix
-            :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor
-            :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec
-            :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights
-            :param use_shard_map: bool: whenever to use shard_map for attention
-            :param backend: Optional[None]: Specify the backend to use
-            """
-        self.axis_dims = axis_dims
-        self.axis_names = axis_names
-        self.q_ps = q_ps
-        self.k_ps = k_ps
-        self.v_ps = v_ps
-        self.b_ps = b_ps
-        self.a_ps = a_ps
-        self.backend = backend
-        self.use_shard_map = use_shard_map
-
-
 def add_start_docstrings(*docstr):
     """
     The add_start_docstrings function is a decorator that adds the docstrings to the beginning of a function.
@@ -665,7 +502,7 @@
     The returned decorator takes in one argument, fn, which is assumed to be a function. The docstring for fn is set equal to
     the concatenation of all the strings passed into add_start_docstrings plus (if it exists) the original docstring for fn.
 
-    :param *docstr: Pass in a variable number of arguments to the function
+    :param docstr: Pass in a variable number of arguments to the function
     :return: A decorator that adds the docstrings to the function
 
     """
Index: lib/python/EasyDel/reinforcement_learning/models/modelling_value_head.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import typing\n\nimport flax.linen\nimport jax.lax\nfrom flax import linen as nn\nfrom transformers import FlaxPreTrainedModel\nfrom EasyDel.modules import FlaxLlamaForCausalLM\nfrom typing import Type\n\nfrom transformers.modeling_flax_outputs import FlaxCausalLMOutput\nfrom ...modules.auto_models import AutoEasyDelModelForCausalLM\nfrom .modelling_base import FlaxPreTrainedModelWrapper\nfrom jax import numpy as jnp\nimport chex\n\n\nclass ValueHead(nn.Module):\n    config: typing.Any\n    summary_dropout_prob: float = 0.0\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: typing.Optional[jax.lax.Precision] = jax.lax.Precision('fastest')\n\n    def setup(self):\n        \"\"\"\n        The setup function is called by the model's constructor.\n        It initializes all the layers in your model, and assigns them to member variables.\n        The setup function should be used for any initialization that needs to happen before running forward().\n        This includes things like loading weights from a file, or setting up an optimizer.\n\n        :param self: Represent the instance of the class\n        :return: A tuple of the following:\n        \n        \"\"\"\n        config = self.config\n\n        self.dropout = nn.Dropout(self.summary_dropout_prob)\n\n        if hasattr(config, \"hidden_size\"):\n            hidden_size = config.hidden_size\n        if hasattr(config, \"word_embed_proj_dim\"):\n            hidden_size = config.word_embed_proj_dim\n        elif hasattr(config, \"is_encoder_decoder\"):\n            if config.is_encoder_decoder and hasattr(config, \"decoder\"):\n                if hasattr(config.decoder, \"hidden_size\"):\n                    hidden_size = config.decoder.hidden_size\n\n        self.summary = nn.Dense(\n            1,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(self, hidden_states: chex.Array, deterministic: bool = True):\n        \"\"\"\n        The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, e.g., x(arg).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        :param self: Represent the instance of the class\n        :param hidden_states: chex.Array: Pass the hidden states of the previous layer\n        :param deterministic: bool: Determine whether to use dropout\n        :return: A tensor of shape (batch_size, num_classes)\n        \n        \"\"\"\n        return self.summary(self.dropout(hidden_states, deterministic=deterministic))\n\n\nclass FlaxAutoModelForCausalLMWithValueHead(FlaxPreTrainedModelWrapper, flax.linen.Module):\n\n    def setup(self):\n        self.v_head = ValueHead(self.pretrained_model.config)\n\n    def __call__(\n            self,\n            pretrained_model_params: dict | flax.core.FrozenDict,\n            input_ids: chex.Array = None,\n            past_key_values: chex.Array = None,\n            attention_mask: chex.Array = None,\n            **kwargs,\n    ):\n        \"\"\"\n        The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and past_key_values as arguments.\n        The output is a tuple containing lm logits and value.\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input to the model\n        :param past_key_values: Pass the past key values to the model\n        :param attention_mask: Mask out the padding tokens\n        :param kwargs: Pass in the past_key_values parameter\n        :param : Pass the past key values to the model\n        :return: The logits and the value\n        \n        \"\"\"\n        kwargs[\"output_hidden_states\"] = True\n        kwargs[\"past_key_values\"] = past_key_values\n\n        base_model_output = self.pretrained_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            params=pretrained_model_params,\n            **kwargs,\n        )\n\n        last_hidden_state = base_model_output.hidden_states[-1]\n        lm_logits = base_model_output.logits\n\n        value = self.v_head(last_hidden_state).squeeze(-1)\n\n        return lm_logits, value\n\n    def generate(self, *args, **kwargs):\n        return self.pretrained_model.generate(*args, **kwargs)\n\n    def push_to_hub(self, *args, **kwargs):\n        \"\"\"\n        The push_to_hub function is used to push the model to a remote location.\n\n        :param self: Represent the instance of the class\n        :param args: Send a non-keyworded variable length argument list to the function\n        :param kwargs: Pass keyworded, variable-length argument list to a function\n        :return: The pretrained model\n        \n        \"\"\"\n        setattr(self.pretrained_model, \"v_head\", self.v_head)\n\n        return self.pretrained_model.push_to_hub(*args, **kwargs)\n\n    def post_init(\n            self,\n            params: dict | flax.core.FrozenDict,\n            input_shape: typing.Tuple[int, int],\n            head_name: str = \"v_head\"\n    ):\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        params = self.init({\n            \"params\": jax.random.key(42)\n        },\n            {\"params\": params},\n            input_ids,\n            None,\n            attention_mask,\n        )[\"params\"] | params\n        return params\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/reinforcement_learning/models/modelling_value_head.py b/lib/python/EasyDel/reinforcement_learning/models/modelling_value_head.py
--- a/lib/python/EasyDel/reinforcement_learning/models/modelling_value_head.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/reinforcement_learning/models/modelling_value_head.py	(date 1703671032673)
@@ -8,7 +8,7 @@
 from typing import Type
 
 from transformers.modeling_flax_outputs import FlaxCausalLMOutput
-from ...modules.auto_models import AutoEasyDelModelForCausalLM
+from ...modules.auto_easydel_model import AutoEasyDelModelForCausalLM
 from .modelling_base import FlaxPreTrainedModelWrapper
 from jax import numpy as jnp
 import chex
Index: lib/python/EasyDel/reinforcement_learning/models/modelling_base.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import json\nimport logging\nimport os\n\nimport chex\nimport flax.core\nimport jax\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub.utils import EntryNotFoundError, HFValidationError, LocalEntryNotFoundError\nfrom transformers import FlaxPreTrainedModel\nfrom flax import linen as nn\nfrom flax.serialization import from_bytes\nimport msgpack\nfrom typing import Sequence, Optional, Type, Tuple\n\nfrom ...modules.auto_models import AutoEasyDelModelForCausalLM\n\nLAYER_PATTERNS = [\n    \"transformer.h.{layer}\",\n    \"model.decoder.layers.{layer}\",\n    \"gpt_neox.layers.{layer}\",\n    \"model.layers.{layer}\",\n]\n\n\ndef match_keywords(string, ts, ns):\n    for t in ts:\n        if t not in string:\n            return False\n    for n in ns:\n        if n in string:\n            return False\n    return True\n\n\nclass FlaxPreTrainedModelWrapper(nn.Module):\n    pretrained_model: Type[FlaxPreTrainedModel]\n    supported_modules = (\"v_head\",)\n    supported_rm_modules = (\"score\",)\n    supported_pretrained_model_architectures = FlaxPreTrainedModel\n\n    @classmethod\n    def from_pretrained(\n            cls,\n            pretrained_model_name_or_path: str,\n            device=jax.devices('cpu')[0],\n            dtype: jax.numpy.dtype = jax.numpy.float32,\n            param_dtype: jax.numpy.dtype = jax.numpy.float32,\n            precision: jax.lax.Precision = jax.lax.Precision('fastest'),\n            sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),\n            sharding_axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, None, None),\n            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n            use_shard_map: bool = False,\n            input_shape: Tuple[int, int] = (1, 1),\n            backend: Optional[str] = None,\n            **kwargs\n    ):\n\n        model, params = AutoEasyDelModelForCausalLM.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            device=device,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            sharding_axis_dims=sharding_axis_dims,\n            sharding_axis_names=sharding_axis_names,\n            q_ps=q_ps,\n            k_ps=k_ps,\n            v_ps=v_ps,\n            b_ps=b_ps,\n            a_ps=a_ps,\n            use_shard_map=use_shard_map,\n            input_shape=input_shape,\n            backend=backend,\n            **kwargs\n        )\n        model = cls(pretrained_model=model)\n        is_resuming_training = \"v_head\" in params.keys()\n        if not is_resuming_training:\n            params = model.post_init(\n                params=params,\n                input_shape=input_shape,\n                head_name=\"v_head\"\n            )\n        return model, params\n\n    @classmethod\n    def _get_checkpoint_from_hub(\n            cls,\n            pretrained_model,\n            pretrained_model_name_or_path,\n            index_filename,\n            token=None,\n            model_name=\"pytorch_model.bin\",\n            model_index_name=\"pytorch_model.bin.index.json\",\n    ):\n        \"\"\"\n        The _get_checkpoint_from_hub function is used to download a pretrained model from the Hugging Face Hub.\n        It will first attempt to download the entire model, and if that fails it will try downloading just the v_head weights.\n        If neither of those attempts succeed, it will return None for all outputs.\n\n        :param cls: Specify the class of the model\n        :param pretrained_model: Load the pretrained model\n        :param pretrained_model_name_or_path: Load the pretrained model from a checkpoint\n        :param index_filename: Load the index file for sharded models\n        :param token: Authenticate with the hugging face model hub\n        :param model_name: Specify the name of the model file to be downloaded\n        :param model_index_name: Specify the name of the index file\n        :param : Load the pretrained model\n        :return: A tuple of four elements:\n        \n        \"\"\"\n        files_to_download = None\n        filename = None\n        is_resuming_training = True\n        is_sharded = False\n\n        try:\n            filename = hf_hub_download(\n                pretrained_model_name_or_path,\n                model_name,\n                token=token,\n            )\n        # sharded\n        except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):\n            index_file_name = ''\n            if os.path.exists(index_filename):\n                index_file_name = index_filename\n            else:\n                try:\n                    index_file_name = hf_hub_download(\n                        pretrained_model_name_or_path,\n                        model_index_name,\n                        token=token,\n                    )\n                except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):\n                    # not continue training, do not have v_head weight\n                    is_resuming_training = False\n                    logging.warning(\n                        f\"A {type(pretrained_model)} model is loaded from '{pretrained_model_name_or_path}', \"\n                        f\"and no v_head weight is found. This IS expected if you are not resuming PPO training.\"\n                    )\n            # load json\n            if is_resuming_training:\n                with open(index_file_name, \"r\") as f:\n                    index = json.load(f)\n                files_to_download = set()\n                for k, v in index[\"weight_map\"].items():\n                    if any([module in k for module in cls.supported_modules]):\n                        files_to_download.add(v)\n                is_sharded = True\n\n        return filename, files_to_download, is_sharded, is_resuming_training\n\n    @classmethod\n    def _split_kwargs(cls, kwargs):\n        \"\"\"\n        The _split_kwargs function is used to split the kwargs into three categories:\n            1. supported_kwargs - These are the arguments that are supported by this class and will be passed on to the\n            parent class.\n            2. unsupported_kwargs - These are arguments that aren't supported by this class, but may be useful for other\n             classes in a chain of inheritance (e.g., if you're using multiple mixins).\n            3. peft_kwargs - These are arguments specific to PEFT and will not be passed on to any other classes.\n\n        :param cls: Refer to the class itself\n        :param kwargs: Pass keyword arguments to the function\n        :return: A tuple of three dictionaries\n        \n        \"\"\"\n        supported_kwargs = {}\n        unsupported_kwargs = {}\n        peft_kwargs = {}\n\n        for key, value in kwargs.items():\n            if key in cls.supported_args:\n                supported_kwargs[key] = value\n            else:\n                unsupported_kwargs[key] = value\n\n        return supported_kwargs, unsupported_kwargs, peft_kwargs\n\n    def push_to_hub(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def post_init(\n            self,\n            params: dict | flax.core.FrozenDict,\n            input_shape: Tuple[int, int],\n            head_name: str = \"v_head\"\n    ):\n        r\"\"\"\n        Post initialization method. This method is called after the model is\n        instantiated and loaded from a checkpoint. It can be used to perform\n        additional operations such as loading the state_dict.\n        \"\"\"\n        raise NotImplementedError\n\n    def compute_reward_score(\n            self,\n            pretrained_params: dict | flax.core.FrozenDict,\n            input_ids: chex.Array = None,\n            attention_mask: chex.Array = None,\n            ppo_adapter_name=\"default\",\n            **kwargs\n    ):\n\n        \"\"\"\n        The compute_reward_score function is used to compute the reward score for a given input.\n        The function takes in an input_ids tensor and returns a tensor of scores. The shape of the returned\n        tensor will be (batch_size, sequence_length). The higher the score, the more likely that token should be kept.\n\n        :param self: Represent the instance of the class\n        :param pretrained_params: dict | flax.core.FrozenDict: parameters to be passed to the model\n        :param input_ids: Pass the input tokens to the model\n        :param attention_mask: Indicate which tokens are padding\n        :param ppo_adapter_name: Set the adapter back to its original state\n        :param kwargs: Pass a variable number of arguments to a function\n        :return: The scores for the given input_ids\n        \n        \"\"\"\n        if not self.supports_rm_adapter:\n            raise ValueError(\"This model does not support reward modeling adapter.\")\n\n        base_model_output = self.pretrained_model(\n            params=pretrained_params,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True,\n            return_dict=True,\n            **kwargs,\n        )\n\n        last_hidden_states = base_model_output.hidden_states[-1]\n        scores = self.score(last_hidden_states)\n\n        return scores\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/reinforcement_learning/models/modelling_base.py b/lib/python/EasyDel/reinforcement_learning/models/modelling_base.py
--- a/lib/python/EasyDel/reinforcement_learning/models/modelling_base.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/reinforcement_learning/models/modelling_base.py	(date 1703671032669)
@@ -13,14 +13,7 @@
 import msgpack
 from typing import Sequence, Optional, Type, Tuple
 
-from ...modules.auto_models import AutoEasyDelModelForCausalLM
-
-LAYER_PATTERNS = [
-    "transformer.h.{layer}",
-    "model.decoder.layers.{layer}",
-    "gpt_neox.layers.{layer}",
-    "model.layers.{layer}",
-]
+from ...modules.auto_easydel_model import AutoEasyDelModelForCausalLM
 
 
 def match_keywords(string, ts, ns):
@@ -88,104 +81,6 @@
             )
         return model, params
 
-    @classmethod
-    def _get_checkpoint_from_hub(
-            cls,
-            pretrained_model,
-            pretrained_model_name_or_path,
-            index_filename,
-            token=None,
-            model_name="pytorch_model.bin",
-            model_index_name="pytorch_model.bin.index.json",
-    ):
-        """
-        The _get_checkpoint_from_hub function is used to download a pretrained model from the Hugging Face Hub.
-        It will first attempt to download the entire model, and if that fails it will try downloading just the v_head weights.
-        If neither of those attempts succeed, it will return None for all outputs.
-
-        :param cls: Specify the class of the model
-        :param pretrained_model: Load the pretrained model
-        :param pretrained_model_name_or_path: Load the pretrained model from a checkpoint
-        :param index_filename: Load the index file for sharded models
-        :param token: Authenticate with the hugging face model hub
-        :param model_name: Specify the name of the model file to be downloaded
-        :param model_index_name: Specify the name of the index file
-        :param : Load the pretrained model
-        :return: A tuple of four elements:
-        
-        """
-        files_to_download = None
-        filename = None
-        is_resuming_training = True
-        is_sharded = False
-
-        try:
-            filename = hf_hub_download(
-                pretrained_model_name_or_path,
-                model_name,
-                token=token,
-            )
-        # sharded
-        except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):
-            index_file_name = ''
-            if os.path.exists(index_filename):
-                index_file_name = index_filename
-            else:
-                try:
-                    index_file_name = hf_hub_download(
-                        pretrained_model_name_or_path,
-                        model_index_name,
-                        token=token,
-                    )
-                except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):
-                    # not continue training, do not have v_head weight
-                    is_resuming_training = False
-                    logging.warning(
-                        f"A {type(pretrained_model)} model is loaded from '{pretrained_model_name_or_path}', "
-                        f"and no v_head weight is found. This IS expected if you are not resuming PPO training."
-                    )
-            # load json
-            if is_resuming_training:
-                with open(index_file_name, "r") as f:
-                    index = json.load(f)
-                files_to_download = set()
-                for k, v in index["weight_map"].items():
-                    if any([module in k for module in cls.supported_modules]):
-                        files_to_download.add(v)
-                is_sharded = True
-
-        return filename, files_to_download, is_sharded, is_resuming_training
-
-    @classmethod
-    def _split_kwargs(cls, kwargs):
-        """
-        The _split_kwargs function is used to split the kwargs into three categories:
-            1. supported_kwargs - These are the arguments that are supported by this class and will be passed on to the
-            parent class.
-            2. unsupported_kwargs - These are arguments that aren't supported by this class, but may be useful for other
-             classes in a chain of inheritance (e.g., if you're using multiple mixins).
-            3. peft_kwargs - These are arguments specific to PEFT and will not be passed on to any other classes.
-
-        :param cls: Refer to the class itself
-        :param kwargs: Pass keyword arguments to the function
-        :return: A tuple of three dictionaries
-        
-        """
-        supported_kwargs = {}
-        unsupported_kwargs = {}
-        peft_kwargs = {}
-
-        for key, value in kwargs.items():
-            if key in cls.supported_args:
-                supported_kwargs[key] = value
-            else:
-                unsupported_kwargs[key] = value
-
-        return supported_kwargs, unsupported_kwargs, peft_kwargs
-
-    def push_to_hub(self, *args, **kwargs):
-        raise NotImplementedError
-
     def post_init(
             self,
             params: dict | flax.core.FrozenDict,
Index: lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import functools\nimport typing\nfrom typing import Sequence, Dict\n\nimport fjformer.attention\nimport flax.core\nfrom flax.struct import dataclass\nfrom jax import numpy as jnp, Array, lax\nfrom jax.experimental.shard_map import shard_map\nfrom jax.sharding import PartitionSpec as PS\nimport jax\nfrom flax import linen as nn\nfrom flax.traverse_util import unflatten_dict, flatten_dict\nfrom flax.core import freeze, unfreeze, FrozenDict\nfrom typing import Union, Optional, Tuple\nfrom transformers import FlaxPreTrainedModel\nfrom flax.linen import partitioning as nn_partitioning, dot_product_attention_weights\n\nfrom ..flax_modelling_utils import (\n    ACT2FN,\n    with_sharding_constraint,\n    get_gradient_checkpoint_policy,\n    repeat_kv_bnsh,\n    apply_rotary_pos_emb,\n    precompute_freq_cis,\n    JaxBaseClassModel,\n    smart_flash_attention,\n    get_dot_general_by_bits\n)\nimport chex\nfrom .mixtral_configuration import MixtralConfig\n\nre_mat = nn_partitioning.remat\n\n\n@dataclass\nclass MoeModelOutput:\n    last_hidden_state: chex.Array = None\n    hidden_states: Optional[Tuple[chex.Array]] = None\n    attentions: Optional[Tuple[chex.Array]] = None\n    router_logits: Optional[Tuple[chex.Array]] = None\n\n\n@dataclass\nclass MoeCausalLMOutput:\n    aux_loss: Optional[chex.Array] = None\n    logits: chex.Array = None\n    hidden_states: Optional[Tuple[chex.Array]] = None\n    attentions: Optional[Tuple[chex.Array]] = None\n    router_logits: Optional[Tuple[chex.Array]] = None\n\n\ndef jax_load_balancing_loss_func(gate_logits: chex.Array, num_experts: chex.Array = None, top_k: int = 2) -> float:\n    if gate_logits is None:\n        return 0\n    if isinstance(gate_logits, tuple):\n        gate_logits = jnp.concatenate([gate for gate in gate_logits], axis=0)\n    routing_weights, selected_experts = jax.lax.top_k(gate_logits, top_k)\n    routing_weights = jax.nn.softmax(routing_weights, axis=-1)\n    if selected_experts.dtype != jnp.int64:\n        selected_experts = selected_experts.astype(jnp.int64)\n    if len(selected_experts.shape) == 2:\n        selected_experts = selected_experts[:, :, jnp.newaxis]\n    expert_mask = jnp.max(jax.nn.one_hot(\n        selected_experts, num_experts), axis=-2)\n    tokens_per_group_and_expert = jnp.mean(\n        expert_mask.astype(jnp.float32), axis=-2)\n    router_prob_per_group_and_expert = jnp.mean(routing_weights, axis=-1)\n    return jnp.mean(tokens_per_group_and_expert * jnp.expand_dims(router_prob_per_group_and_expert, axis=-1)) * (\n            num_experts ** 2)\n\n\nclass MixtralRMSNorm(nn.Module):\n    dim: int\n    eps: float = 1e-6\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n\n    def setup(self) -> None:\n        self.weight = self.param(\n            'kernel',\n            nn.initializers.ones,\n            (self.dim,),\n            self.param_dtype,\n        )\n\n    def _norm(self, x: jnp.ndarray) -> jnp.ndarray:\n        return x * jax.lax.rsqrt(jnp.square(x).mean(-1, keepdims=True) + self.eps)\n\n    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n        x = x.astype(jnp.promote_types(self.dtype, jnp.float32))\n        output = self._norm(x).astype(self.dtype)\n        weight = jnp.asarray(self.weight, self.dtype)\n        return output * weight\n\n\nclass FlaxMixtralRotaryEmbedding(nn.Module):\n    dtype: jnp.dtype = jnp.float32\n\n    def __call__(self, key, query, freq_cis, position_ids):\n        sin, cos = freq_cis\n\n        sin = sin[position_ids][:, None, :, :]\n        cos = cos[position_ids][:, None, :, :]\n\n        key = apply_rotary_pos_emb(key, sin, cos)\n        query = apply_rotary_pos_emb(query, sin, cos)\n\n        return query.astype(self.dtype), key.astype(self.dtype)\n\n\nclass FlaxMixtralAttention(nn.Module):\n    config: MixtralConfig\n    layer_index: int\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.q_proj = dense(self.num_heads * self.head_dim)\n        self.k_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.v_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.o_proj = dense(self.hidden_size)\n        self.rotary = FlaxMixtralRotaryEmbedding(self.dtype)\n\n    @nn.compact\n    def concatenate_to_cache_(self, query: chex.Array, key: chex.Array, value: chex.Array, attention_mask: chex.Array):\n        is_cache_available = self.has_variable('cache', 'key')\n        key_cache = self.variable(\n            'cache', 'key', jnp.zeros, key.shape, key.dtype)\n        value_cache = self.variable(\n            'cache', 'value', jnp.zeros, key.shape, value.dtype)\n        index_cache = self.variable(\n            'cache', 'index', lambda: jnp.array(0, dtype=jnp.int32))\n        if is_cache_available:\n            *bd, ml, nh, dph = key_cache.value.shape\n            indices = (0,) * len(bd) + (index_cache.value, 0, 0)\n            key = jax.lax.dynamic_update_slice(key_cache.value, key, indices)\n            value = jax.lax.dynamic_update_slice(\n                value_cache.value, value, indices)\n            key_cache.value = key\n            value_cache.value = value\n            num_updated_cache_vector = query.shape[1]\n            index_cache.value = index_cache.value + num_updated_cache_vector\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(ml) < index_cache.value,\n                tuple(bd) + (1, num_updated_cache_vector, ml)\n            )\n            attention_mask = nn.combine_masks(pad_mask, attention_mask)\n        return query, key, value, attention_mask\n\n    @staticmethod\n    def _t(query, key, value):\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def t_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        query = query.reshape(batch_size, sequence_length,\n                              self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length,\n                          self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length,\n                              self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._t(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._t(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n        The __call__ method takes an input tensor (x) and returns an output tensor (y).\n        In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n        :param self: Refer to the object itself\n        :param hidden_states: chex.Array: Pass in the hidden state of the model\n        :param freq_cis: chex.Array: Create the t_rotary variable\n        :param attention_mask: chex.Array: Mask the attention weights\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param position_ids: chex.Array: Specify the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attention weights\n        :return: A tuple of (out, attn_output)\n\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query, key, value = self.q_proj(hidden_states), self.k_proj(\n            hidden_states), self.v_proj(hidden_states)\n\n        if self.config.use_pjit_attention_force:\n            query = with_sharding_constraint(query, PS(\"fsdp\", \"sp\", None))\n            key = with_sharding_constraint(key, PS(\"fsdp\", \"sp\", None))\n            value = with_sharding_constraint(value, PS(\"fsdp\", \"sp\", None))\n        query, key, value = self.t_rotary(\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            query=query,\n            key=key,\n            value=value,\n            freq_cis=freq_cis,\n            position_ids=position_ids\n        )\n        if self.has_variable('cache', 'key') or init_cache:\n            query, key, value, attention_mask = self.concatenate_to_cache_(\n                query, key, value, attention_mask)\n\n        q_l, k_l = query.shape[1], key.shape[1]\n        if self.has_variable('cache', 'key'):\n            mask_shift: int = self.variables['cache']['index']\n            dl = self.variables['cache']['key'].shape[1]\n            causal_mask = jax.lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, q_l, dl)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :q_l, :k_l]\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(\n                attention_mask, axis=(-3, -2)), causal_mask.shape)\n\n        attention_mask = nn.combine_masks(attention_mask, causal_mask)\n\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(\n                    self.config.num_attention_heads, 1, )\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(\n                    self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(query, rtp_axis),\n                k=jnp.transpose(key, rtp_axis),\n                v=jnp.transpose(value, rtp_axis),\n                q_ps=self.config.q_ps,\n                k_ps=self.config.k_ps,\n                v_ps=self.config.v_ps,\n                b_ps=self.config.b_ps,\n                a_ps=self.config.a_ps,\n                bias=attention_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=q_l,\n                kv_seq_len=k_l,\n                attn_pdrop=self.config.attn_pdrop,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(\n                    self.dtype).min).astype(self.dtype),\n            )\n            if self.config.use_shard_map:\n                attn_weights = shard_map(\n                    functools.partial(\n                        dot_product_attention_weights,\n                        dtype=jnp.promote_types(self.dtype, jnp.float32),\n                        deterministic=deterministic,\n                        dropout_rate=self.config.attn_pdrop,\n                        precision=self.precision,\n                    ),\n                    mesh=self.config.jax_mesh(),\n                    in_specs=(\n                        self.config.q_ps,\n                        self.config.k_ps,\n                        self.config.b_ps\n                    ),\n                    out_specs=PS((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n                    check_rep=False\n                )(\n                    query, key, attention_bias\n                )\n            else:\n                attn_weights = dot_product_attention_weights(\n                    query=query,\n                    key=key,\n                    bias=attention_bias,\n                    dtype=jnp.promote_types(self.dtype, jnp.float32),\n                    deterministic=deterministic,\n                    dropout_rate=self.config.attn_pdrop,\n                    precision=self.precision,\n                )\n\n            if self.config.use_pjit_attention_force:\n                attn_weights = with_sharding_constraint(\n                    attn_weights, PS((\"dp\", \"fsdp\"), \"sp\", \"tp\", None))\n\n            attn_output = jnp.einsum(\n                \"...hqk,...khd->...qhd\", attn_weights, value)\n\n        out = self.o_proj(attn_output.reshape(\n            batch_size, sequence_length, self.hidden_size))\n        return out, attn_weights\n\n\nclass FlaxMixtralBLockSparseTop2MLP(nn.Module):\n    config: MixtralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.w1 = dense(self.config.intermediate_size)\n        self.w3 = dense(self.config.intermediate_size)\n        self.w2 = dense(self.config.hidden_size)\n        self.act_fn = ACT2FN[self.config.hidden_act]\n\n    def __call__(self, x: chex.Array):\n        return self.w2(self.act_fn(self.w1(x)) * self.w3(x))\n\n\nclass FlaxMixtralBlocKSparesTop2MLPCollection(nn.Module):\n    config: MixtralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        self.layers = [\n            FlaxMixtralBLockSparseTop2MLP(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            )\n            for i in range(self.config.num_local_experts)\n        ]\n\n    def __call__(self,\n                 expert_mask: chex.Array,\n                 hidden_states: chex.Array,\n                 routing_weights: chex.Array,\n                 batch_size: int,\n                 sequence_length: int,\n                 hidden_dim: int\n                 ) -> chex.Array:\n        assert hidden_states.ndim == 2\n        final_hidden_states = jnp.zeros(\n            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype\n        )\n\n        def custom_index_add_without_index_add(\n                final_hidden_states_,\n                top_x_,\n                idx_,\n                current_hidden_states_\n        ):\n            for i in range(top_x_.size):\n                final_hidden_states_.at[top_x[i]].set(\n                    final_hidden_states_[top_x[i]] + current_hidden_states_[i])\n            return final_hidden_states_\n\n        for expert_idx, expert_layer in enumerate(self.layers):\n            selected_mask = expert_mask[expert_idx]\n\n            idx, top_x = jnp.nonzero(\n                selected_mask, size=selected_mask.shape[-1])\n            top_x = jnp.where(top_x != 0, top_x, -1)\n            if top_x.shape[0] == 0:\n                continue\n\n            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n\n            current_hidden_states = expert_layer(\n                current_state) * routing_weights[top_x, idx, None]\n\n            final_hidden_states = custom_index_add_without_index_add(\n                final_hidden_states,\n                top_x,\n                idx,\n                current_hidden_states.astype(hidden_states.dtype)\n            )\n\n        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n\n\nclass FlaxMixtralSparseMoeBlock(nn.Module):\n    \"\"\"\n    This implementation is\n    strictly equivalent to standard MoE with full capacity (no\n    dropped tokens). It's faster since it formulates MoE operations\n    in terms of block-sparse operations to accomodate imbalanced\n    assignments of tokens to experts, whereas standard MoE either\n    (1) drop tokens at the cost of reduced performance or (2) set\n    capacity factor to number of experts and thus waste computation\n    and memory on padding.\n    \"\"\"\n    config: MixtralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[\n        Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        self.gate = nn.Dense(\n            self.config.num_local_experts,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n        )\n\n        self.experts = FlaxMixtralBlocKSparesTop2MLPCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(self, hidden_states: chex.Array) -> Tuple[chex.Array, chex.Array]:\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n        hidden_states = hidden_states.reshape(-1, hidden_dim)\n        router_logits = self.gate(hidden_states).astype(\n            jnp.promote_types(self.dtype, jnp.float32))\n        routing_weights = jax.nn.softmax(router_logits.astype(\n            jnp.promote_types(self.dtype, jnp.float32)), axis=1)\n        routing_weights, selected_experts = jax.lax.top_k(\n            routing_weights, k=self.config.num_experts_per_tok)\n        routing_weights /= jnp.sum(routing_weights, axis=-1, keepdims=True)\n        routing_weights = routing_weights.astype(hidden_states.dtype)\n        expert_mask = jax.nn.one_hot(\n            selected_experts, num_classes=self.config.num_local_experts).transpose(2, 1, 0)\n        return self.experts(\n            expert_mask=expert_mask,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            hidden_dim=hidden_dim,\n            hidden_states=hidden_states,\n            routing_weights=routing_weights\n        ), router_logits\n\n\nclass FlaxMixtralDecoderLayer(nn.Module):\n    config: MixtralConfig\n    layer_index: int\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        self.self_attn = FlaxMixtralAttention(\n            config=self.config,\n            layer_index=self.layer_index,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.block_sparse_moe = FlaxMixtralSparseMoeBlock(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.input_layernorm = MixtralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attention_layernorm = MixtralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True,\n            output_router_logits: Optional[bool] = False,\n    ):\n        \"\"\"\n        The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        :param self: Represent the instance of the class\n        :param hidden_states: chex.Array: Represent the input to the encoder layer\n        :param freq_cis: chex.Array: Pass the frequency information to the attention layer\n        :param attention_mask: chex.Array: Mask out the attention weights for certain positions\n        :param causal_mask: chex.Array: Mask the future tokens\n        :param position_ids: chex.Array: Indicate the position of each token in the sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache for the self-attention layer\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :return: A tuple of hidden_states and attention_output\n\n        \"\"\"\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states, self_attn_weights = self.self_attn(\n            hidden_states=hidden_states,\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            causal_mask=causal_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions\n        )\n\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (self_attn_weights,)\n        if output_router_logits:\n            outputs += (router_logits,)\n        return outputs\n\n\nclass FlaxMixtralDecoderLayerCollection(nn.Module):\n    config: MixtralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -> None:\n        self.blocks = [\n            FlaxMixtralDecoderLayer(\n                layer_index=layer_index,\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(layer_index)\n            )\n\n            for layer_index in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_hidden_states: Optional[bool] = False,\n            output_attentions: Optional[bool] = False,\n            output_router_logits: Optional[bool] = False,\n    ):\n        \"\"\"\n        The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        :param self: Represent the instance of the class\n        :param hidden_states: chex.Array: Represent the input to the encoder layer\n        :param freq_cis: chex.Array: Pass the frequency information to the attention layer\n        :param attention_mask: chex.Array: Mask out the attention weights for certain positions\n        :param causal_mask: chex.Array: Mask the future tokens\n        :param position_ids: chex.Array: Indicate the position of each token in the sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache for the self-attention layer\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :return: A tuple of hidden_states, attention_output, all_hidden_states and all_router_logits\n\n        \"\"\"\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_router_logits = () if output_router_logits else None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                output_attentions=output_attentions,\n                output_router_logits=output_router_logits,\n                init_cache=init_cache,\n                freq_cis=freq_cis,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n            )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n            if output_router_logits:\n                all_router_logits += (layer_outputs[-1],)\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (all_self_attns,)\n        if output_hidden_states:\n            outputs += (all_hidden_states,)\n        if output_router_logits:\n            outputs += (all_router_logits,)\n        return outputs\n\n\nclass MixtralPreTrainedModel(FlaxPreTrainedModel):\n    config_class: MixtralConfig = MixtralConfig\n    module_class: nn.Module = None\n    base_model_prefix = \"model\"\n\n    # main_input_name = \"input_ids\"\n\n    def __init__(\n            self,\n            config: MixtralConfig,\n            dtype: jnp.dtype = jnp.bfloat16,\n            param_dtype: jnp.dtype = jnp.bfloat16,\n            precision: jax.lax.Precision = jax.lax.Precision(\"fastest\"),\n            input_shape: Tuple[int, int] = (1, 1),\n            seed: int = 0,\n            _do_init: bool = False,\n            **kwargs\n    ):\n        module = self.module_class(\n            config=config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            **kwargs\n        )\n\n        super().__init__(\n            dtype=dtype, _do_init=_do_init,\n            module=module, config=config, input_shape=input_shape,\n            seed=seed,\n        )\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: FrozenDict = None\n    ) -> FrozenDict:\n        \"\"\"\n        The init_weights function is used to initialize the weights of a model.\n        It takes in a rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        :param self: Access variables that belong to the class\n        :param rng: jax.random.PRNGKey: Initialize the weights of the model\n        :param input_shape: Tuple: Initialize the input_ids, attention_mask and position_ids\n        :param params: flax.core.FrozenDict: Pass in the parameters of a pre-trained model\n        :return: A frozendict of parameters\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids, dtype=\"i4\")\n        position_ids = jnp.broadcast_to(\n            jnp.arange(jnp.atleast_2d(input_ids).shape[-1], dtype=\"i4\"),\n            input_shape,\n        )\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                return_dict=False\n            )\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input sequence to the model\n        :param attention_mask: Mask out the padding tokens\n        :param position_ids: Specify the position of each token in the sequence\n        :param params: dict: Pass in the parameters of the model\n        :param past_key_values: dict: Pass the past key values to the model\n        :param dropout_rng: jax.random.PRNGKey: Pass in a random number generator key to the model\n        :param train: bool: Determine whether to use dropout or not\n        :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n        :param output_hidden_states: Optional[bool]: Determine whether to return the hidden states of all layers\n        :param return_dict: Optional[bool]: Return a dictionary of the outputs\n        :param add_params_field: bool: Add a params field to the inputs dictionary\n        :return: A tuple of (last_hidden_state, past_key_values)\n\n        \"\"\"\n\n        # TODO: Here needs to be fixed\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),  # input_ids: chex.Array\n            # attention_mask: Optional[chex.Array] = None\n            jnp.array(attention_mask, dtype=\"i4\"),\n            # position_ids: Optional[chex.Array] = None\n            jnp.array(position_ids, dtype=\"i4\"),\n            None,  # inputs_embeds: Optional[chex.Array] = None\n            output_attentions,  # output_attentions: Optional[bool] = None\n            # output_hidden_states: Optional[bool] = None\n            output_hidden_states,\n            # output_router_logits: Optional[bool] = None\n            output_router_logits,\n            False,  # init_cache: bool = False\n            not train,  # deterministic: bool = True\n            return_dict,  # return_dict: bool = True\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxMixtralModule(nn.Module):\n    config: MixtralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -> None:\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n\n        self.layers = FlaxMixtralDecoderLayerCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n        self.norm = MixtralRMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n        initial_rope_kwargs = dict(\n            rope_type=\"none\"\n        )\n        if self.config.rope_scaling is not None:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            initial_rope_kwargs = dict(\n                scaling_factor=scaling_factor,\n                rope_type=scaling_type\n            )\n        self.freq_cis = precompute_freq_cis(\n            max_position_embeddings=self.config.max_position_embeddings,\n            dim=self.config.hidden_size // self.config.num_attention_heads,\n            base=self.config.rope_theta,\n            **initial_rope_kwargs\n        )\n        self.causal_mask = nn.make_causal_mask(\n            jnp.ones((1, self.config.c_max_position_embeddings), dtype='i4'))\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            inputs_embeds: Optional[chex.Array] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            init_cache: bool = False,\n            deterministic: bool = True,\n            return_dict: bool = True,\n    ) -> MoeModelOutput | Tuple:\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n\n        if inputs_embeds is None and input_ids is not None:\n            inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n        else:\n            raise ValueError(\n                \"you should specify inputs_embeds or input_ids one of them\")\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_router_logits = (\n            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n        )\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        collection_outputs = self.layers(\n            hidden_states=inputs_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=self.causal_mask,\n            freq_cis=self.freq_cis,\n            output_attentions=output_attentions,\n            output_router_logits=output_router_logits,\n            output_hidden_states=output_hidden_states,\n            init_cache=init_cache,\n            deterministic=deterministic,\n        )\n        all_self_attns = None\n        all_hidden_states = None\n        all_router_logits = None\n        hidden_states = collection_outputs[0]\n        if output_attentions:\n            all_self_attns = collection_outputs[1]\n        if output_hidden_states:\n            all_hidden_states = collection_outputs[2 if output_attentions else 1]\n        if output_router_logits:\n            all_router_logits = collection_outputs[-1]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, all_hidden_states, all_self_attns, all_router_logits]\n                if v is not None\n            )\n        return MoeModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            router_logits=all_router_logits,\n        )\n\n\nclass FlaxMixtralModel(MixtralPreTrainedModel):\n    module_class = FlaxMixtralModule\n\n\nclass FlaxMixtralForCausalLMModule(nn.Module):\n    config: MixtralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[jax.lax.Precision] = jax.lax.Precision(\"fastest\")\n\n    def setup(self) -> None:\n        self.model = FlaxMixtralModule(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            use_bias=False,\n            kernel_init=nn.initializers.normal(self.config.initializer_range),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            inputs_embeds: Optional[chex.Array] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            output_router_logits: Optional[bool] = None,\n            init_cache: bool = False,\n            deterministic: bool = True,\n            return_dict: bool = True,\n    ) -> MoeCausalLMOutput | Tuple:\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            output_router_logits=output_router_logits,\n            init_cache=init_cache,\n            deterministic=deterministic,\n            return_dict=True,\n        )\n        logits = self.lm_head(outputs.last_hidden_state)\n        aux_loss = None\n        if output_router_logits and outputs.router_logits is not None:\n            aux_loss = jax_load_balancing_loss_func(\n                outputs.router_logits, self.num_experts, self.num_experts_per_tok\n            )\n\n        if not return_dict:\n            outputs = (logits,) + tuple(\n                v\n                for v in [\n                    outputs.hidden_states,\n                    outputs.attentions,\n                    outputs.router_logits\n                ]\n                if v is not None\n            )\n            return outputs\n\n        return MoeCausalLMOutput(\n            aux_loss=aux_loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            router_logits=outputs.router_logits,\n        )\n\n\nclass FlaxMixtralForCausalLM(MixtralPreTrainedModel):\n    module_class = FlaxMixtralForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"\n        The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        :param self: Access variables that belong to the class\n        :param input_ids: Pass in the input tokens\n        :param max_length: Set the length of the sequence to be generated\n        :param attention_mask: Optional[chex.Array]: Mask the attention weights\n        :return: A dictionary of the past_key_values, attention_mask and position ids\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py b/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py
--- a/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py	(date 1703669116080)
@@ -13,22 +13,20 @@
 from flax.traverse_util import unflatten_dict, flatten_dict
 from flax.core import freeze, unfreeze, FrozenDict
 from typing import Union, Optional, Tuple
-from transformers import FlaxPreTrainedModel
 from flax.linen import partitioning as nn_partitioning, dot_product_attention_weights
 
 from ..flax_modelling_utils import (
     ACT2FN,
     with_sharding_constraint,
-    get_gradient_checkpoint_policy,
     repeat_kv_bnsh,
     apply_rotary_pos_emb,
     precompute_freq_cis,
-    JaxBaseClassModel,
     smart_flash_attention,
     get_dot_general_by_bits
 )
 import chex
 from .mixtral_configuration import MixtralConfig
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 
 re_mat = nn_partitioning.remat
 
@@ -682,7 +680,7 @@
         return outputs
 
 
-class MixtralPreTrainedModel(FlaxPreTrainedModel):
+class MixtralPreTrainedModel(EasyDelFlaxPretrainedModel):
     config_class: MixtralConfig = MixtralConfig
     module_class: nn.Module = None
     base_model_prefix = "model"
@@ -1010,6 +1008,12 @@
 class FlaxMixtralModel(MixtralPreTrainedModel):
     module_class = FlaxMixtralModule
 
+    def set_input_embeddings(self, value):
+        self.module.embed_tokens = value
+
+    def get_input_embeddings(self):
+        return self.module.embed_tokens
+
 
 class FlaxMixtralForCausalLMModule(nn.Module):
     config: MixtralConfig
@@ -1090,6 +1094,24 @@
 class FlaxMixtralForCausalLM(MixtralPreTrainedModel):
     module_class = FlaxMixtralForCausalLMModule
 
+    def set_input_embeddings(self, value):
+        self.module.model.embed_tokens = value
+
+    def get_input_embeddings(self):
+        return self.module.model.embed_tokens
+
+    def set_decoder(self, decoder):
+        self.module.model = decoder
+
+    def get_decoder(self):
+        return self.module.model
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
     def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):
         """
         The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.
Index: lib/python/EasyDel/modules/t5/t5_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass T5Config(JaxBaseClassModel):\n    model_type = \"t5\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n    attribute_map = {\"hidden_size\": \"d_model\", \"num_attention_heads\": \"num_heads\", \"num_hidden_layers\": \"num_layers\"}\n\n    def __init__(\n            self,\n            vocab_size=32128,\n            d_model=512,\n            d_kv=64,\n            d_ff=2048,\n            num_layers=6,\n            num_decoder_layers=None,\n            num_heads=8,\n            relative_attention_num_buckets=32,\n            relative_attention_max_distance=128,\n            dropout_rate=0.1,\n            layer_norm_epsilon=1e-6,\n            initializer_factor=1.0,\n            feed_forward_proj=\"relu\",\n            is_encoder_decoder=True,\n            use_cache=True,\n            pad_token_id=0,\n            eos_token_id=1,\n            gradient_checkpointing: str = 'nothing_saveable',\n            use_pjit_attention_force: bool = False,\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.d_model = d_model\n        self.d_kv = d_kv\n        self.d_ff = d_ff\n        self.num_layers = num_layers\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.num_decoder_layers = (\n            num_decoder_layers if num_decoder_layers is not None else self.num_layers\n        )  # default = symmetry\n        self.num_heads = num_heads\n        self.relative_attention_num_buckets = relative_attention_num_buckets\n        self.relative_attention_max_distance = relative_attention_max_distance\n        self.dropout_rate = dropout_rate\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_factor = initializer_factor\n        self.feed_forward_proj = feed_forward_proj\n        self.use_cache = use_cache\n\n        act_info = self.feed_forward_proj.split(\"-\")\n        self.dense_act_fn = act_info[-1]\n        self.is_gated_act = act_info[0] == \"gated\"\n\n        if len(act_info) > 1 and act_info[0] != \"gated\" or len(act_info) > 2:\n            raise ValueError(\n                f\"`feed_forward_proj`: {feed_forward_proj} is not a valid activation function of the dense layer.\"\n                \"Please make sure `feed_forward_proj` is of the format `gated-{ACT_FN}` or `{ACT_FN}`, e.g. \"\n                \"'gated-gelu' or 'relu'\"\n            )\n\n        # for backwards compatibility\n        if feed_forward_proj == \"gated-gelu\":\n            self.dense_act_fn = \"gelu_new\"\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            is_encoder_decoder=is_encoder_decoder,\n            **kwargs,\n        )\n\n    def get_partition_rules(self, fully_fsdp: bool = True):\n        return (\n            (\"wi_0/kernel\", PartitionSpec(\"fsdp\")),\n            (\"wi_1/kernel\", PartitionSpec(\"fsdp\")),\n            (\"wi/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"wo/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"SelfAttention/(q|k|v|o)/kernel\", PartitionSpec(\"fsdp\")),\n            (\"EncDecAttention/(q|k|v|o)/kernel\", PartitionSpec(\"fsdp\")),\n            ('.*', PartitionSpec(None))\n        ) if not fully_fsdp else (\n            ('.*', PartitionSpec(\"fsdp\"))\n        )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/t5/t5_configuration.py b/lib/python/EasyDel/modules/t5/t5_configuration.py
--- a/lib/python/EasyDel/modules/t5/t5_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/t5/t5_configuration.py	(date 1703666055324)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class T5Config(JaxBaseClassModel):
+class T5Config(EasyDelPretrainedConfig):
     model_type = "t5"
     keys_to_ignore_at_inference = ["past_key_values"]
     attribute_map = {"hidden_size": "d_model", "num_attention_heads": "num_heads", "num_hidden_layers": "num_layers"}
Index: lib/python/EasyDel/modules/phi/phi_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import math\nfrom typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass PhiConfig(JaxBaseClassModel):\n    \"\"\"Phi configuration.\"\"\"\n\n    model_type = \"phi\"\n    attribute_map = {\n        \"max_position_embeddings\": \"n_positions\",\n        \"hidden_size\": \"n_embd\",\n        \"num_attention_heads\": \"n_head\",\n        \"num_hidden_layers\": \"n_layer\",\n    }\n\n    def __init__(\n            self,\n            vocab_size: int = 50304,\n            n_positions: int = 2048,\n            n_embd: int = 1024,\n            n_layer: int = 20,\n            n_inner: Optional[int] = None,\n            n_head: int = 16,\n            n_head_kv: Optional[int] = None,\n            rotary_dim: Optional[int] = 32,\n            activation_function: Optional[str] = \"gelu_new\",\n            flash_attn: bool = False,\n            flash_rotary: bool = False,\n            fused_dense: bool = False,\n            attn_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            resid_pdrop: float = 0.0,\n            layer_norm_epsilon: float = 1e-5,\n            initializer_range: float = 0.02,\n            tie_word_embeddings: bool = False,\n            pad_vocab_size_multiple: int = 64,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ) -> None:\n        self.vocab_size = int(math.ceil(vocab_size / pad_vocab_size_multiple) * pad_vocab_size_multiple)\n        self.n_positions = n_positions\n        self.n_embd = n_embd\n        self.n_layer = n_layer\n        self.n_inner = n_inner\n        self.n_head = n_head\n        self.n_head_kv = n_head_kv\n        self.rotary_dim = min(rotary_dim, n_embd // n_head)\n        self.activation_function = activation_function\n        self.flash_attn = flash_attn\n        self.flash_rotary = flash_rotary\n        self.fused_dense = fused_dense\n        self.attn_pdrop = attn_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.resid_pdrop = resid_pdrop\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n        super().__init__(\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs\n        )\n\n    def add_jax_args(\n            self,\n            bits: Optional[int] = None,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            **kwargs\n    ):\n        self.bits = bits\n        self.gradient_checkpointing = gradient_checkpointing\n\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/phi/phi_configuration.py b/lib/python/EasyDel/modules/phi/phi_configuration.py
--- a/lib/python/EasyDel/modules/phi/phi_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/phi/phi_configuration.py	(date 1703666055300)
@@ -3,10 +3,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class PhiConfig(JaxBaseClassModel):
+class PhiConfig(EasyDelPretrainedConfig):
     """Phi configuration."""
 
     model_type = "phi"
Index: lib/python/EasyDel/modules/opt/opt_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass OPTConfig(JaxBaseClassModel):\n    model_type = \"opt\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    def __init__(\n            self,\n            vocab_size: int = 50272,\n            hidden_size: int = 768,\n            num_hidden_layers: int = 12,\n            ffn_dim: int = 3072,\n            max_position_embeddings: int = 2048,\n            do_layer_norm_before: bool = True,\n            _remove_final_layer_norm: bool = False,\n            word_embed_proj_dim: int = None,\n            dropout: float = 0.1,\n            attention_dropout: float = 0.0,\n            num_attention_heads: int = 12,\n            activation_function: str = \"relu\",\n            layerdrop: float = 0.0,\n            init_std: float = 0.02,\n            use_cache: bool = True,\n            pad_token_id: int = 1,\n            bos_token_id: int = 2,\n            eos_token_id: int = 2,\n            enable_bias: bool = True,\n            layer_norm_elementwise_affine: bool = True,\n            gradient_checkpointing: str = 'nothing_saveable',\n            use_pjit_attention_force: bool = False,\n            **kwargs,\n    ):\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            **kwargs,\n        )\n        self.vocab_size = vocab_size\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.gradient_checkpointing = gradient_checkpointing\n        self.max_position_embeddings = max_position_embeddings\n        self.num_attention_heads = num_attention_heads\n        self.word_embed_proj_dim = word_embed_proj_dim if word_embed_proj_dim is not None else hidden_size\n        self.ffn_dim = ffn_dim\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.dropout = dropout\n        self.attention_dropout = attention_dropout\n        self.activation_function = activation_function\n        self.init_std = init_std\n        self.layerdrop = layerdrop\n        self.use_cache = use_cache\n        self.do_layer_norm_before = do_layer_norm_before\n        self.enable_bias = enable_bias\n        self.layer_norm_elementwise_affine = layer_norm_elementwise_affine\n        self._remove_final_layer_norm = _remove_final_layer_norm\n        self.from_pt = False\n\n    def get_partition_rules(self, fully_fsdp: bool = True):\n        if not fully_fsdp:\n            raise NotImplementedError\n        else:\n            return (\n                ('.*', PartitionSpec((\"fsdp\", \"sp\")))\n            )\n\n    def add_jax_args(\n            self,\n            vocab_size: int = 50272,\n            hidden_size: int = 768,\n            num_hidden_layers: int = 12,\n            ffn_dim: int = 3072,\n            max_position_embeddings: int = 2048,\n            do_layer_norm_before: bool = True,\n            _remove_final_layer_norm: bool = False,\n            word_embed_proj_dim: int = None,\n            dropout: float = 0.1,\n            attention_dropout: float = 0.0,\n            num_attention_heads: int = 12,\n            activation_function: str = \"relu\",\n            layerdrop: float = 0.0,\n            init_std: float = 0.02,\n            use_cache: bool = True,\n            pad_token_id: int = 1,\n            bos_token_id: int = 2,\n            eos_token_id: int = 2,\n            enable_bias: bool = True,\n            layer_norm_elementwise_affine: bool = True,\n            gradient_checkpointing: str = 'nothing_saveable',\n            use_pjit_attention_force: bool = False,\n            **kwargs,\n    ):\n        basics = dict(\n            vocab_size=vocab_size,\n            hidden_size=hidden_size,\n            num_hidden_layers=num_hidden_layers,\n            ffn_dim=ffn_dim,\n            max_position_embeddings=max_position_embeddings,\n            do_layer_norm_before=do_layer_norm_before,\n            _remove_final_layer_norm=_remove_final_layer_norm,\n            word_embed_proj_dim=word_embed_proj_dim,\n            dropout=dropout,\n            attention_dropout=attention_dropout,\n            num_attention_heads=num_attention_heads,\n            activation_function=activation_function,\n            layerdrop=layerdrop,\n            init_std=init_std,\n            use_cache=use_cache,\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            enable_bias=enable_bias,\n            layer_norm_elementwise_affine=layer_norm_elementwise_affine,\n            gradient_checkpointing=gradient_checkpointing,\n            use_pjit_attention_force=use_pjit_attention_force,\n            **kwargs\n        )\n        for k, v in basics.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n        self.from_pt = False\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/opt/opt_configuration.py b/lib/python/EasyDel/modules/opt/opt_configuration.py
--- a/lib/python/EasyDel/modules/opt/opt_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/opt/opt_configuration.py	(date 1703665995417)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class OPTConfig(JaxBaseClassModel):
+class OPTConfig(EasyDelPretrainedConfig):
     model_type = "opt"
     keys_to_ignore_at_inference = ["past_key_values"]
 
Index: lib/python/EasyDel/modules/palm/palm_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass PalmConfig(JaxBaseClassModel):\n    def __init__(self,\n                 vocab_size: Optional[int] = 32000,\n                 hidden_size: Optional[int] = 4096,\n                 dim_head: Optional[int] = None,\n                 num_hidden_layers: Optional[int] = 32,\n                 num_attention_heads: Optional[int] = 32,\n                 up_inner_dim: Optional[int] = 4,\n                 eps: Optional[float] = 1e-5,\n                 max_length: int = 8196,  # Easydel trained palm with length of 8196\n                 bos_token_id: int = 0,\n                 eos_token_id: int = 1,\n                 gradient_checkpointing='nothing_saveable',\n                 use_pjit_attention_force: bool = False,\n                 use_tie_word_embedding: bool = True,\n                 **kwargs\n                 ):\n\n        dim_head = dim_head if dim_head is not None else hidden_size // num_attention_heads\n        self.dim_head = dim_head\n        self.up_inner_dim = up_inner_dim\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.gradient_checkpointing = gradient_checkpointing\n        self.num_attention_heads = num_attention_heads\n        self.use_tie_word_embedding = use_tie_word_embedding\n        self.num_hidden_layers = num_hidden_layers\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.eps = eps\n        self.max_length = max_length\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            **kwargs\n        )\n\n    @staticmethod\n    def _set_config_defaults(config, config_defaults):\n        for (k, v) in config_defaults.items():\n            if k not in config:\n                config[k] = v\n        return config\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = False):\n        return (\n            ('wi/kernel', PartitionSpec(\"fsdp\")),\n            ('attn_wo/kernel', PartitionSpec(\"fsdp\", \"dp\")),\n            ('ff_wo/kernel', PartitionSpec(\"fsdp\", \"dp\")),\n            ('wte/embedding', PartitionSpec(\"fsdp\", \"dp\")),\n            ('lm_head/kernel', PartitionSpec(\"fsdp\")),\n            ('post_norm/kernel', PartitionSpec(\"fsdp\")),\n            ('norm/kernel', PartitionSpec(\"fsdp\", \"dp\")),\n            ('.*', PartitionSpec(None)),\n        ) if not fully_fsdp else (\n            ('wi/kernel', PartitionSpec(\"fsdp\")),\n            ('attn_wo/kernel', PartitionSpec(\"fsdp\")),\n            ('ff_wo/kernel', PartitionSpec(\"fsdp\")),\n            ('wte/embedding', PartitionSpec(\"fsdp\")),\n            ('lm_head/kernel', PartitionSpec(\"fsdp\")),\n            ('post_norm/kernel', PartitionSpec(\"fsdp\")),\n            ('norm/kernel', PartitionSpec(\"fsdp\")),\n            ('.*', PartitionSpec(\"fsdp\")),\n        )\n\n    def add_jax_args(\n            self,\n            **kwargs,\n    ):\n        ...\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/palm/palm_configuration.py b/lib/python/EasyDel/modules/palm/palm_configuration.py
--- a/lib/python/EasyDel/modules/palm/palm_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/palm/palm_configuration.py	(date 1703666055312)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class PalmConfig(JaxBaseClassModel):
+class PalmConfig(EasyDelPretrainedConfig):
     def __init__(self,
                  vocab_size: Optional[int] = 32000,
                  hidden_size: Optional[int] = 4096,
Index: lib/python/EasyDel/modules/mixtral/mixtral_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass MixtralConfig(JaxBaseClassModel):\n    model_type = \"mixtral\"\n\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=14336,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=8,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096 * 32,\n            initializer_range=0.02,\n            rms_norm_eps=1e-5,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            rope_theta=1e6,\n            sliding_window=4096,\n            attention_dropout=0.0,\n            num_experts_per_tok=2,\n            num_local_experts=8,\n            output_router_logits=False,\n            router_aux_loss_coef=0.001,\n            gradient_checkpointing: str = 'nothing_saveable',\n            use_pjit_attention_force: bool = False,\n            use_flash_attention: bool = False,\n            use_sacn_mlp: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 1024,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            attn_pdrop: float = 0.0,\n            c_max_position_embeddings: int = 4096,\n            freq_max_position_embeddings: int = 4096,\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It allows the class to initialize the attributes of a class.\n        The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n        :param self: Represent the instance of the class\n        :param vocab_size: Define the size of the vocabulary\n        :param hidden_size: Determine the size of the embedding layers\n        :param intermediate_size: Define the size of the intermediate layer in each transformer block\n        :param num_hidden_layers: Determine the number of layers in the encoder and decoder\n        :param num_attention_heads: Determine the number of attention heads in each layer\n        :param num_key_value_heads: Specify the number of heads for key and value\n        :param hidden_act: Specify the activation function used in the hidden layers\n        :param max_position_embeddings: Set the maximum length of the sequence\n        :param initializer_range: Initialize the weights of the model\n        :param rms_norm_eps: Avoid division by zero in the rms normalization\n        :param use_cache: Determine whether to use the cache in the decoder\n        :param pad_token_id: Specify the token id of the padding token\n        :param bos_token_id: Specify the beginning of sentence token id\n        :param eos_token_id: Specify the end of sentence token\n        :param tie_word_embeddings: Tie the word embeddings and the output layer\n        :param rope_theta: Control the number of tokens in a rope\n        :param sliding_window: Control the number of tokens that are processed in parallel\n        :param gradient_checkpointing: str: Specify whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Force the use of pjit attention\n        :param use_flash_attention: bool: Enable the flash attention mechanism\n        :param use_sacn_mlp: bool: Determine whether or not to use the scan_mlp function\n        :param flash_attn_query_chunk_size: int: Determine the number of rows in each chunk\n        :param flash_attn_key_chunk_size: int: Control the size of chunks that are used for the key matrix in flash attention\n        :param scan_mlp_chunk_size: int: Specify the chunk size of the scan mlp\n        :param number_rep_kv: int: Specify the number of times to repeat the key and value vectors\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of tokens in a sequence\n        :param freq_max_position_embeddings: int: Set the maximum number of frequency bins that can be used in the model\n        :param bits: Optional[int]: Specify the number of bits used for quantization\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Specify the names of each axis in the tensor\n        :param &quot;mp&quot;): Define the maximum position embeddings\n        :param kwargs: Pass a variable number of keyword arguments to a function\n        :param : Define the number of layers in the model\n        :return: An instance of the class\n\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n        self.bits = bits\n        self.attention_dropout = attention_dropout\n        self.num_local_experts = num_local_experts\n        self.num_experts_per_tok = num_experts_per_tok\n        self.output_router_logits = output_router_logits\n        self.router_aux_loss_coef = router_aux_loss_coef\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = True):\n        \"\"\"\n        The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        :param fully_fsdp: bool: Determine whether to use the fully_fsdp partitioning scheme or not\n        :return: A list of tuples\n\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"sp\", \"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"sp\", \"fsdp\")),\n\n            (\"mlp/w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n            ('.*', PartitionSpec(None)),\n        ) if not fully_fsdp else (\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"mlp/w1/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/w2/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/w3/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            ('.*', PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(self,\n                     gradient_checkpointing: str = 'nothing_saveable',\n                     use_pjit_attention_force: bool = False,\n                     use_flash_attention: bool = False,\n                     use_sacn_mlp: bool = False,\n                     flash_attn_query_chunk_size: int = 1024,\n                     flash_attn_key_chunk_size: int = 1024,\n                     scan_mlp_chunk_size: int = 1024,\n                     number_rep_kv: int = 1,\n                     attn_pdrop: float = 0.0,\n                     c_max_position_embeddings: int = 4096,\n                     freq_max_position_embeddings: int = None,\n                     bits: Optional[int] = None,\n                     **kwargs,\n                     ):\n        \"\"\"\n        The add_jax_args function adds the following arguments to the model:\n\n        :param self: Bind the attributes and methods of a class to an instance of that class\n        :param gradient_checkpointing: str: Determine whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Determine whether to use the pjit_attention_force function\n        :param use_flash_attention: bool: Determine if the flash attention module is used or not\n        :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n        :param flash_attn_query_chunk_size: int: Specify the number of tokens that will be processed at a time\n        :param flash_attn_key_chunk_size: int: Chunk the keys for flash attention\n        :param scan_mlp_chunk_size: int: Chunk the input to the mlp\n        :param number_rep_kv: int: Control the number of times that the key and value vectors are repeated\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of positional embeddings for the causal axis\n        :param freq_max_position_embeddings: int: Set the maximum length of the frequency axis\n        :param bits: Optional[int]: Specify the number of bits to use for quantization\n        :return: A tuple of the following:\n\n        \"\"\"\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/mixtral/mixtral_configuration.py b/lib/python/EasyDel/modules/mixtral/mixtral_configuration.py
--- a/lib/python/EasyDel/modules/mixtral/mixtral_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/mixtral/mixtral_configuration.py	(date 1703665995449)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class MixtralConfig(JaxBaseClassModel):
+class MixtralConfig(EasyDelPretrainedConfig):
     model_type = "mixtral"
 
     def __init__(
Index: lib/python/EasyDel/modules/mosaic_mpt/mosaic_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional, Union\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass MptConfig(JaxBaseClassModel):\n    model_type = 'mpt'\n\n    def __init__(self,\n                 d_model: int = 2048,\n                 n_heads: int = 16,\n                 n_layers: int = 24,\n                 expansion_ratio: int = 4,\n                 max_seq_len: int = 2048,\n                 vocab_size: int = 50368,\n                 resid_prob_drop: float = 0.0,\n                 emb_prob_drop: float = 0.0,\n                 alibi: bool = True,\n                 use_bias: bool = False,\n                 learned_pos_emb: bool = True,\n                 act_fn: str = 'gelu',\n                 logit_scale: Optional[Union[float, str]] = None,\n                 no_bias: bool = False,\n                 verbose: int = 0,\n                 embedding_fraction: float = 1.0,\n                 use_cache: bool = False,\n                 qk_ln: bool = False,\n                 use_lm_head: bool = False,\n                 use_norm_bias: bool = False,\n                 gradient_checkpointing: str = 'nothing_saveable',\n                 use_pjit_attention_force: bool = False,\n                 use_flash_attention: bool = False,\n                 flash_attn_query_chunk_size: int = 1024,\n                 flash_attn_key_chunk_size: int = 2048,\n                 bits: Optional[int] = None,\n                 **kwargs\n                 ):\n\n        self.d_model = d_model\n        self.use_norm_bias = use_norm_bias\n        self.use_lm_head = use_lm_head\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.expansion_ratio = expansion_ratio\n        self.max_seq_len = max_seq_len\n        self.vocab_size = vocab_size\n        self.resid_prob_drop = resid_prob_drop\n        self.use_bias = use_bias\n        self.emb_prob_drop = emb_prob_drop\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.gradient_checkpointing = gradient_checkpointing\n        self.learned_pos_emb = learned_pos_emb\n        self.act_fn = act_fn\n        self.logit_scale = logit_scale\n        self.no_bias = no_bias\n        self.qk_ln = qk_ln\n        self.alibi = alibi\n        self.verbose = verbose\n        self.embedding_fraction = embedding_fraction\n        self.use_cache = use_cache\n        self.use_flash_attention = use_flash_attention\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.bits = bits\n\n        self.from_pt = False\n        if 'name' in kwargs:\n            del kwargs['name']\n        if 'loss_fn' in kwargs:\n            del kwargs['loss_fn']\n        super().__init__(\n            **kwargs\n        )\n\n    @staticmethod\n    def _set_config_defaults(config, config_defaults):\n        for (k, v) in config_defaults.items():\n            if k not in config:\n                config[k] = v\n        return config\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = False):\n        return (\n\n            (\"transformer/wte/embedding\", PartitionSpec(\"dp\", \"fsdp\")),\n            (\"transformer/wpe/embedding\", PartitionSpec(\"dp\", \"fsdp\")),\n\n            (\"attn/w_qkv/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"attn/wo/kernel\", PartitionSpec(\"dp\", \"fsdp\")),\n            (\"attn/w_qkv/bias\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"attn/wo/bias\", PartitionSpec(\"dp\", \"fsdp\")),\n\n            (\"ffn/down/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"ffn/up/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"ffn/down/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"ffn/up/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n\n            (\"attention_norm/kernel\", PartitionSpec(None)),\n            (\"norm_f/kernel\", PartitionSpec(None)),\n            (\"norm_f/bias\", PartitionSpec(None)),\n\n            (\"transformer/norm_f/kernel\", PartitionSpec(None)),\n            (\"transformer/norm_f/bias\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"lm_head/bias\", PartitionSpec(\"fsdp\", \"dp\")),\n            ('.*', PartitionSpec(None)),\n        ) if not fully_fsdp else (\n\n            (\"transformer/wte/embedding\", PartitionSpec(\"fsdp\")),\n            (\"transformer/wpe/embedding\", PartitionSpec(\"fsdp\")),\n\n            (\"attn/w_qkv/kernel\", PartitionSpec(\"fsdp\")),\n            (\"attn/wo/kernel\", PartitionSpec(\"fsdp\")),\n            (\"attn/w_qkv/bias\", PartitionSpec(\"fsdp\")),\n            (\"attn/wo/bias\", PartitionSpec(\"fsdp\")),\n\n            (\"ffn/down/kernel\", PartitionSpec(\"fsdp\")),\n            (\"ffn/up/kernel\", PartitionSpec(\"fsdp\")),\n            (\"ffn/down/kernel\", PartitionSpec(\"fsdp\")),\n            (\"ffn/up/kernel\", PartitionSpec(\"fsdp\")),\n\n            (\"attention_norm/kernel\", PartitionSpec(None)),\n            (\"norm_f/kernel\", PartitionSpec(None)),\n            (\"norm_f/bias\", PartitionSpec(None)),\n\n            (\"transformer/norm_f/kernel\", PartitionSpec(None)),\n            (\"transformer/norm_f/bias\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec(\"fsdp\")),\n            (\"lm_head/bias\", PartitionSpec(\"fsdp\")),\n            ('.*', PartitionSpec(None)),\n        )\n\n    def add_jax_args(self,\n                     d_model: int = 2048,\n                     n_heads: int = 16,\n                     n_layers: int = 24,\n                     expansion_ratio: int = 4,\n                     max_seq_len: int = 2048,\n                     vocab_size: int = 50368,\n                     resid_prob_drop: float = 0.0,\n                     emb_prob_drop: float = 0.0,\n                     alibi: bool = True,\n                     use_bias: bool = True,\n                     learned_pos_emb: bool = True,\n                     act_fn: str = 'gelu',\n                     logit_scale: Optional[Union[float, str]] = None,\n                     no_bias: bool = False,\n                     verbose: int = 0,\n                     embedding_fraction: float = 1.0,\n                     use_cache: bool = False,\n                     qk_ln: bool = True,\n                     use_lm_head: bool = False,\n                     use_norm_bias: bool = False,\n                     gradient_checkpointing: str = 'nothing_saveable',\n                     use_pjit_attention_force: bool = False,\n                     use_flash_attention: bool = False,\n                     flash_attn_query_chunk_size: int = 1024,\n                     flash_attn_key_chunk_size: int = 2048,\n                     bits: Optional[int] = None,\n                     **kwargs,\n                     ):\n        if hasattr(self, 'attn_config'):\n            for k, v in self.attn_config.items():\n                setattr(self, k, v)\n        basics = dict(\n            bits=bits,\n            d_model=d_model,\n            n_heads=n_heads,\n            n_layers=n_layers,\n            expansion_ratio=expansion_ratio,\n            max_seq_len=max_seq_len,\n            vocab_size=vocab_size,\n            resid_prob_drop=resid_prob_drop,\n            emb_prob_drop=emb_prob_drop,\n            alibi=alibi,\n            use_bias=use_bias,\n            learned_pos_emb=learned_pos_emb,\n            act_fn=act_fn,\n            logit_scale=logit_scale,\n            no_bias=no_bias,\n            verbose=verbose,\n            embedding_fraction=embedding_fraction,\n            use_cache=use_cache,\n            qk_ln=qk_ln,\n            use_lm_head=use_lm_head,\n            use_norm_bias=use_norm_bias,\n            gradient_checkpointing=gradient_checkpointing,\n            use_pjit_attention_force=use_pjit_attention_force,\n            use_flash_attention=use_flash_attention,\n            flash_attn_query_chunk_size=flash_attn_query_chunk_size,\n            flash_attn_key_chunk_size=flash_attn_key_chunk_size,\n            **kwargs\n        )\n        for k, v in basics.items():\n            if not hasattr(self, k):\n                print(f' Key {k} not found in loaded config setting that to default of {v}')\n                setattr(self, k, v)\n\n        self.from_pt = False\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/mosaic_mpt/mosaic_configuration.py b/lib/python/EasyDel/modules/mosaic_mpt/mosaic_configuration.py
--- a/lib/python/EasyDel/modules/mosaic_mpt/mosaic_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/mosaic_mpt/mosaic_configuration.py	(date 1703665995429)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class MptConfig(JaxBaseClassModel):
+class MptConfig(EasyDelPretrainedConfig):
     model_type = 'mpt'
 
     def __init__(self,
Index: lib/python/EasyDel/modules/lucid_transformer/lt_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass FlaxLTConfig(JaxBaseClassModel):\n    def __init__(self,\n                 initializer_range: float = 0.02,\n                 hidden_size: int = 4096,\n                 bos_token_id=2,\n                 eos_token_id=1,\n                 pad_token_id=0,\n                 intermediate_size: int = 8192,\n                 num_hidden_layers: int = 32,\n                 vocab_size: int = 32000,\n                 num_attention_heads: int = 32,\n                 weight_decay: float = 0.02,\n                 max_sequence_length: int = 2048,\n                 softmax_scale: float = None,\n                 alibi_bias_max: int = 8,\n                 fsdp=False,\n                 hidden_act=\"silu\",\n                 **kwargs\n                 ):\n        self.max_sequence_length = max_sequence_length\n        self.weight_decay = weight_decay\n        self.alibi_bias_max = alibi_bias_max\n        self.num_attention_heads = num_attention_heads\n        self.vocab_size = vocab_size\n        self.num_hidden_layers = num_hidden_layers\n        self.intermediate_size = intermediate_size\n        self.pad_token_id = pad_token_id\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.hidden_size = hidden_size\n        self.initializer_range = initializer_range\n        self.softmax_scale = softmax_scale\n        self.fsdp = fsdp\n        self.hidden_act = hidden_act\n\n        super().__init__(\n            eos_token_id=eos_token_id,\n            bos_token_id=bos_token_id,\n            pad_token_id=pad_token_id,\n            **kwargs\n        )\n\n    @staticmethod\n    def get_partition_rules():\n        return (\n            # Emb\n            (\"model/wte/embedding\", PartitionSpec(\"sp\", \"fsdp\")),\n            (\"attn/(k_proj|v_proj|q_proj)/kernel\", PartitionSpec(\"fsdp\")),\n            (\"attn/o_proj/kernel\", PartitionSpec(\"sp\", \"fsdp\")),\n            (\"mlp/down/kernel\", PartitionSpec(\"sp\", \"fsdp\")),\n            (\"mlp/up/kernel\", PartitionSpec(\"fsdp\")),\n            (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"sp\")),\n            ('.*', PartitionSpec(None)),\n            ('ln/kernel', PartitionSpec(None)),\n            ('ln1/kernel', PartitionSpec(None)),\n            ('ln2/kernel', PartitionSpec(None)),\n        )\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n\n    def add_jax_args(self, *args, **kwargs):\n        ...\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/lucid_transformer/lt_configuration.py b/lib/python/EasyDel/modules/lucid_transformer/lt_configuration.py
--- a/lib/python/EasyDel/modules/lucid_transformer/lt_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/lucid_transformer/lt_configuration.py	(date 1703665995433)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class FlaxLTConfig(JaxBaseClassModel):
+class FlaxLTConfig(EasyDelPretrainedConfig):
     def __init__(self,
                  initializer_range: float = 0.02,
                  hidden_size: int = 4096,
Index: lib/python/EasyDel/modules/mistral/mistral_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass MistralConfig(JaxBaseClassModel):\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=14336,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=8,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096 * 32,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            sliding_window=4096,\n            gradient_checkpointing: str = 'nothing_saveable',\n            use_pjit_attention_force: bool = False,\n            use_flash_attention: bool = False,\n            use_sacn_mlp: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 1024,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            attn_pdrop: float = 0.0,\n            c_max_position_embeddings: int = 4096,\n            freq_max_position_embeddings: int = 4096,\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It allows the class to initialize the attributes of a class.\n        The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n        :param self: Represent the instance of the class\n        :param vocab_size: Define the size of the vocabulary\n        :param hidden_size: Determine the size of the embedding layers\n        :param intermediate_size: Define the size of the intermediate layer in each transformer block\n        :param num_hidden_layers: Determine the number of layers in the encoder and decoder\n        :param num_attention_heads: Determine the number of attention heads in each layer\n        :param num_key_value_heads: Specify the number of heads for key and value\n        :param hidden_act: Specify the activation function used in the hidden layers\n        :param max_position_embeddings: Set the maximum length of the sequence\n        :param initializer_range: Initialize the weights of the model\n        :param rms_norm_eps: Avoid division by zero in the rms normalization\n        :param use_cache: Determine whether to use the cache in the decoder\n        :param pad_token_id: Specify the token id of the padding token\n        :param bos_token_id: Specify the beginning of sentence token id\n        :param eos_token_id: Specify the end of sentence token\n        :param tie_word_embeddings: Tie the word embeddings and the output layer\n        :param rope_theta: Control the number of tokens in a rope\n        :param sliding_window: Control the number of tokens that are processed in parallel\n        :param gradient_checkpointing: str: Specify whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Force the use of pjit attention\n        :param use_flash_attention: bool: Enable the flash attention mechanism\n        :param use_sacn_mlp: bool: Determine whether or not to use the scan_mlp function\n        :param flash_attn_query_chunk_size: int: Determine the number of rows in each chunk\n        :param flash_attn_key_chunk_size: int: Control the size of chunks that are used for the key matrix in flash attention\n        :param scan_mlp_chunk_size: int: Specify the chunk size of the scan mlp\n        :param number_rep_kv: int: Specify the number of times to repeat the key and value vectors\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of tokens in a sequence\n        :param freq_max_position_embeddings: int: Set the maximum number of frequency bins that can be used in the model\n        :param bits: Optional[int]: Specify the number of bits used for quantization\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Specify the names of each axis in the tensor\n        :param &quot;mp&quot;): Define the maximum position embeddings\n        :param kwargs: Pass a variable number of keyword arguments to a function\n        :param : Define the number of layers in the model\n        :return: An instance of the class\n\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n        self.bits = bits\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = True):\n        \"\"\"\n        The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        :param fully_fsdp: bool: Determine whether to use the fully_fsdp partitioning scheme or not\n        :return: A list of tuples\n\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"dp\", \"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"dp\", \"fsdp\")),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"dp\", \"fsdp\")),\n            (\"mlp/up_proj/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec(\"fsdp\", \"dp\")),\n            ('.*', PartitionSpec(None)),\n        ) if not fully_fsdp else (\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            ('.*', PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(self,\n                     gradient_checkpointing: str = 'nothing_saveable',\n                     use_pjit_attention_force: bool = False,\n                     use_flash_attention: bool = False,\n                     use_sacn_mlp: bool = False,\n                     flash_attn_query_chunk_size: int = 1024,\n                     flash_attn_key_chunk_size: int = 1024,\n                     scan_mlp_chunk_size: int = 1024,\n                     number_rep_kv: int = 1,\n                     attn_pdrop: float = 0.0,\n                     c_max_position_embeddings: int = 4096,\n                     freq_max_position_embeddings: int = None,\n                     bits: Optional[int] = None,\n                     **kwargs,\n                     ):\n        \"\"\"\n        The add_jax_args function adds the following arguments to the model:\n\n        :param self: Bind the attributes and methods of a class to an instance of that class\n        :param gradient_checkpointing: str: Determine whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Determine whether to use the pjit_attention_force function\n        :param use_flash_attention: bool: Determine if the flash attention module is used or not\n        :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n        :param flash_attn_query_chunk_size: int: Specify the number of tokens that will be processed at a time\n        :param flash_attn_key_chunk_size: int: Chunk the keys for flash attention\n        :param scan_mlp_chunk_size: int: Chunk the input to the mlp\n        :param number_rep_kv: int: Control the number of times that the key and value vectors are repeated\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of positional embeddings for the causal axis\n        :param freq_max_position_embeddings: int: Set the maximum length of the frequency axis\n        :param bits: Optional[int]: Specify the number of bits to use for quantization\n        :return: A tuple of the following:\n\n        \"\"\"\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/mistral/mistral_configuration.py b/lib/python/EasyDel/modules/mistral/mistral_configuration.py
--- a/lib/python/EasyDel/modules/mistral/mistral_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/mistral/mistral_configuration.py	(date 1703665995453)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class MistralConfig(JaxBaseClassModel):
+class MistralConfig(EasyDelPretrainedConfig):
     def __init__(
             self,
             vocab_size=32000,
Index: lib/python/EasyDel/modules/llama/llama_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional, Dict, Union\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass LlamaConfig(JaxBaseClassModel):\n    model_type = \"llama\"\n\n    def __init__(\n            self,\n            vocab_size: int = 32000,\n            hidden_size: int = 4096,\n            intermediate_size: int = 11008,\n            num_hidden_layers: int = 32,\n            num_attention_heads: int = 32,\n            number_rep_kv: int = 1,\n            num_key_value_heads: Optional[int] = None,\n            max_position_embeddings: int = 2048,\n            rms_norm_eps: float = 1e-6,\n            initializer_range: float = 0.02,\n            use_cache: bool = True,\n            bos_token_id: int = 0,\n            eos_token_id: int = 1,\n            resid_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            attention_dropout: float = 0.0,\n            rope_theta: float = 10000.,\n            attention_bias: bool = False,\n            tie_word_embeddings: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            fcm_min_ratio: float = -1,\n            fcm_max_ratio: float = -1,\n            use_pjit_attention_force: bool = False,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            use_flash_attention: bool = False,\n            use_sacn_mlp: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 1024,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            hidden_act: str = 'silu',\n            pretraining_tp: int = 1,\n            scan_layers: bool = True,\n            use_shard_map: bool = True,\n            **kwargs,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the attributes of an object, which are sometimes called fields or properties.\n        The __init__ function can accept arguments, but self must be the first one.\n\n        :param self: Refer to the object itself\n        :param vocab_size: int: Set the size of the vocabulary\n        :param hidden_size: int: Set the size of the hidden layers in each transformer block\n        :param intermediate_size: int: Set the size of the intermediate layer\n        :param num_hidden_layers: int: Determine the number of layers in the transformer\n        :param num_attention_heads: int: Determine the number of attention heads\n        :param number_rep_kv: int: Set the number of times to repeat the key and value vectors\n        :param num_key_value_heads: Optional[int]: Define the number of key-value heads\n        :param max_position_embeddings: int: Set the maximum length of a sequence\n        :param rms_norm_eps: float: Prevent division by zero in the rms normalization\n        :param initializer_range: float: Initialize the weights of the model\n        :param use_cache: bool: Determine whether the attention layer should use a cache for faster computation\n        :param bos_token_id: int: Set the beginning of sequence token\n        :param eos_token_id: int: Specify the end of sentence token\n        :param resid_pdrop: float: Set the dropout rate for residual connections\n        :param embd_pdrop: float: Dropout the embedding layer\n        :param attention_dropout: float: Dropout the attention weights\n        :param tie_word_embeddings: bool: Tie the word embeddings and output layer weights\n        :param gradient_checkpointing: str: Specify how to checkpoint the gradients\n        :param fcm_min_ratio: float: Set the minimum ratio of the number of elements in a tensor to be processed by flash\n        :param fcm_max_ratio: float: Determine the maximum ratio of\n        :param use_pjit_attention_force: bool: Determine whether to use the pytorch jit compiler\n        :param rope_scaling: Dict[str: Define the scaling of the rope\n        :param Union[str: Specify the type of the parameter\n        :param float]]: Specify the type of the parameter\n        :param use_shard_map: bool: when ever to use shard_map for attention\n        :param use_flash_attention: bool: Determine whether to use the flash attention or not\n        :param use_sacn_mlp: bool: Determine whether to use scan_mlp or not\n        :param flash_attn_query_chunk_size: int: Specify the chunk size of the query tensor\n        :param flash_attn_key_chunk_size: int: Determine the chunk size of the key tensor\n        :param scan_mlp_chunk_size: int: Specify the chunk size of the scan_mlp\n        :param bits: Optional[int]: Specify the number of bits used to quantize the weights\n        :param rope_theta: float : rope_theta for compute rope\n        :param attention_bias: bool : whenever to use attention bias or no\n        :param hidden_act: str : hidden_act for mlp\n        :param axis_dims: Sequence[int]: Specify the dimensions of each axis\n        :param axis_names: Sequence[str]: Specify the names of the axes in a tensor\n        :param scan_layers: bool: Determine whether to use the scan_layers or not\n        :param kwargs: Pass a variable number of keyword arguments to a function\n        :param : Define the number of layers in the model\n        :return: Nothing\n\n        \"\"\"\n        num_key_value_heads = num_key_value_heads or number_rep_kv * num_attention_heads\n        self.num_key_value_heads = num_key_value_heads\n        self.vocab_size = vocab_size\n\n        self.number_rep_kv = number_rep_kv\n        self.hidden_size = hidden_size\n        self.initializer_range = initializer_range\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.num_attention_heads = num_attention_heads\n        self.max_position_embeddings = max_position_embeddings\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.pretraining_tp = pretraining_tp\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attention_dropout = attention_dropout\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.fcm_min_ratio = fcm_min_ratio\n        self.hidden_act = hidden_act\n        self.fcm_max_ratio = fcm_max_ratio\n        self.rope_scaling = rope_scaling\n        self.use_flash_attention = use_flash_attention\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n        self.use_sacn_mlp = use_shard_map\n        self.scan_layers = scan_layers\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = True):\n        \"\"\"\n        The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        :param fully_fsdp: bool: Determine whether to partition the model fully or not\n        :return: A list of tuples\n\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n            ('.*', PartitionSpec(None)),\n        ) if not fully_fsdp else (\n\n            (\"model/embed_tokens/embedding\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"self_attn/o_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"mlp/gate_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/down_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            (\"mlp/up_proj/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n\n            (\"input_layernorm/kernel\", PartitionSpec(None)),\n            (\"post_attention_layernorm/kernel\", PartitionSpec(None)),\n\n            (\"model/norm/kernel\", PartitionSpec(None)),\n            (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"sp\"))),\n            ('.*', PartitionSpec((\"fsdp\", \"sp\"))),\n        )\n\n    def add_jax_args(self,\n                     resid_pdrop: float = 0.0,\n                     embd_pdrop: float = 0.0,\n                     attention_dropout: float = 0.0,\n                     tie_word_embeddings: bool = False,\n                     gradient_checkpointing: str = 'nothing_saveable',\n                     fcm_min_ratio: float = 0.0,\n                     fcm_max_ratio: float = 0.0,\n                     use_pjit_attention_force: bool = False,\n                     use_flash_attention: bool = False,\n                     use_sacn_mlp: bool = False,\n                     flash_attn_query_chunk_size: int = 1024,\n                     flash_attn_key_chunk_size: int = 1024,\n                     scan_mlp_chunk_size: int = 1024,\n                     number_rep_kv: int = 1,\n                     bits: Optional[int] = None,\n                     rope_theta: float = 10000.,\n                     attention_bias: bool = False,\n                     hidden_act: str = 'silu',\n                     scan_layers: bool = True,\n                     **kwargs,\n                     ):\n        \"\"\"\n        The add_jax_args function adds the following arguments to the Transformer class:\n\n        :param self: Refer to the current object\n        :param resid_pdrop: float: Set the dropout rate for residual connections\n        :param embd_pdrop: float: Set the probability of dropping an embedding\n        :param attention_dropout: float: Set the probability of dropping out the attention layer\n        :param tie_word_embeddings: bool: Tie the word embeddings to the decoder\n        :param gradient_checkpointing: str: Control the amount of memory used by jax\n        :param fcm_min_ratio: float: Control the minimum ratio of the number of chunks to be used in flash-based computation\n        :param fcm_max_ratio: float: Set the maximum ratio of the number of input tokens to output tokens\n        :param use_pjit_attention_force: bool: Determine if the attention force is used\n        :param use_flash_attention: bool: Determine whether to use the flash attention or not\n        :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n        :param flash_attn_query_chunk_size: int: Determine the size of the chunks that will be used to compute\n        :param flash_attn_key_chunk_size: int: Set the size of the key chunk\n        :param scan_mlp_chunk_size: int: Set the chunk size for scan_mlp\n        :param number_rep_kv: int: Determine how many times the key and value vectors are repeated\n        :param bits: Optional[int]: Determine the number of bits used in the quantization\n        :param rope_theta: float : rope_theta for compute rope\n        :param attention_bias: bool : whenever to use attention bias or no\n        :param hidden_act: str : hidden_act for mlp\n        :param scan_layers: bool: Determine whether to use scan layers or not\n        :return: The following:\n\n        \"\"\"\n        self.scan_layers = scan_layers\n        self.use_flash_attention = use_flash_attention\n        self.embd_pdrop = embd_pdrop\n        self.number_rep_kv = number_rep_kv\n        self.resid_pdrop = resid_pdrop\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.hidden_act = hidden_act\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.fcm_min_ratio = fcm_min_ratio\n        self.fcm_max_ratio = fcm_max_ratio\n        self.use_pjit_attention_force = use_pjit_attention_force\n\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/llama/llama_configuration.py b/lib/python/EasyDel/modules/llama/llama_configuration.py
--- a/lib/python/EasyDel/modules/llama/llama_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/llama/llama_configuration.py	(date 1703666055328)
@@ -1,11 +1,11 @@
-from typing import Sequence, Optional, Dict, Union
+from typing import Optional, Dict, Union
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class LlamaConfig(JaxBaseClassModel):
+class LlamaConfig(EasyDelPretrainedConfig):
     model_type = "llama"
 
     def __init__(
Index: lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import math\nfrom flax import linen as nn\nfrom flax.core import FrozenDict\nfrom typing import Optional, Union, Tuple\nfrom transformers import FlaxPreTrainedModel\nfrom jax import numpy as jnp\nimport jax\nfrom jax.sharding import PartitionSpec\nfrom transformers.modeling_flax_outputs import FlaxCausalLMOutput, FlaxBaseModelOutput\nimport flax\nfrom einops import rearrange\nfrom ..flax_modelling_utils import (\n    get_gradient_checkpoint_policy,\n    with_sharding_constraint,\n    ACT2FN,\n    smart_flash_attention, get_dot_general_by_bits\n)\nimport chex\n\nfrom .mosaic_configuration import MptConfig\n\n\nclass RMSNorm(nn.Module):\n    dim: int\n    eps: float = 1e-6\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n\n    def setup(self) -> None:\n        self.weight = self.param(\n            'kernel',\n            nn.initializers.ones,\n            (self.dim,),\n            self.param_dtype,\n        )\n\n    def _norm(self, x: jnp.ndarray) -> jnp.ndarray:\n        return x * jax.lax.rsqrt(jnp.square(x).mean(-1, keepdims=True) + self.eps)\n\n    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n        x = x.astype(jnp.promote_types(self.dtype, jnp.bfloat16))\n        output = self._norm(x).astype(self.dtype)\n        weight = jnp.asarray(self.weight, self.dtype)\n        return output * weight\n\n\nclass FlaxMptMLP(nn.Module):\n    config: MptConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.up = nn.Dense(\n            self.config.d_model * self.config.expansion_ratio,\n            kernel_init=jax.nn.initializers.normal(),\n            use_bias=self.config.use_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.down = nn.Dense(\n            self.config.d_model,\n            kernel_init=jax.nn.initializers.normal(),\n            use_bias=self.config.use_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.act = ACT2FN[self.config.act_fn]\n\n    def __call__(self, hidden_states: chex.Array):\n        return self.down(self.act(self.up(hidden_states)))\n\n\nclass FlaxMptAttention(nn.Module):\n    config: MptConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n\n        self.w_qkv = nn.Dense(\n            self.config.d_model * 3,\n            kernel_init=jax.nn.initializers.normal(),\n            use_bias=self.config.use_bias,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision)\n        self.wo = nn.Dense(\n            self.config.d_model,\n            kernel_init=jax.nn.initializers.normal(),\n            use_bias=self.config.use_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        if self.config.qk_ln:\n            self.q_ln = nn.LayerNorm(use_bias=self.config.use_norm_bias)\n            self.k_ln = nn.LayerNorm(use_bias=self.config.use_norm_bias)\n        self.causal_mask = nn.make_causal_mask(jnp.ones((1, self.config.max_seq_len)))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, query, value, attention_mask):\n        is_initialized = self.has_variable('cache', 'key')\n        cache_key = self.variable('cache', 'key', jnp.zeros, key.shape, key.dtype)\n        cache_value = self.variable('cache', 'value', jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable('cache', 'index', lambda: jnp.array(0, dtype=jnp.int32))\n        if is_initialized:\n            *b, s, h, d = cache_key.value.shape\n            cur_index = cache_index.value\n            indices = (0,) * len(b) + (cur_index, 0, 0)\n            key = jax.lax.dynamic_update_slice(cache_key.value, key, indices)\n            value = jax.lax.dynamic_update_slice(cache_value.value, value, indices)\n            cache_value.value = value\n            cache_key.value = key\n            num_updated_vector = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_vector\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(s) < cur_index + num_updated_vector,\n                tuple(b) + (1, num_updated_vector, s),\n            )\n\n            attention_mask = nn.combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(self,\n                 hidden_states: chex.Array,\n                 attention_mask: chex.Array,\n                 position_ids: chex.Array,\n                 attn_bias: chex.Array = None,\n                 init_cache: bool = False\n                 ):\n\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, just like any other Python function.\n        The difference is that __call__ can also take in state (e.g., parameters) from the module itself,\n        and it can update that state as part of its computation.\n\n        :param self: Access variables that belong to the class\n        :param hidden_states: chex.Array: Pass the input to the attention layer\n        :param attention_mask: chex.Array: Mask out certain positions in the sequence\n        :param position_ids: chex.Array: Specify the position of each token in the sequence\n        :param attn_bias: chex.Array: Add a bias to the attention scores\n        :param init_cache: bool: Initialize the cache\n        :return: The output of the attention layer\n        \n        \"\"\"\n        inp_shape = hidden_states.shape\n        b, s, ds = inp_shape\n        qkv = self.w_qkv(hidden_states)\n        q, k, v = jnp.split(qkv, 3, -1)\n        if self.config.qk_ln:\n            q = self.q_ln(q)\n            k = self.k_ln(k)\n        if self.config.use_pjit_attention_force:\n            q = with_sharding_constraint(q, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            k = with_sharding_constraint(k, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            v = with_sharding_constraint(v, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n        q = rearrange(q, 'b s (h d) -> b s h d', h=self.config.n_heads)\n        k = rearrange(k, 'b s (h d) -> b s h d', h=self.config.n_heads)\n        v = rearrange(v, 'b s (h d) -> b s h d', h=self.config.n_heads)\n        attention_mask = attention_mask.reshape(b, 1, 1, -1)\n        if self.has_variable('cache', 'key') or init_cache:\n            k, v, attention_mask = self._concatenate_to_cache(key=k, value=v, query=q, attention_mask=attention_mask)\n\n        q_l = q.shape[1]\n        k_l = k.shape[1]\n        dropout_rng = None\n        deterministic = False\n        if deterministic:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n            attention_bias = jax.lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(q, rtp_axis),\n                k=jnp.transpose(k, rtp_axis),\n                v=jnp.transpose(b, rtp_axis),\n                bias=attention_bias + attn_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=q_l,\n                kv_seq_len=k_l,\n                attn_pdrop=self.config.attn_pdrop,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            d = q.shape[-1]\n            attn_output = jnp.einsum('...qhd,...khd->...hqk', q, k, precision=self.precision) * jax.lax.rsqrt(\n                jnp.asarray(d).astype(v.dtype))\n            if self.config.use_pjit_attention_force:\n                attn_output = with_sharding_constraint(attn_output, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None))\n            if attn_bias is not None:\n                attn_output += attn_bias[:, :, :, :attn_output.shape[-1]]\n            mask = jnp.where(self.causal_mask == 1, 0, jnp.finfo(attn_output).min)\n            if attention_mask is not None:\n                attention_mask = jnp.where(\n                    attention_mask == 1,\n                    0,\n                    jnp.finfo(attn_output).min\n                )\n                attn_output += attention_mask\n            attn_output += mask[:, :, :attn_output.shape[-2], :attn_output.shape[-1]]\n            attn_output = nn.softmax(attn_output, -1)\n            attn_output = jnp.einsum('...hqk,...khd->...qhd', attn_output, v)\n        return self.wo(attn_output.reshape(inp_shape))\n\n\nclass FlaxMptBlock(nn.Module):\n    config: MptConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.norm_1 = nn.LayerNorm(use_bias=self.config.use_norm_bias)\n        self.norm_2 = nn.LayerNorm(use_bias=self.config.use_norm_bias)\n        self.attn = FlaxMptAttention(config=self.config, dtype=self.dtype, param_dtype=self.param_dtype,\n                                     precision=self.precision)\n        self.ffn = FlaxMptMLP(config=self.config, dtype=self.dtype, param_dtype=self.param_dtype,\n                              precision=self.precision)\n\n    def __call__(self,\n                 hidden_states: chex.Array,\n                 attention_mask: chex.Array,\n                 position_ids: chex.Array,\n                 attn_bias: chex.Array = None,\n                 init_cache: bool = False\n                 ):\n        hidden_states = (\n                self.attn(\n                    self.norm_1(hidden_states),\n                    attn_bias=attn_bias,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    init_cache=init_cache\n                ) + hidden_states\n        )\n        hidden_states = self.ffn(self.norm_2(hidden_states)) + hidden_states\n        return hidden_states\n\n\nclass FlaxMptCollection(nn.Module):\n    config: MptConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        block = FlaxMptBlock\n\n        if self.config.gradient_checkpointing != '':\n            block = flax.linen.remat(\n                block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(5,)\n            )\n\n        self.blocks = [\n            block(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            )\n            for i in range(\n                self.config.n_layers\n            )\n        ]\n\n    def __call__(self,\n                 hidden_states: chex.Array,\n                 attention_mask: chex.Array,\n                 position_ids: chex.Array,\n                 attn_bias: chex.Array = None,\n                 init_cache: bool = False\n                 ):\n        for block in self.blocks:\n            hidden_states = block(\n                hidden_states=hidden_states,\n                attn_bias=attn_bias,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                init_cache=init_cache\n            )\n        return hidden_states\n\n\ndef build_alibi(max_length, num_attention_heads, alibi_max: int = 8):\n    w_range = jnp.arange(1 - max_length, 1).reshape(1, 1, 1, max_length)\n    # cp2 = jnp.power(2, jnp.ceil(jnp.log2(num_attention_heads)))\n    cp2 = 2 ** math.ceil(math.log2(num_attention_heads))\n    h_range = jnp.arange(1, 1 + num_attention_heads, ).reshape(1, -1, 1, 1)\n    h_range = jnp.matmul(h_range, jnp.asarray(alibi_max / cp2).reshape(1, 1))\n    slop = 1 / jnp.power(2, h_range)\n    if cp2 != num_attention_heads:\n        slop = jnp.concatenate([slop[1::2], slop[::2]], axis=-1)[:num_attention_heads]\n    alibi = (w_range * slop).reshape(1, num_attention_heads, 1, max_length)\n    return alibi\n\n\nclass FlaxMptModule(nn.Module):\n    config: MptConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.wte = nn.Embed(num_embeddings=self.config.vocab_size, features=self.config.d_model)\n        if not self.config.alibi:\n            self.wpe = nn.Embed(num_embeddings=self.config.vocab_size, features=self.config.max_seq_len)\n        self.h = FlaxMptCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm_f = nn.LayerNorm(use_bias=self.config.use_norm_bias)\n        self.alibi = build_alibi(self.config.max_seq_len, self.config.n_heads)\n\n    def __call__(self,\n\n                 input_ids: chex.Array,\n                 attention_mask: chex.Array = None,\n                 position_ids: chex.Array = None,\n                 init_cache: bool = False,\n                 return_dict: bool = True,\n                 extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n                 ):\n        b, s = input_ids.shape\n        hidden_states = self.wte(input_ids)\n        hidden_states = hidden_states + extra_embedding if extra_embedding is not None else hidden_states\n\n        if self.config.alibi:\n            alibi = self.alibi\n        else:\n            pos_id = self.wpe(jnp.arange(s, dtype='i4').reshape(1, -1))\n            hidden_states += pos_id\n            alibi = None\n        hidden_states = self.norm_f(\n            self.h(\n                hidden_states,\n                attn_bias=alibi,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                init_cache=init_cache\n            )\n        )\n        if return_dict:\n            return FlaxBaseModelOutput(last_hidden_state=hidden_states, hidden_states=None)\n        else:\n            return (hidden_states,)\n\n\nclass FlaxMptPretrainedModel(FlaxPreTrainedModel):\n    module_class: nn.Module = None\n    config_class: MptConfig = MptConfig\n\n    def __init__(self,\n                 config,\n                 dtype: jnp.dtype = jnp.float32,\n                 param_dtype: jnp.dtype = jnp.float32,\n                 _do_init: bool = False,\n                 precision: Optional[Union[jax.lax.Precision, None]] = jax.lax.Precision('fastest'),\n                 input_shape: Tuple = (1, 16),\n                 **kwargs\n                 ):\n        module = self.module_class(\n            config,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision\n        )\n        super().__init__(_do_init=_do_init, config=config, input_shape=input_shape, module=module, **kwargs)\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length), dtype='i4')\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0),\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            return_dict=False,\n            init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        input_ids = jnp.ones(input_shape, dtype='i4')\n        if params is None:\n            return self.module.init(\n                rngs=rng,\n                input_ids=input_ids,\n                attention_mask=jnp.ones(input_shape, dtype='i4'),\n                position_ids=jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape),\n                init_cache=False\n            )['params']\n        else:\n            return params\n\n    def __call__(self,\n                 input_ids,\n                 attention_mask=None,\n                 past_key_values=None,\n                 position_ids=None,\n                 init_cache: bool = False,\n                 params: dict = None,\n                 add_params_field: bool = False,\n                 return_dict: bool = True,\n                 extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n                 ):\n        params = {'params': params or self.params} if add_params_field else params or self.params\n        input_ids = jnp.asarray(input_ids, dtype='i4')\n        mutable = False\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids, dtype='i4')\n        if position_ids is None:\n            position_ids = jnp.arange(0, attention_mask.shape[-1], 1, dtype='i4').reshape(\n                1, -1\n            ).repeat(input_ids.shape[0], axis=0)\n\n        if past_key_values is not None:\n            params['cache'] = past_key_values\n            mutable = ['cache']\n        rngs = {}\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n        predict = self.module.apply(\n            params,\n            input_ids=input_ids,\n            attention_mask=jnp.asarray(attention_mask, dtype='i4'),\n            return_dict=return_dict,\n            extra_embedding=extra_embedding,\n            position_ids=position_ids,\n            init_cache=init_cache,\n            mutable=mutable,\n            rngs=rngs\n        )\n        if past_key_values is not None and return_dict:\n            predict, past_key_values = predict\n            predict[\"past_key_values\"] = flax.core.unfreeze(past_key_values[\"cache\"])\n            return predict\n        elif past_key_values is not None and not return_dict:\n            predict, past_key_values = predict\n            predict = predict[:1] + (flax.core.unfreeze(past_key_values[\"cache\"]),) + predict[1:]\n        return predict\n\n\nclass FlaxMptModel(FlaxMptPretrainedModel):\n    module_class = FlaxMptModule\n\n\nclass FlaxMptForCausalLMModule(nn.Module):\n    config: MptConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.transformer = FlaxMptModule(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n        if self.config.use_lm_head:\n            self.lm_head = nn.Dense(self.config.vocab_size, kernel_init=jax.nn.initializers.normal(),\n                                    use_bias=self.config.use_bias,\n                                    dtype=self.dtype, param_dtype=self.param_dtype, precision=self.precision,\n                                    **get_dot_general_by_bits(self.config.bits, self.config.easy_method))\n\n    def __call__(self,\n                 input_ids: chex.Array,\n                 attention_mask: chex.Array = None,\n                 init_cache: bool = False,\n                 position_ids: chex.Array = None,\n                 return_dict: bool = True,\n                 extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n\n                 ):\n        predict: FlaxBaseModelOutput = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            extra_embedding=extra_embedding,\n            position_ids=position_ids,\n            init_cache=init_cache\n        )\n        if self.config.use_lm_head:\n            logits = self.lm_head(predict.last_hidden_state)\n        else:\n            logits = predict.last_hidden_state @ self.transformer.wte.embedding.T\n        if return_dict:\n\n            return FlaxCausalLMOutput(\n                logits=logits,\n                hidden_states=predict.last_hidden_state\n            )\n        else:\n            return (logits,)\n\n\nclass FlaxMptForCausalLM(FlaxMptPretrainedModel):\n    module_class = FlaxMptForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(\n            batch_size, max_length\n        )\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = jax.lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py b/lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py
--- a/lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py	(date 1703669573199)
@@ -2,7 +2,7 @@
 from flax import linen as nn
 from flax.core import FrozenDict
 from typing import Optional, Union, Tuple
-from transformers import FlaxPreTrainedModel
+
 from jax import numpy as jnp
 import jax
 from jax.sharding import PartitionSpec
@@ -15,6 +15,7 @@
     ACT2FN,
     smart_flash_attention, get_dot_general_by_bits
 )
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 import chex
 
 from .mosaic_configuration import MptConfig
@@ -382,7 +383,7 @@
             return (hidden_states,)
 
 
-class FlaxMptPretrainedModel(FlaxPreTrainedModel):
+class FlaxMptPretrainedModel(EasyDelFlaxPretrainedModel):
     module_class: nn.Module = None
     config_class: MptConfig = MptConfig
 
@@ -483,6 +484,12 @@
 class FlaxMptModel(FlaxMptPretrainedModel):
     module_class = FlaxMptModule
 
+    def get_input_embeddings(self):
+        return self.module.wte
+
+    def set_input_embeddings(self, value):
+        self.module.wte = value
+
 
 class FlaxMptForCausalLMModule(nn.Module):
     config: MptConfig
@@ -538,6 +545,24 @@
 class FlaxMptForCausalLM(FlaxMptPretrainedModel):
     module_class = FlaxMptForCausalLMModule
 
+    def get_input_embeddings(self):
+        return self.module.transformer.wte
+
+    def get_decoder(self):
+        return self.module.transformer
+
+    def set_input_embeddings(self, value):
+        self.module.transformer.wte = value
+
+    def set_decoder(self, decoder):
+        self.module.transformer = decoder
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
     def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):
 
         batch_size, seq_length = input_ids.shape
Index: lib/python/EasyDel/modules/gpt_neo_x/gpt_neo_x_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass GPTNeoXConfig(JaxBaseClassModel):\n    model_type = \"gpt_neox\"\n\n    def __init__(\n            self,\n            vocab_size=50432,\n            hidden_size=6144,\n            num_hidden_layers=44,\n            num_attention_heads=64,\n            intermediate_size=24576,\n            hidden_act=\"gelu\",\n            rotary_pct=0.25,\n            rotary_emb_base=10000,\n            classifier_dropout=0.1,\n            max_position_embeddings=2048,\n            initializer_range=0.02,\n            layer_norm_eps=1e-5,\n            use_cache=True,\n            bos_token_id=0,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            gradient_checkpointing='everything_saveable',\n            use_parallel_residual=True,\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.rotary_pct = rotary_pct\n        self.rotary_emb_base = rotary_emb_base\n        self.classifier_dropout = classifier_dropout\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.use_cache = use_cache\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n\n        self.use_parallel_residual = use_parallel_residual\n        self.from_pt = False\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            **kwargs\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = False):\n        return (\n            ('wte/embedding', PartitionSpec(\"fsdp\", \"dp\")),\n            ('attention/w_qkv/(kernel|bias)', PartitionSpec(\"fsdp\", \"dp\")),\n            ('attention/wo/(kernel|bias)', PartitionSpec(\"fsdp\", \"dp\")),\n            ('mlp/dense_h_to_4h/(kernel|bias)', PartitionSpec(\"fsdp\", \"dp\")),\n            ('mlp/dense_4h_to_h/(kernel|bias)', PartitionSpec(\"dp\", \"fsdp\")),\n\n            ('post_attention_layernorm/(bias|scale)', PartitionSpec(\"fsdp\", \"dp\")),\n            ('input_layernorm/(bias|scale)', PartitionSpec(\"fsdp\", \"dp\")),\n\n            ('transformer/final_layer_norm/(scale|bias)', PartitionSpec(\"dp\", \"fsdp\")),\n            ('lm_head/kernel', PartitionSpec(\"dp\", \"fsdp\")),\n            ('.*', PartitionSpec(None))\n        ) if not fully_fsdp else (\n\n            ('embed_in/embedding', PartitionSpec(\"fsdp\")),\n\n            ('attention/w_qkv/(kernel|bias)', PartitionSpec(\"fsdp\")),\n            ('attention/wo/(kernel|bias)', PartitionSpec(\"fsdp\")),\n            ('mlp/dense_h_to_4h/(kernel|bias)', PartitionSpec(\"fsdp\")),\n            ('mlp/dense_4h_to_h/(kernel|bias)', PartitionSpec(\"fsdp\")),\n\n            ('post_attention_layernorm/(bias|scale)', PartitionSpec(\"fsdp\")),\n            ('input_layernorm/(bias|scale)', PartitionSpec(\"fsdp\")),\n\n            ('transformer/final_layer_norm/(scale|bias)', PartitionSpec(\"fsdp\")),\n            ('lm_head/kernel', PartitionSpec(\"fsdp\")),\n            ('.*', PartitionSpec(None))\n        )\n\n    @staticmethod\n    def get_mesh_names():\n        return \"dp\", \"fsdp\", \"tp\", \"sp\"\n\n    def add_jax_args(\n            self,\n            **kwargs,\n    ):\n        self.from_pt = False\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/gpt_neo_x/gpt_neo_x_configuration.py b/lib/python/EasyDel/modules/gpt_neo_x/gpt_neo_x_configuration.py
--- a/lib/python/EasyDel/modules/gpt_neo_x/gpt_neo_x_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/gpt_neo_x/gpt_neo_x_configuration.py	(date 1703665995425)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class GPTNeoXConfig(JaxBaseClassModel):
+class GPTNeoXConfig(EasyDelPretrainedConfig):
     model_type = "gpt_neox"
 
     def __init__(
Index: lib/python/EasyDel/modules/gpt_neo_x/modelling_gpt_neo_x_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import math\n\nfrom flax import linen as nn\nfrom flax.core import FrozenDict\nfrom typing import Optional, Dict, Union, Tuple\nfrom transformers import FlaxPreTrainedModel\nfrom jax import numpy as jnp\nimport jax\nfrom jax.sharding import PartitionSpec\nfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput\nfrom einops import rearrange\nfrom ..flax_modelling_utils import get_gradient_checkpoint_policy, \\\n    with_sharding_constraint, ACT2FN\nimport chex\nfrom .gpt_neo_x_configuration import GPTNeoXConfig\n\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0,\n                         dtype: jnp.dtype = jnp.bfloat16) -> jnp.ndarray:\n    freqs = 1.0 / (theta ** (jnp.arange(0, dim, 2)[: (dim // 2)].astype(dtype) / dim))\n    t = jnp.arange(end)  # type: ignore\n    freqs = jnp.outer(t, freqs).astype(dtype)\n    sin, cos = jnp.sin(freqs), jnp.cos(freqs)\n    freqs_cis = jnp.complex64(cos + 1j * sin)\n    return jnp.asarray(freqs_cis)\n\n\ndef apply_rotary_emb(\n        xq: jnp.ndarray,\n        xk: jnp.ndarray,\n        freqs_cis: jnp.ndarray,\n        dtype: jnp.dtype = jnp.bfloat16,\n) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    reshape_xq = xq.astype(jnp.float32).reshape(*xq.shape[:-1], -1, 2)\n    reshape_xk = xk.astype(jnp.float32).reshape(*xk.shape[:-1], -1, 2)\n\n    xq_ = jax.lax.complex(reshape_xq[..., 0], reshape_xq[..., 1])\n    xk_ = jax.lax.complex(reshape_xk[..., 0], reshape_xk[..., 1])\n\n    freqs_cis = jnp.reshape(freqs_cis, (*freqs_cis.shape[:2], 1, *freqs_cis.shape[2:]))\n\n    xq_out = xq_ * freqs_cis\n    xq_out = jnp.stack((jnp.real(xq_out), jnp.imag(xq_out)), axis=-1).reshape(*xq_out.shape[:-1], -1)\n\n    xk_out = xk_ * freqs_cis\n    xk_out = jnp.stack((jnp.real(xk_out), jnp.imag(xk_out)), axis=-1).reshape(*xk_out.shape[:-1], -1)\n\n    return xq_out.astype(dtype), xk_out.astype(dtype)\n\n\nclass FlaxGPTNeoXAttention(nn.Module):\n    config: GPTNeoXConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.head_size = self.config.hidden_size // self.config.num_attention_heads\n        self.freq_cis = precompute_freqs_cis(\n            dtype=self.dtype,\n            dim=self.head_size,\n            end=self.config.max_position_embeddings\n        )\n        self.w_qkv = nn.Dense(\n            3 * self.config.hidden_size\n        )\n        self.w_o = nn.Dense(\n            self.config.hidden_size\n        )\n\n        self.factor = jnp.sqrt(jnp.asarray(self.head_size, dtype=jnp.float32))\n        self.bias = nn.make_causal_mask(jnp.ones((1, self.config.max_position_embeddings)))\n\n    def __call__(self,\n                 hidden_states: chex.Array,\n                 attention_mask: chex.Array = None,\n                 ):\n        b, s, d = hidden_states.shape\n        q, k, v = jnp.split(self.w_qkv(hidden_states), indices_or_sections=3, axis=-1)\n        freq = self.freq_cis[:s].reshape(1, s, -1)\n        q = with_sharding_constraint(q, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n        k = with_sharding_constraint(k, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n        v = with_sharding_constraint(v, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n\n        q = rearrange(q, 'b s (h d) -> b s h d', h=self.config.num_attention_heads)\n        k = rearrange(k, 'b s (h d) -> b s h d', h=self.config.num_attention_heads)\n        v = rearrange(v, 'b s (h d) -> b s h d', h=self.config.num_attention_heads)\n        bias = jnp.where(self.bias == 1, 0, jnp.finfo(\n            hidden_states.dtype\n        ).min\n                         )\n        q, k = apply_rotary_emb(q, k, freqs_cis=freq, dtype=self.dtype)\n\n        attn = jnp.einsum(\n            '...qhd,...khd->...hqk', q, k, precision=self.precision\n        ) * self.factor\n        attn = attn + bias[:, :, :s, :s]\n        if attention_mask is not None:\n            attn += attention_mask\n        attn = jax.nn.softmax(attn, axis=-1)\n        attn = with_sharding_constraint(attn, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None))\n        attn = jnp.einsum('...hqk,..khd->qhd', attn, v, precision=self.precision)\n        attn = self.w_o(attn.reshape(b, s, d))\n        return attn\n\n\nclass FlaxGPTNeoXMlp(nn.Module):\n    config: GPTNeoXConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.dense_h_to_4h = nn.Dense(self.config.intermediate_size)\n        self.dense_4h_to_h = nn.Dense(self.config.hidden_size)\n        self.act = ACT2FN[self.config.hidden_act]\n\n    def __call__(self, x):\n        return self.dense_4h_to_h(self.act(self.dense_h_to_4h(x)))\n\n\nclass FlaxGPTNeoXBlock(nn.Module):\n    config: GPTNeoXConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.use_parallel_residual = self.config.use_parallel_residual\n        self.input_layernorm = nn.LayerNorm(\n            epsilon=self.config.layer_norm_eps,\n            dtype=self.dtype\n        )\n        self.post_attention_layernorm = nn.LayerNorm(\n            epsilon=self.config.layer_norm_eps,\n            dtype=self.dtype\n        )\n        self.attention = FlaxGPTNeoXAttention(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.mlp = FlaxGPTNeoXMlp(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(self,\n                 hidden_states: chex.Array,\n                 attention_mask: chex.Array,\n                 ):\n        attn = self.attention(\n            self.input_layernorm(hidden_states),\n            attention_mask=attention_mask\n        )\n\n        if self.use_parallel_residual:\n            mlp = self.mlp(\n                self.post_attention_layernorm(\n                    hidden_states\n                )\n            )\n            hidden_states = mlp + hidden_states + attn\n        else:\n            hidden_states = attn + hidden_states\n            hidden_states = self.mlp(self.post_attention_layernorm(hidden_states)) + hidden_states\n        return hidden_states\n\n\ndef get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'checkpoint_dots': jax.checkpoint_policies.checkpoint_dots,\n        'checkpoint_dots_with_no_batch_dims': jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims,\n    }[name]\n\n\nclass FlaxGPTNeoXCollection(nn.Module):\n    config: GPTNeoXConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        block = FlaxGPTNeoXBlock\n        if self.config.gradient_checkpointing != '':\n            block = nn.remat(\n                block, static_argnums=None,\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing\n                ),\n\n            )\n        self.blocks = [\n            block(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            )\n            for i in range(\n                self.config.num_hidden_layers\n            )\n        ]\n\n    def __call__(self,\n                 hidden_states: chex.Array,\n                 attention_mask: chex.Array,\n\n                 ):\n        for block in self.blocks:\n            hidden_states = block(\n                hidden_states,\n                attention_mask=attention_mask\n            )\n        return hidden_states\n\n\nclass FlaxGPTNeoXModule(nn.Module):\n    config: GPTNeoXConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.embed_in = nn.Embed(self.config.vocab_size, self.config.hidden_size)\n        self.layers = FlaxGPTNeoXCollection(\n            config=self.config,\n            param_dtype=self.param_dtype,\n            dtype=self.dtype,\n            precision=self.precision\n        )\n        self.final_layer_norm = nn.LayerNorm(\n            epsilon=self.config.layer_norm_eps,\n            dtype=self.dtype\n        )\n\n    def __call__(self,\n                 input_ids: jnp.int32 = None,\n                 attention_mask: Optional[chex.Array] = None,\n                 return_dict: Optional[bool] = None,\n                 ):\n        b, s = input_ids.shape\n        hidden_state = self.embed_in(\n            inputs=input_ids\n        )\n        hidden_state = self.final_layer_norm(self.layers(\n            hidden_state=hidden_state,\n            attention_mask=attention_mask\n        ))\n        if return_dict:\n            return FlaxBaseModelOutput(\n                last_hidden_state=hidden_state\n            )\n        else:\n            return hidden_state,\n\n\nclass FlaxGPTNeoXPretrainedModel(FlaxPreTrainedModel):\n    module_class: nn.Module = None\n    config_class = GPTNeoXConfig\n\n    def __init__(self, config, _do_init=False, dtype: jnp.dtype = jnp.float32, param_dtype: jnp.dtype = jnp.float32,\n                 input_shape: Tuple = (1, 12)):\n        module = self.module_class(config=config, dtype=dtype, param_dtype=param_dtype)\n        super().__init__(_do_init=_do_init, module=module, config=config, dtype=dtype, input_shape=input_shape)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> Dict:\n        if params is None:\n            params = self.module.init(\n                rngs=rng,\n                input_ids=jnp.ones(input_shape),\n                attention_mask=jnp.ones(input_shape)\n            )\n        return params['params']\n\n    def __call__(self, input_ids,\n                 attention_mask=None,\n                 params: FrozenDict = None,\n                 add_params_field: bool = False,\n                 return_dict: bool = True):\n        params = {'params': params or self.params} if add_params_field else params or self.params\n        predict = self.module.apply(\n            params,\n            input_ids=jnp.asarray(input_ids, dtype=jnp.int32),\n            attention_mask=jnp.asarray(attention_mask,\n                                       dtype=jnp.int32) if attention_mask is not None else attention_mask,\n            return_dict=return_dict\n        )\n        return predict\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        return {\n            \"attention_mask\": attention_mask,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        return model_kwargs\n\n\nclass FlaxGPTNeoXModel(FlaxGPTNeoXPretrainedModel):\n    module_class = FlaxGPTNeoXModule\n\n\nclass FlaxGPTNeoXForCausalLMModule(nn.Module):\n    config: GPTNeoXConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.transformer = FlaxGPTNeoXModule(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            use_bias=False\n        )\n\n    def __call__(self, input_ids, attention_mask, return_dict: bool = False):\n        pred = self.transformer(input_ids=input_ids, attention_mask=attention_mask, return_dict=True).last_hidden_state\n        return self.lm_head(pred)\n\n\nclass FlaxGPTNeoXForCausalLM(FlaxGPTNeoXPretrainedModel):\n    module_class = FlaxGPTNeoXForCausalLMModule\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/gpt_neo_x/modelling_gpt_neo_x_flax.py b/lib/python/EasyDel/modules/gpt_neo_x/modelling_gpt_neo_x_flax.py
--- a/lib/python/EasyDel/modules/gpt_neo_x/modelling_gpt_neo_x_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/gpt_neo_x/modelling_gpt_neo_x_flax.py	(date 1703669116084)
@@ -13,6 +13,7 @@
     with_sharding_constraint, ACT2FN
 import chex
 from .gpt_neo_x_configuration import GPTNeoXConfig
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 
 
 def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0,
@@ -261,7 +262,7 @@
             return hidden_state,
 
 
-class FlaxGPTNeoXPretrainedModel(FlaxPreTrainedModel):
+class FlaxGPTNeoXPretrainedModel(EasyDelFlaxPretrainedModel):
     module_class: nn.Module = None
     config_class = GPTNeoXConfig
 
@@ -306,6 +307,12 @@
 class FlaxGPTNeoXModel(FlaxGPTNeoXPretrainedModel):
     module_class = FlaxGPTNeoXModule
 
+    def get_input_embeddings(self):
+        return self.module.wte
+
+    def set_input_embeddings(self, value):
+        self.module.wte = value
+
 
 class FlaxGPTNeoXForCausalLMModule(nn.Module):
     config: GPTNeoXConfig
@@ -332,3 +339,21 @@
 
 class FlaxGPTNeoXForCausalLM(FlaxGPTNeoXPretrainedModel):
     module_class = FlaxGPTNeoXForCausalLMModule
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
+    def get_decoder(self):
+        return self.module.transformer
+
+    def get_input_embeddings(self):
+        return self.module.transformer.wte
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
+    def set_decoder(self, decoder):
+        self.module.transformer = decoder
+
+    def set_input_embeddings(self, value):
+        self.module.transformer.wte = value
Index: lib/python/EasyDel/modules/gpt2/gpt2_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass GPT2Config(JaxBaseClassModel):\n    model_type = \"gpt2\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n    attribute_map = {\n        \"hidden_size\": \"n_embd\",\n        \"max_position_embeddings\": \"n_positions\",\n        \"num_attention_heads\": \"n_head\",\n        \"num_hidden_layers\": \"n_layer\",\n    }\n\n    def __init__(\n            self,\n            vocab_size=50257,\n            n_positions=1024,\n            n_embd=768,\n            n_layer=12,\n            n_head=12,\n            n_inner=None,\n            activation_function=\"gelu_new\",\n            resid_pdrop=0.1,\n            embd_pdrop=0.1,\n            attn_pdrop=0.1,\n            layer_norm_epsilon=1e-5,\n            initializer_range=0.02,\n            summary_type=\"cls_index\",\n            summary_use_proj=True,\n            summary_activation=None,\n            summary_proj_to_labels=True,\n            summary_first_dropout=0.1,\n            scale_attn_weights=True,\n            use_cache=True,\n            bos_token_id=50256,\n            eos_token_id=50256,\n            scale_attn_by_inverse_layer_idx=False,\n            reorder_and_upcast_attn=False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_pjit_attention_force: bool = False,\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.n_positions = n_positions\n        self.n_embd = n_embd\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.n_inner = n_inner\n        self.activation_function = activation_function\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n        self.summary_type = summary_type\n        self.summary_use_proj = summary_use_proj\n        self.summary_activation = summary_activation\n        self.summary_first_dropout = summary_first_dropout\n        self.summary_proj_to_labels = summary_proj_to_labels\n        self.scale_attn_weights = scale_attn_weights\n        self.use_cache = use_cache\n        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n        self.reorder_and_upcast_attn = reorder_and_upcast_attn\n\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.gradient_checkpointing = gradient_checkpointing\n        self.bits = bits\n        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n\n    def add_jax_args(\n            self,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            use_pjit_attention_force: bool = False,\n            bits: Optional[int] = None,\n            **kwargs\n    ):\n        args = dict(\n            use_pjit_attention_force=use_pjit_attention_force,\n            gradient_checkpointing=gradient_checkpointing,\n            bits=bits,\n            **kwargs\n        )\n        for k, v in args.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/gpt2/gpt2_configuration.py b/lib/python/EasyDel/modules/gpt2/gpt2_configuration.py
--- a/lib/python/EasyDel/modules/gpt2/gpt2_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/gpt2/gpt2_configuration.py	(date 1703666055304)
@@ -1,11 +1,8 @@
-from typing import Sequence, Optional
-
-from jax.sharding import PartitionSpec
+from typing import Optional
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
-from ..flax_modelling_utils import JaxBaseClassModel
 
-
-class GPT2Config(JaxBaseClassModel):
+class GPT2Config(EasyDelPretrainedConfig):
     model_type = "gpt2"
     keys_to_ignore_at_inference = ["past_key_values"]
     attribute_map = {
@@ -90,4 +87,3 @@
         for k, v in args.items():
             if not hasattr(self, k):
                 setattr(self, k, v)
-
Index: lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import functools\nimport typing\nfrom typing import Sequence\n\nimport fjformer.attention\nimport flax.core\nfrom jax import numpy as jnp, Array, lax\nfrom jax.experimental.shard_map import shard_map\nfrom jax.sharding import PartitionSpec as PS\nimport jax\nfrom flax import linen as nn\nfrom flax.traverse_util import unflatten_dict, flatten_dict\nfrom flax.core import freeze, unfreeze\nfrom typing import Union, Optional, Tuple\nfrom transformers import FlaxPreTrainedModel\nfrom flax.linen import partitioning as nn_partitioning, dot_product_attention_weights\nfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput\n\nfrom ..flax_modelling_utils import (\n    ACT2FN,\n    with_sharding_constraint,\n    get_gradient_checkpoint_policy,\n    repeat_kv_bnsh,\n    apply_rotary_pos_emb,\n    precompute_freq_cis,\n    smart_flash_attention, get_dot_general_by_bits\n)\nimport chex\nfrom .mistral_configuration import MistralConfig\n\nre_mat = nn_partitioning.remat\n\n\ndef matmul_4d_loop(x, y):\n    \"\"\"Computes the matrix product of two 4D arrays x and y using a loop.\"\"\"\n    result = jnp.zeros(*x.shape[:-2] + x.shape[-2] + y.shape[-1])\n    for i in range(x.shape[0]):\n        for j in range(y.shape[1]):\n            for key in range(x.shape[2]):\n                for l in range(y.shape[3]):\n                    result[i, j, key, l] += x[i, j, key, :] * y[key, l, :, :]\n    return result\n\n\ndef _make_sliding_window_causal_mask(\n        input_ids_shape,\n        dtype: jnp.dtype,\n        past_key_values_length: int = 0,\n        sliding_window: int = 4096,\n):\n    \"\"\"\n    Make causal mask used for sliding window attention\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n\n    tensor = jnp.full(\n        (tgt_len, tgt_len),\n        fill_value=1,\n    )\n    mask = jnp.tril(tensor, 0)\n    mask = jnp.triu(mask, -sliding_window)\n    mask = jnp.log(mask).astype(dtype)\n\n    if past_key_values_length > 0:\n        mask = jnp.concatenate(\n            [jnp.zeros((tgt_len, past_key_values_length), dtype=dtype), mask], axis=-1)\n    return mask[None, None, :, :].repeat(bsz, 0)\n\n\nclass MistralRMSNorm(nn.Module):\n    dim: int\n    eps: float = 1e-6\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n\n    def setup(self) -> None:\n        self.weight = self.param(\n            'kernel',\n            nn.initializers.ones,\n            (self.dim,),\n            self.param_dtype,\n        )\n\n    def _norm(self, x: jnp.ndarray) -> jnp.ndarray:\n        return x * jax.lax.rsqrt(jnp.square(x).mean(-1, keepdims=True) + self.eps)\n\n    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n        x = x.astype(jnp.promote_types(self.dtype, jnp.float32))\n        output = self._norm(x).astype(self.dtype)\n        weight = jnp.asarray(self.weight, self.dtype)\n        return output * weight\n\n\nclass FlaxMistralRotaryEmbedding(nn.Module):\n    dtype: jnp.dtype = jnp.float32\n\n    def __call__(self, key, query, freq_cis, position_ids):\n        sin, cos = freq_cis\n\n        sin = sin[position_ids][:, None, :, :]\n        cos = cos[position_ids][:, None, :, :]\n\n        key = apply_rotary_pos_emb(key, sin, cos)\n        query = apply_rotary_pos_emb(query, sin, cos)\n\n        return query.astype(self.dtype), key.astype(self.dtype)\n\n\nclass FlaxMistralMLP(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.gate_proj = dense(self.config.intermediate_size)\n        self.up_proj = dense(self.config.intermediate_size)\n        self.down_proj = dense(self.config.hidden_size)\n        self.act_fn = ACT2FN[self.config.hidden_act]\n\n    def __call__(self, x: chex.Array):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass FlaxMistralAttention(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.q_proj = dense(self.num_heads * self.head_dim)\n        self.k_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.v_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.o_proj = dense(self.hidden_size)\n        self.rotary = FlaxMistralRotaryEmbedding(self.dtype)\n\n    @nn.compact\n    def concatenate_to_cache_(self, query: chex.Array, key: chex.Array, value: chex.Array, attention_mask: chex.Array):\n        is_cache_available = self.has_variable('cache', 'key')\n        key_cache = self.variable(\n            'cache', 'key', jnp.zeros, key.shape, key.dtype)\n        value_cache = self.variable(\n            'cache', 'value', jnp.zeros, key.shape, value.dtype)\n        index_cache = self.variable(\n            'cache', 'index', lambda: jnp.array(0, dtype=jnp.int32))\n        if is_cache_available:\n            *bd, ml, nh, dph = key_cache.value.shape\n            indices = (0,) * len(bd) + (index_cache.value, 0, 0)\n            key = jax.lax.dynamic_update_slice(key_cache.value, key, indices)\n            value = jax.lax.dynamic_update_slice(\n                value_cache.value, value, indices)\n            key_cache.value = key\n            value_cache.value = value\n            num_updated_cache_vector = query.shape[1]\n            index_cache.value = index_cache.value + num_updated_cache_vector\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(ml) < index_cache.value,\n                tuple(bd) + (1, num_updated_cache_vector, ml)\n            )\n            attention_mask = nn.combine_masks(pad_mask, attention_mask)\n        return query, key, value, attention_mask\n\n    @staticmethod\n    def _t(query, key, value):\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def t_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        query = query.reshape(batch_size, sequence_length,\n                              self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length,\n                          self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length,\n                              self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._t(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._t(query, key, value)\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n        The __call__ method takes an input tensor (x) and returns an output tensor (y).\n        In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n        :param self: Refer to the object itself\n        :param hidden_state: chex.Array: Pass in the hidden state of the model\n        :param freq_cis: chex.Array: Create the t_rotary variable\n        :param attention_mask: chex.Array: Mask the attention weights\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param position_ids: chex.Array: Specify the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attention weights\n        :return: A tuple of (out, attn_output)\n\n        \"\"\"\n        batch_size, sequence_length = hidden_state.shape[:2]\n        query, key, value = self.q_proj(hidden_state), self.k_proj(\n            hidden_state), self.v_proj(hidden_state)\n\n        if self.config.use_pjit_attention_force:\n            query = with_sharding_constraint(query, PS(\"fsdp\", \"sp\", None))\n            key = with_sharding_constraint(key, PS(\"fsdp\", \"sp\", None))\n            value = with_sharding_constraint(value, PS(\"fsdp\", \"sp\", None))\n        query, key, value = self.t_rotary(\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            query=query,\n            key=key,\n            value=value,\n            freq_cis=freq_cis,\n            position_ids=position_ids\n        )\n        if self.has_variable('cache', 'key') or init_cache:\n            query, key, value, attention_mask = self.concatenate_to_cache_(\n                query, key, value, attention_mask)\n\n        q_l, k_l = query.shape[1], key.shape[1]\n        if self.has_variable('cache', 'key'):\n            mask_shift: int = self.variables['cache']['index']\n            dl = self.variables['cache']['key'].shape[1]\n            causal_mask = jax.lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, q_l, dl)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :q_l, :k_l]\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(\n                attention_mask, axis=(-3, -2)), causal_mask.shape)\n\n        attention_mask = nn.combine_masks(attention_mask, causal_mask)\n\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(\n                    self.config.num_attention_heads, 1, )\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(\n                    self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(query, rtp_axis),\n                k=jnp.transpose(key, rtp_axis),\n                v=jnp.transpose(value, rtp_axis),\n                q_ps=self.config.q_ps,\n                k_ps=self.config.k_ps,\n                v_ps=self.config.v_ps,\n                b_ps=self.config.b_ps,\n                a_ps=self.config.a_ps,\n                bias=attention_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=q_l,\n                kv_seq_len=k_l,\n                attn_pdrop=self.config.attn_pdrop,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(\n                    self.dtype).min).astype(self.dtype),\n            )\n            if self.config.use_shard_map:\n                attn_weights = shard_map(\n                    functools.partial(\n                        dot_product_attention_weights,\n                        dtype=jnp.promote_types(self.dtype, jnp.float32),\n                        deterministic=deterministic,\n                        dropout_rate=self.config.attn_pdrop,\n                        precision=self.precision,\n                    ),\n                    mesh=self.config.jax_mesh(),\n                    in_specs=(\n                        self.config.q_ps,\n                        self.config.k_ps,\n                        self.config.b_ps\n                    ),\n                    out_specs=PS((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n                    check_rep=False\n                )(\n                    query, key, attention_bias\n                )\n            else:\n                attn_weights = dot_product_attention_weights(\n                    query=query,\n                    key=key,\n                    bias=attention_bias,\n                    dtype=jnp.promote_types(self.dtype, jnp.float32),\n                    deterministic=deterministic,\n                    dropout_rate=self.config.attn_pdrop,\n                    precision=self.precision,\n                )\n\n            if self.config.use_pjit_attention_force:\n                attn_weights = with_sharding_constraint(\n                    attn_weights, PS((\"dp\", \"fsdp\"), \"sp\", \"tp\", None))\n\n            attn_output = jnp.einsum(\n                \"...hqk,...khd->...qhd\", attn_weights, value)\n\n        out = self.o_proj(attn_output.reshape(\n            batch_size, sequence_length, self.hidden_size))\n        outputs = (out, attn_weights) if output_attentions else (out,)\n        return outputs\n\n\nclass FlaxMistralDecoderLayer(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        self.self_attn = FlaxMistralAttention(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.mlp = FlaxMistralMLP(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.input_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attention_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_state (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        :param self: Represent the instance of the class\n        :param hidden_state: chex.Array: Represent the input to the encoder layer\n        :param freq_cis: chex.Array: Pass the frequency information to the attention layer\n        :param attention_mask: chex.Array: Mask out the attention weights for certain positions\n        :param causal_mask: chex.Array: Mask the future tokens\n        :param position_ids: chex.Array: Indicate the position of each token in the sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache for the self-attention layer\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :return: A tuple of hidden_state and attention_output\n\n        \"\"\"\n        residual = hidden_state\n        attention_output = self.self_attn(\n            hidden_state=self.input_layernorm(hidden_state),\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            causal_mask=causal_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions\n        )\n\n        hidden_state = attention_output[0] + residual\n\n        hidden_state = self.mlp(\n            self.post_attention_layernorm(hidden_state)) + hidden_state\n        outputs = (hidden_state,)\n        if output_attentions:\n            outputs += attention_output[1]\n        return outputs\n\n\nclass FlaxMistralPretrainedModel(FlaxPreTrainedModel):\n    config_class = MistralConfig\n    base_model_prefix = 'mistral'\n    module_class: nn.Module = None\n\n    def __init__(self,\n                 config: MistralConfig,\n                 input_shape: Tuple = (1, 1),\n                 seed: int = 0,\n                 dtype: jnp.dtype = jnp.bfloat16,\n                 _do_init: bool = True,\n                 **kwargs\n                 ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape,\n                         seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: flax.core.FrozenDict = None\n    ) -> flax.core.FrozenDict:\n        \"\"\"\n        The init_weights function is used to initialize the weights of a model.\n        It takes in an rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        :param self: Access variables that belong to the class\n        :param rng: jax.random.PRNGKey: Initialize the weights of the model\n        :param input_shape: Tuple: Initialize the input_ids, attention_mask and position_ids\n        :param params: flax.core.FrozenDict: Pass in the parameters of a pre-trained model\n        :return: A frozendict of parameters\n\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rng_s,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rng_s, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask=None,\n            position_ids=None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input sequence to the model\n        :param attention_mask: Mask out the padding tokens\n        :param position_ids: Specify the position of each token in the sequence\n        :param params: dict: Pass in the parameters of the model\n        :param past_key_values: dict: Pass the past key values to the model\n        :param dropout_rng: jax.random.PRNGKey: Pass in a random number generator key to the model\n        :param train: bool: Determine whether to use dropout or not\n        :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n        :param output_hidden_states: Optional[bool]: Determine whether to return the hidden states of all layers\n        :param return_dict: Optional[bool]: Return a dictionary of the outputs\n        :param add_params_field: bool: Add a params field to the inputs dictionary\n        :return: A tuple of (last_hidden_state, past_key_values)\n\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            None,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxMistralDecoratorCollection(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]\n    ] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        block = FlaxMistralDecoderLayer\n        if self.config.gradient_checkpointing != '':\n            block = re_mat(\n                block,\n                static_argnums=(5, 6, 7),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing\n                )\n            )\n        self.layers = [\n            block(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            ) for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False\n    ):\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n        for layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_state,)\n            output = layer(\n                hidden_state,\n                freq_cis,\n                attention_mask,\n                causal_mask,\n                position_ids,\n                deterministic,\n                init_cache,\n                output_attentions\n            )\n            hidden_state = output[0]\n\n            if output_attentions:\n                output_attentions += (output[1],)\n\n        return hidden_state, all_hidden_states, all_attentions\n\n\nclass FlaxMistralModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n\n        self.layers = FlaxMistralDecoratorCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm = MistralRMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n        initial_rope_kwargs = dict(\n            rope_type=\"none\"\n        )\n        if self.config.rope_scaling is not None:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            initial_rope_kwargs = dict(\n                scaling_factor=scaling_factor,\n                rope_type=scaling_type\n            )\n        self.freq_cis = precompute_freq_cis(\n            max_position_embeddings=self.config.max_position_embeddings,\n            dim=self.config.hidden_size // self.config.num_attention_heads,\n            base=self.config.rope_theta,\n            **initial_rope_kwargs\n        )\n        self.causal_mask = nn.make_causal_mask(\n            jnp.ones((1, self.config.c_max_position_embeddings), dtype='i4'))\n\n    def __call__(\n            self,\n            input_ids: Optional[chex.Array] = None,\n            attention_mask: Optional[chex.Array] = None,\n            position_ids: Optional[chex.Array] = None,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n\n    ) -> typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n        \"\"\"\n        The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n        The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n\n        :param self: Represent the instance of the class\n        :param input_ids: chex.Array: Pass in the input ids\n        :param attention_mask: chex.Array: Mask out the attention weights for certain tokens\n        :param position_ids: chex.Array: Determine the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param inputs_embeds: chex.Array: Pass in the embedding of the input_ids\n        :param init_cache: bool: Initialize the cache for the decoder\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :param output_hidden_states: bool: Return all hidden states or just the last one\n        :param return_dict: bool: Return a dictionary of the outputs or not\n        :param : Determine whether the model is in training mode or not\n        :return: A tuple of the hidden states, all hidden states, and attentions\n\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n        if attention_mask.ndim == 2:\n            b, s = attention_mask.shape\n            attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n        outputs = self.layers(\n            hidden_state=inputs_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            freq_cis=self.freq_cis,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            causal_mask=self.causal_mask\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(value for value in outputs if value is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n\n\nclass FlaxMistralModel(FlaxMistralPretrainedModel):\n    module_class = FlaxMistralModule\n\n\nclass FlaxMistralForCausalLMModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model: FlaxMistralModule = FlaxMistralModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"\n            The __call__ function is the main function of a Flax module. It defines how the model will be called,\n            and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n            as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n            the call function: deterministic (whether to use dropout), inputs_embeds (if you want to pass your own embeddings),\n            output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n            :param self: Refer to the object itself\n            :param input_ids: chex.Array: Pass in the input tokens\n            :param attention_mask: chex.Array: Mask out the padding tokens\n            :param position_ids: chex.Array: Specify the position of each token in the sequence\n            :param deterministic: bool: Determine whether to use dropout in the model\n            :param inputs_embeds: chex.Array: Pass in the embeddings of the input tokens\n            :param init_cache: bool: Initialize the cache for the decoder\n            :param output_attentions: bool: Return the attention weights\n            :param output_hidden_states: bool: Return the hidden states of all layers\n            :param return_dict: bool: Return a dictionary of the outputs or just the logits\n            :param : Determine whether to return the logits or not\n            :return: A tuple of (lm_logits, hidden_states, attentions)\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            inputs_embeds=inputs_embeds,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        # lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n\n\nclass FlaxMistralForCausalLM(FlaxMistralPretrainedModel):\n    module_class = FlaxMistralForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = jax.lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    @staticmethod\n    def update_inputs_for_generation(model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py b/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py
--- a/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py	(date 1703669116096)
@@ -12,7 +12,7 @@
 from flax.traverse_util import unflatten_dict, flatten_dict
 from flax.core import freeze, unfreeze
 from typing import Union, Optional, Tuple
-from transformers import FlaxPreTrainedModel
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 from flax.linen import partitioning as nn_partitioning, dot_product_attention_weights
 from transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput
 
@@ -420,8 +420,11 @@
         """
         The __call__ function is the main function of a TransformerEncoderLayer.
         It takes in the following arguments:
-            hidden_state (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.
-            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num
+            hidden_state (chex.Array): The input to the encoder layer, which is also its output after being processed
+            by all sublayers.
+            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector,
+            used for computing self-attention weights and biases in a more efficient manner than using position
+            embeddings or sinusoidal positional encoding vectors would allow for [2].
 
         :param self: Represent the instance of the class
         :param hidden_state: chex.Array: Represent the input to the encoder layer
@@ -457,7 +460,7 @@
         return outputs
 
 
-class FlaxMistralPretrainedModel(FlaxPreTrainedModel):
+class FlaxMistralPretrainedModel(EasyDelFlaxPretrainedModel):
     config_class = MistralConfig
     base_model_prefix = 'mistral'
     module_class: nn.Module = None
@@ -822,6 +825,12 @@
 class FlaxMistralModel(FlaxMistralPretrainedModel):
     module_class = FlaxMistralModule
 
+    def set_input_embeddings(self, value):
+        self.module.embed_tokens = value
+
+    def get_input_embeddings(self):
+        return self.module.embed_tokens
+
 
 class FlaxMistralForCausalLMModule(nn.Module):
     config: MistralConfig
@@ -922,6 +931,24 @@
 class FlaxMistralForCausalLM(FlaxMistralPretrainedModel):
     module_class = FlaxMistralForCausalLMModule
 
+    def set_input_embeddings(self, value):
+        self.module.model.embed_tokens = value
+
+    def get_input_embeddings(self):
+        return self.module.model.embed_tokens
+
+    def set_decoder(self, decoder):
+        self.module.model = decoder
+
+    def get_decoder(self):
+        return self.module.model
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
     def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):
         batch_size, seq_length = input_ids.shape
 
Index: lib/python/EasyDel/modules/gpt_j/gpt_j_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Optional, List, Mapping, Any\n\nfrom jax.sharding import PartitionSpec\nfrom transformers import PreTrainedTokenizer, TensorType, is_torch_available\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.onnx import OnnxConfigWithPast, PatchingSpec\nfrom collections import OrderedDict\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass GPTJConfig(JaxBaseClassModel):\n    model_type = \"gptj\"\n    attribute_map = {\n        \"max_position_embeddings\": \"n_positions\",\n        \"hidden_size\": \"n_embd\",\n        \"num_attention_heads\": \"n_head\",\n        \"num_hidden_layers\": \"n_layer\",\n    }\n\n    def __init__(\n            self,\n            vocab_size: int = 50400,\n            n_positions: int = 2048,\n            n_embd: int = 4096,\n            n_layer: int = 28,\n            n_head: int = 16,\n            rotary_dim: int = 64,\n            n_inner: int = None,\n            activation_function: str = \"gelu_new\",\n            resid_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            attn_pdrop: float = 0.0,\n            layer_norm_epsilon: float = 1e-5,\n            initializer_range: int = 0.02,\n            use_cache: int = True,\n            bos_token_id: int = 50256,\n            eos_token_id: int = 50256,\n            tie_word_embeddings: bool = False,\n            use_pjit_attention_force: bool = False,\n            use_flash_attention: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 2048,\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        self.bits = bits\n        self.vocab_size = vocab_size\n        self.n_positions = n_positions\n        self.n_embd = n_embd\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.n_inner = n_inner\n        self.rotary_dim = rotary_dim\n        self.activation_function = activation_function\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n        self.use_cache = use_cache\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.use_flash_attention = use_flash_attention\n        self.from_pt = False\n        super().__init__(\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs\n        )\n\n    @staticmethod\n    def set_custom_partition(embedding_partition: PartitionSpec,\n                             kvq_partition: PartitionSpec,\n                             o_proj_partition: PartitionSpec,\n                             fc_out_partition: PartitionSpec,\n                             fc_in_partition: PartitionSpec,\n                             fc_lm_head_partition: PartitionSpec,\n                             rest_partitions: PartitionSpec = PartitionSpec(None)\n                             ):\n        return (\n            (\"model/wte/embedding\", embedding_partition),\n\n            (\"attn/(k_proj|v_proj|q_proj)/kernel\", kvq_partition),\n            (\"attn/out_proj/kernel\", o_proj_partition),\n\n            (\"mlp/fc_out/kernel\", fc_out_partition),\n            (\"mlp/fc_out/bias\", fc_out_partition),\n\n            (\"mlp/fc_in/kernel\", fc_in_partition),\n            (\"mlp/fc_in/bias\", fc_in_partition),\n\n            (\"lm_head/kernel\", fc_lm_head_partition),\n            (\"lm_head/bias\", fc_lm_head_partition),\n            ('.*', rest_partitions),\n        )\n\n    @staticmethod\n    def get_partition_rules(just_fsdp: bool = True):\n        if just_fsdp:\n            rules = (\n                (\"model/wte/embedding\", PartitionSpec((\"fsdp\", \"tp\"), )),\n\n                (\"attn/(k_proj|v_proj|q_proj)/kernel\", PartitionSpec((\"fsdp\", \"tp\"), )),\n                (\"attn/out_proj/kernel\", PartitionSpec((\"fsdp\", \"tp\"), )),\n\n                (\"mlp/fc_out/kernel\", PartitionSpec((\"fsdp\", \"tp\"), )),\n                (\"mlp/fc_out/bias\", PartitionSpec((\"fsdp\", \"tp\"), )),\n\n                (\"mlp/fc_in/kernel\", PartitionSpec((\"fsdp\", \"tp\"), )),\n                (\"mlp/fc_in/bias\", PartitionSpec((\"fsdp\", \"tp\"), )),\n\n                (\"lm_head/kernel\", PartitionSpec((\"fsdp\", \"tp\"), )),\n                (\"lm_head/bias\", PartitionSpec((\"fsdp\", \"tp\"), )),\n                ('.*', PartitionSpec(None)),\n            )\n        else:\n            rules = (\n                (\"model/wte/embedding\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"))),\n\n                (\"attn/(k_proj|v_proj|q_proj)/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n                (\"attn/out_proj/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"), )),\n\n                (\"mlp/fc_out/kernel\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n                (\"mlp/fc_out/bias\", PartitionSpec((\"fsdp\", \"sp\"), \"tp\")),\n\n                (\"mlp/fc_in/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"), )),\n                (\"mlp/fc_in/bias\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"), )),\n\n                (\"lm_head/kernel\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"), )),\n                (\"lm_head/bias\", PartitionSpec(\"tp\", (\"fsdp\", \"sp\"), )),\n                ('.*', PartitionSpec(None)),\n            )\n        return rules\n\n    @staticmethod\n    def get_mesh_names():\n        return \"dp\", \"fsdp\", \"tp\", \"sp\"\n\n    def add_jax_args(\n            self,\n            vocab_size: int = 50400,\n            n_positions: int = 2048,\n            n_embd: int = 4096,\n            n_layer: int = 28,\n            n_head: int = 16,\n            rotary_dim: int = 64,\n            n_inner: int = None,\n            activation_function: str = \"gelu_new\",\n            resid_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            attn_pdrop: float = 0.0,\n            layer_norm_epsilon: float = 1e-5,\n            initializer_range: int = 0.02,\n            use_cache: int = True,\n            bos_token_id: int = 50256,\n            eos_token_id: int = 50256,\n            tie_word_embeddings: bool = False,\n            use_pjit_attention_force: bool = False,\n            use_flash_attention: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 2048,\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        basics = dict(\n            bits=bits,\n            vocab_size=vocab_size,\n            n_positions=n_positions,\n            n_embd=n_embd,\n            n_layer=n_layer,\n            n_head=n_head,\n            rotary_dim=rotary_dim,\n            n_inner=n_inner,\n            activation_function=activation_function,\n            resid_pdrop=resid_pdrop,\n            embd_pdrop=embd_pdrop,\n            attn_pdrop=attn_pdrop,\n            layer_norm_epsilon=layer_norm_epsilon,\n            initializer_range=initializer_range,\n            use_cache=use_cache,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            use_pjit_attention_force=use_pjit_attention_force,\n            use_flash_attention=use_flash_attention,\n            flash_attn_query_chunk_size=flash_attn_query_chunk_size,\n            flash_attn_key_chunk_size=flash_attn_key_chunk_size,\n        )\n\n        for k, v in basics.items():\n            if not hasattr(self, k):\n                setattr(self, k, v)\n        self.from_pt = False\n        return self\n\n\nclass GPTJOnnxConfig(OnnxConfigWithPast):\n    def __init__(\n            self,\n            config: PretrainedConfig,\n            task: str = \"default\",\n            patching_specs: List[PatchingSpec] = None,\n            use_past: bool = False,\n    ):\n        super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)\n        if not getattr(self._config, \"pad_token_id\", None):\n            self._config.pad_token_id = 0\n\n    @property\n    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n        common_inputs = OrderedDict({\"input_ids\": {0: \"batch\", 1: \"sequence\"}})\n        if self.use_past:\n            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"past_sequence + sequence\"}\n        else:\n            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"sequence\"}\n\n        return common_inputs\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.n_layer\n\n    @property\n    def num_attention_heads(self) -> int:\n        return self._config.n_head\n\n    def generate_dummy_inputs(\n            self,\n            tokenizer: PreTrainedTokenizer,\n            batch_size: int = -1,\n            seq_length: int = -1,\n            is_pair: bool = False,\n            framework: Optional[TensorType] = None,\n    ) -> Mapping[str, Any]:\n        common_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n            tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair, framework=framework\n        )\n\n        # We need to order the input in the way they appears in the forward()\n        ordered_inputs = OrderedDict({\"input_ids\": common_inputs[\"input_ids\"]})\n\n        # Need to add the past_keys\n        if self.use_past:\n            if not is_torch_available():\n                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n            else:\n                import torch\n\n                batch, seqlen = common_inputs[\"input_ids\"].shape\n                # Not using the same length for past_key_values\n                past_key_values_length = seqlen + 2\n                past_shape = (\n                    batch,\n                    self.num_attention_heads,\n                    past_key_values_length,\n                    self._config.hidden_size // self.num_attention_heads,\n                )\n                ordered_inputs[\"past_key_values\"] = [\n                    (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(self.num_layers)\n                ]\n\n        ordered_inputs[\"attention_mask\"] = common_inputs[\"attention_mask\"]\n        if self.use_past:\n            mask_dtype = ordered_inputs[\"attention_mask\"].dtype\n            ordered_inputs[\"attention_mask\"] = torch.cat(\n                [ordered_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n            )\n\n        return ordered_inputs\n\n    @property\n    def default_onnx_opset(self) -> int:\n        return 13\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/gpt_j/gpt_j_configuration.py b/lib/python/EasyDel/modules/gpt_j/gpt_j_configuration.py
--- a/lib/python/EasyDel/modules/gpt_j/gpt_j_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/gpt_j/gpt_j_configuration.py	(date 1703665995441)
@@ -6,10 +6,10 @@
 from transformers.onnx import OnnxConfigWithPast, PatchingSpec
 from collections import OrderedDict
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class GPTJConfig(JaxBaseClassModel):
+class GPTJConfig(EasyDelPretrainedConfig):
     model_type = "gptj"
     attribute_map = {
         "max_position_embeddings": "n_positions",
Index: lib/python/EasyDel/modules/falcon/falcon_configuration.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Sequence, Optional\n\nfrom jax.sharding import PartitionSpec\n\nfrom ..flax_modelling_utils import JaxBaseClassModel\n\n\nclass FalconConfig(JaxBaseClassModel):\n    model_type = \"falcon\"\n    attribute_map = {\n        \"num_hidden_layers\": \"num_hidden_layers\",\n        \"num_attention_heads\": \"num_attention_heads\",\n    }\n\n    def __init__(\n            self,\n            vocab_size: int = 65024,\n            hidden_size: int = 4544,\n            num_hidden_layers: int = 32,\n            num_attention_heads: int = 71,\n            layer_norm_epsilon: float = 1e-5,\n            initializer_range: float = 0.02,\n            use_cache: bool = True,\n            hidden_dropout: float = 0.0,\n            attention_dropout: float = 0.0,\n            num_kv_heads=None,\n            alibi: bool = False,\n            new_decoder_architecture: bool = False,\n            multi_query: bool = True,\n            parallel_attn: bool = True,\n            bias: bool = False,\n            max_position_embeddings: int = 2048,\n            rope_theta: float = 10000.0,\n            rope_scaling=None,\n            bos_token_id: int = 11,\n            eos_token_id: int = 11,\n            use_pjit_attention_force: bool = False,\n            gradient_checkpointing: str = '',\n            bits: Optional[int] = None,\n            axis_dims: Sequence[int] = (1, -1, 1, 1),\n            axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n            **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        n_embed = kwargs.pop(\"n_embed\", None)\n        self.hidden_size = hidden_size if n_embed is None else n_embed\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.max_position_embeddings = max_position_embeddings\n        self.use_cache = use_cache\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.bos_token_id = bos_token_id\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.eos_token_id = eos_token_id\n        self.multi_query = multi_query\n        self.alibi = alibi\n        self.bias = bias\n        self.gradient_checkpointing = gradient_checkpointing\n        self.parallel_attn = parallel_attn\n        self.num_kv_heads = num_kv_heads\n        self.new_decoder_architecture = new_decoder_architecture\n        self.bits = bits\n        self.from_pt = False\n\n        super().__init__(\n            axis_dims=axis_dims,\n            axis_names=axis_names,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            **kwargs\n        )\n\n    @property\n    def head_dim(self):\n        return self.hidden_size // self.num_attention_heads\n\n    @property\n    def rotary(self):\n        return not self.alibi\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = False):\n        return (\n            ('word_embeddings/embedding', PartitionSpec(\"dp\", \"fsdp\")),\n            ('self_attention/query_key_value/(kernel)', PartitionSpec(\"dp\", \"fsdp\")),\n            ('self_attention/dense/(kernel)', PartitionSpec(\"dp\", \"fsdp\")),\n            ('mlp/dense_4h_to_h/(kernel)', PartitionSpec(\"dp\", \"fsdp\")),\n            ('mlp/dense_h_to_4h/(kernel)', PartitionSpec(\"dp\", \"fsdp\")),\n            ('lm_head/kernel', PartitionSpec(\"dp\", \"fsdp\")),\n            ('transformer/ln_f/bias', PartitionSpec(\"fsdp\")),\n            ('transformer/ln_f/scale', PartitionSpec(\"fsdp\")),\n            ('transformer/post_attention_layernorm/scale', PartitionSpec(\"fsdp\")),\n            ('transformer/post_attention_layernorm/bias', PartitionSpec(\"fsdp\")),\n            ('.*', PartitionSpec(\"fsdp\"))\n        ) if not fully_fsdp else (\n            ('word_embeddings/embedding', PartitionSpec(\"fsdp\")),\n            ('self_attention/query_key_value/(kernel|bias)', PartitionSpec(\"fsdp\")),\n            ('self_attention/dense/(kernel|bias)', PartitionSpec(\"fsdp\")),\n            ('mlp/dense_4h_to_h/(kernel|bias)', PartitionSpec(\"fsdp\")),\n            ('mlp/dense_h_to_4h/(kernel|bias)', PartitionSpec(\"fsdp\")),\n            ('lm_head/kernel', PartitionSpec(\"fsdp\")),\n            ('transformer/ln_f/bias', PartitionSpec(\"fsdp\")),\n            ('transformer/ln_f/scale', PartitionSpec(\"fsdp\")),\n            ('transformer/post_attention_layernorm/scale', PartitionSpec(\"fsdp\")),\n            ('transformer/post_attention_layernorm/bias', PartitionSpec(\"fsdp\")),\n            ('.*', PartitionSpec(\"fsdp\"))\n        )\n\n    @staticmethod\n    def get_mesh_names():\n        return \"dp\", \"fsdp\", \"tp\", \"sp\"\n\n    def add_jax_args(self,\n                     vocab_size: int = 65024,\n                     hidden_size: int = 4544,\n                     num_hidden_layers: int = 32,\n                     num_attention_heads: int = 71,\n                     layer_norm_epsilon: float = 1e-5,\n                     initializer_range: float = 0.02,\n                     use_cache: bool = True,\n                     hidden_dropout: float = 0.0,\n                     attention_dropout: float = 0.0,\n                     num_kv_heads=None,\n                     alibi: bool = False,\n                     new_decoder_architecture: bool = False,\n                     multi_query: bool = True,\n                     parallel_attn: bool = True,\n                     bias: bool = False,\n                     max_position_embeddings: int = 2048,\n                     rope_theta: float = 10000.0,\n                     rope_scaling=None,\n                     bos_token_id: int = 11,\n                     eos_token_id: int = 11,\n                     use_pjit_attention_force: bool = False,\n                     gradient_checkpointing: str = '',\n                     bits: Optional[int] = None,\n                     **kwargs,\n                     ):\n        basics = dict(\n            bits=bits,\n            vocab_size=vocab_size,\n            hidden_size=hidden_size,\n            num_hidden_layers=num_hidden_layers,\n            num_attention_heads=num_attention_heads,\n            layer_norm_epsilon=layer_norm_epsilon,\n            rope_theta=rope_theta,\n            initializer_range=initializer_range,\n            use_cache=use_cache,\n            bos_token_id=bos_token_id,\n            num_kv_heads=num_kv_heads,\n            eos_token_id=eos_token_id,\n            max_position_embeddings=max_position_embeddings,\n            hidden_dropout=hidden_dropout,\n            attention_dropout=attention_dropout,\n            multi_query=multi_query,\n            alibi=alibi,\n            bias=bias,\n            parallel_attn=parallel_attn,\n            rope_scaling=rope_scaling,\n            use_pjit_attention_force=use_pjit_attention_force,\n            gradient_checkpointing=gradient_checkpointing,\n            new_decoder_architecture=new_decoder_architecture,\n            **kwargs\n        )\n        for key_state, value_state in basics.items():\n            if not hasattr(self, key_state):\n                setattr(self, key_state, value_state)\n\n        self.from_pt = False\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/falcon/falcon_configuration.py b/lib/python/EasyDel/modules/falcon/falcon_configuration.py
--- a/lib/python/EasyDel/modules/falcon/falcon_configuration.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/falcon/falcon_configuration.py	(date 1703665995421)
@@ -2,10 +2,10 @@
 
 from jax.sharding import PartitionSpec
 
-from ..flax_modelling_utils import JaxBaseClassModel
+from ..easydel_modelling_utils import EasyDelPretrainedConfig
 
 
-class FalconConfig(JaxBaseClassModel):
+class FalconConfig(EasyDelPretrainedConfig):
     model_type = "falcon"
     attribute_map = {
         "num_hidden_layers": "num_hidden_layers",
Index: lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import math\nfrom flax import linen as nn\nfrom flax.core import FrozenDict, unfreeze\nfrom typing import Optional, Dict, Union, Tuple, Sequence\n\nfrom flax.linen import combine_masks\nfrom transformers import FlaxPreTrainedModel, PretrainedConfig\nfrom jax import numpy as jnp, lax\nimport jax\nfrom jax.sharding import PartitionSpec\nfrom transformers.modeling_flax_outputs import FlaxCausalLMOutput, FlaxBaseModelOutput\nfrom ..flax_modelling_utils import get_gradient_checkpoint_policy, \\\n    with_sharding_constraint, get_dot_general_by_bits\nimport chex\nfrom fjformer.func import transpose\nfrom .falcon_configuration import FalconConfig\n\n\ndef built_bloom_alibi(attention_mask, num_attention_heads):\n    \"\"\"\n    The built_bloom_alibi function is used to create a bloom alibi for the attention mask.\n    The bloom alibi is used in the Bloom Attention layer to ensure that each token has a unique\n    attention vector, even if it's masked out. This ensures that all tokens have an equal chance of being selected as\n    the most important token in the sequence, which helps with training stability and performance.\n\n    :param attention_mask: Mask out the padding tokens in the input sequence\n    :param num_attention_heads: Determine the number of attention heads in the model\n    :return: A tensor of shape (batch_size, num_attention_heads, 1, sequence_length)\n    \n    \"\"\"\n    batch_size, sequence_length = attention_mask.shape\n    cp2 = 2 ** math.floor(math.log2(num_attention_heads))\n    base = jnp.asarray(\n        2 ** (- (2 ** -(math.log2(cp2) - 3))), dtype=jnp.float32\n    )\n    powers = jnp.arange(1, 1 + cp2, dtype=jnp.float32)\n    slops = jnp.power(base, powers)\n    if cp2 != num_attention_heads:\n        extra_base = jnp.asarray(\n            2 ** (-(2 ** -(math.log2(2 * cp2) - 3))), dtype=jnp.float32\n        )\n        num_rem_heads = min(cp2, num_attention_heads - cp2)\n        extra_power = jnp.arange(1, 1 + 2 * num_rem_heads, 2, dtype=jnp.dtype)\n        slops = jnp.concatenate([slops, jnp.power(extra_base, extra_power)], axis=0)\n    arange_tensor = (((jnp.cumsum(attention_mask, axis=-1)) - 1) * attention_mask)[:, jnp.newaxis, :]\n    alibi = slops[..., jnp.newaxis].astype(jnp.bfloat16) * arange_tensor\n    return alibi.reshape(batch_size, num_attention_heads, 1, sequence_length)\n\n\ndef precompute_falcon_freq_cis(max_position_embedding: int, head_dim: int, theta: float = 10000):\n    \"\"\"\n    The precompute_falcon_freq_cis function is used to precompute the sinusoidal frequencies for the FALCON model.\n    The function takes in three arguments: max_position_embedding, head_dim, and theta. The first two are self-explanatory;\n    the third is a hyperparameter that controls how quickly the frequency increases with position (i.e., how many times\n    higher it will be at position i than at position 0). The default value of 10000 was chosen because it worked well on\n    the tasks we tested.\n\n    :param max_position_embedding: int: Set the maximum length of the sequence\n    :param head_dim: int: Determine the size of the positional embedding\n    :param theta: float: Adjust the frequency of the sinusoid\n    :return: A tuple of two arrays\n    \n    \"\"\"\n    inv_freq_cis = 1.0 / (theta ** (jnp.arange(0, head_dim, 2, dtype=jnp.float32) / head_dim))\n    freq = jnp.einsum(\"i , j -> i j\", jnp.arange(max_position_embedding), inv_freq_cis).astype(\"float32\")\n\n    embed = jnp.concatenate((freq, freq), axis=-1)\n    return jnp.sin(embed)[:, :], jnp.cos(embed)[:, :]\n\n\ndef _rotate_half(x):\n    \"\"\"\n    The _rotate_half function takes a 1D array and rotates it by half its length.\n    For example, if the input is [0, 1, 2, 3], then the output will be [-2,-3,-0,-4].\n    This function is used to rotate the Fourier transform of an image so that its zero-frequency component\n    is in the center of the spectrum.\n\n    :param x: Specify the input array\n    :return: The negative of the second half of x concatenated with the first half\n    \n    \"\"\"\n    return jnp.concatenate((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), axis=-1)\n\n\ndef apply_rotary_pos_embedding(tensor, sin_, cos_):\n    \"\"\"\n    The apply_rotary_pos_embedding function applies a rotary positional embedding to the input tensor.\n\n    :param tensor: Pass in the tensor that we want to apply the positional embedding to\n    :param sin_: Rotate the tensor by half of its length\n    :param cos_: Multiply the tensor and cosine of the angle\n    :return: A tensor with the same shape as its input,\n    \n    \"\"\"\n    return (tensor * cos_) + (_rotate_half(tensor) * sin_)\n\n\ndef dropout_add(linen_drop: nn.Dropout, x: chex.Array, residual: chex.Array, deterministic: bool) -> chex.Array:\n    \"\"\"\n    The dropout_add function is a helper function that adds the residual to the output of\n    the dropout layer. This is necessary because we want to use deterministic=True when\n    we are evaluating our model, but we still need to add in the residual. The reason for this\n    is that during training, we have two paths through our network: one with dropout and one without.\n    The path without dropout (residual) allows us to backpropagate gradients through both paths at once.\n\n    :param linen_drop: nn.Dropout: Specify the dropout layer\n    :param x: chex.Array: Pass in the input to the dropout layer\n    :param residual: chex.Array: Add the residual to the output of dropout_add\n    :param deterministic: bool: Determine whether the dropout layer is active or not\n    :return: A tensor that is the sum of the residual and a dropout layer\n    \n    \"\"\"\n    out = linen_drop(inputs=x, deterministic=deterministic)\n    out = residual + out\n    return out\n\n\nclass FlaxFalconRotaryEmbedding(nn.Module):\n    dtype: jnp.dtype = jnp.float32\n\n    def __call__(self, key, query, freq_cis, position_ids):\n        sin, cos = freq_cis\n\n        sin = sin[position_ids][:, :]\n        cos = cos[position_ids][:, :]\n\n        _, sequence_length, _ = query.shape\n\n        # query_expansion_factor = int(query.shape[0] / cos.shape[0])\n        # key_expansion_factor = int(key.shape[0] / cos.shape[0])\n\n        query_expansion_factor = 1\n        key_expansion_factor = 1\n\n        if query_expansion_factor > 1:\n            query_cos = jnp.tile(cos, (query_expansion_factor,))\n            query_sin = jnp.tile(sin, (query_expansion_factor,))\n        else:\n            query_cos, query_sin = cos, sin\n\n        if key_expansion_factor > 1:\n            if key_expansion_factor != query_expansion_factor:\n                key_cos = jnp.tile(cos, (key_expansion_factor,))\n                key_sin = jnp.tile(sin, (key_expansion_factor,))\n            else:\n                key_cos, key_sin = query_cos, query_sin\n        else:\n            key_cos, key_sin = cos, sin\n\n        query = apply_rotary_pos_embedding(query, query_sin, query_cos)\n        key = apply_rotary_pos_embedding(key, key_sin, key_cos)\n        return query.astype(self.dtype), key.astype(self.dtype)\n\n\nclass FlaxFalconAttention(nn.Module):\n    config: FalconConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        head_dim = self.config.hidden_size // self.config.num_attention_heads\n\n        self.query_key_value = nn.Dense(\n            features=3 * self.config.hidden_size if not self.config.multi_query else (\n                    self.config.hidden_size + 2 * head_dim),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.bias,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.inv_norm_factor = 1 / math.sqrt(head_dim)\n        self.dense = nn.Dense(\n            features=self.config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.bias,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.head_dim = head_dim\n        self.maybe_rotary = FlaxFalconRotaryEmbedding(self.dtype) if not self.config.alibi else lambda q, k, a, s: (\n            q, k)\n        assert self.head_dim * self.config.num_attention_heads == self.config.hidden_size\n        if self.config.num_kv_heads is not None:\n\n            self.num_kv_heads = self.config.num_kv_heads if (\n                    self.config.new_decoder_architecture or not self.config.multi_query) else 1\n        else:\n            self.num_kv_heads = self.config.num_attention_heads\n        self.num_heads = self.config.num_attention_heads\n\n    @nn.compact\n    def _concatenate_to_cache(self, key: chex.Array, value: chex.Array, query: chex.Array, attention_mask: chex.Array):\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (int(cur_index), 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    @staticmethod\n    def _t(query, key, value):\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_maybe_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._t(query, key, value)\n        query, key = self.rotary(position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        return self._t(query, key, value)\n\n    def split_head(self, qkv: chex.Array):\n        batch_size, sequence_length, _ = qkv.shape\n        if self.config.new_decoder_architecture:\n            batch, sequence_length, _ = qkv.shape\n            qkv = qkv.reshape(batch, sequence_length, -1, self.num_heads // self.num_kv_heads + 2, self.head_dim)\n            query_state = qkv[:, :, :, :-2]\n            key_state = qkv[:, :, :, [-2]]\n            value_state = qkv[:, :, :, [-1]]\n            key_state = jnp.broadcast_to(key_state, query_state.shape)\n            value_state = jnp.broadcast_to(value_state, query_state.shape)\n\n            query_state, key_state, value_state = [x.reshape(x.shape[:-2] + (x.shape[-2] * x.shape[-1],)) for x in\n                                                   (query_state, key_state, value_state)]\n            if self.config.use_pjit_attention_force:\n                query_state = with_sharding_constraint(query_state, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n                key_state = with_sharding_constraint(key_state, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n                value_state = with_sharding_constraint(value_state, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            return query_state, key_state, value_state\n        if self.config.multi_query:\n            qkv = qkv.reshape(\n                batch_size, sequence_length, self.config.num_attention_heads + 2, -1\n            )\n            query_state, key_state, value_state = qkv[..., :-2, :], qkv[..., [-2], :], qkv[..., [-1], :]\n\n        else:\n            query_state, key_state, value_state = jnp.split(qkv, 3, -1)\n\n        if self.config.use_pjit_attention_force:\n            query_state = with_sharding_constraint(query_state, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            key_state = with_sharding_constraint(key_state, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            value_state = with_sharding_constraint(value_state, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n        return query_state, key_state, value_state\n\n    def _merge_heads(self, x: chex.Array) -> chex.Array:\n\n        batch_size_and_num_heads, seq_length, _ = x.shape\n        batch_size = batch_size_and_num_heads // self.num_heads\n        x = x.reshape(batch_size, self.config.num_attention_heads, seq_length, self.head_dim)\n\n        x = x.transpose(0, 2, 1, 3)\n        return x.reshape(batch_size, seq_length, self.config.num_attention_heads * self.head_dim)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array = None,\n            alibi: chex.Array = None,\n            freq_cis: Tuple[chex.Array, chex.Array] = None,\n            output_attentions: bool = False,\n    ):\n        batch_size, sequence_length, _ = hidden_states.shape\n        num_kv_heads = self.num_kv_heads\n        query_layer, key_layer, value_layer = self.split_head(self.query_key_value(hidden_states))\n        query_layer = transpose(\n            query_layer, 1, 2\n        ).reshape(\n            batch_size * self.config.num_attention_heads,\n            sequence_length,\n            self.head_dim\n        )\n        key_layer = transpose(\n            key_layer, 1, 2\n        ).reshape(\n            batch_size * num_kv_heads,\n            sequence_length,\n            self.head_dim,\n        )\n        value_layer = transpose(\n            value_layer, 1, 2\n        ).reshape(\n            batch_size * num_kv_heads,\n            sequence_length,\n            self.head_dim\n        )\n        kv_length = key_layer.shape[1]\n        if not self.config.alibi:\n            query_layer, key_layer = self.maybe_rotary(\n                query_layer,\n                key_layer,\n                freq_cis,\n                position_ids\n            )\n\n        float_min = jnp.finfo(query_layer.dtype).min\n        attention_bias = lax.select(\n            attention_mask > 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, float_min).astype(self.dtype),\n        )\n\n        query_layer_ = query_layer.reshape(batch_size, self.config.num_attention_heads, -1, self.head_dim)\n        key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n        value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n\n        dtype = jnp.promote_types(key_layer_.dtype, jnp.float32)\n\n        query_layer_, key_layer_, value_layer_, attention_bias = map(lambda x: x.astype(dtype=dtype), (\n            query_layer_, key_layer_, value_layer_, attention_bias))\n\n        attention_scores = jax.lax.batch_matmul(query_layer_, transpose(key_layer_, len(key_layer_.shape) - 2,\n                                                                        len(key_layer_.shape) - 1))\n        if alibi is None:\n\n            attention_scores /= math.sqrt(self.head_dim)\n\n            attention_scores = jax.nn.softmax(\n                attention_scores + attention_bias, axis=-1\n            )\n            attn_output = jax.lax.batch_matmul(attention_scores, value_layer_)\n            attn_output = attn_output.reshape(batch_size, self.num_heads, sequence_length, self.head_dim)\n            attn_output = transpose(attn_output, 2, 1)\n            attn_output = attn_output.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n\n            output_tensor = self.dense(attn_output)\n\n            if output_attentions:\n                return output_tensor, attention_scores\n            else:\n                return output_tensor,\n        else:\n\n            attention_scores = attention_scores.reshape(batch_size, self.num_heads, sequence_length, kv_length)\n            attention_scores = attention_scores + alibi.reshape(batch_size, self.num_heads, 1, -1)\n            attention_scores *= self.inv_norm_factor\n            attention_scores = jax.nn.softmax(\n                attention_scores + attention_bias, axis=-1\n            )\n            attention_scores = attention_scores.reshape(batch_size, self.num_heads, sequence_length, kv_length)\n\n            # matmul: [batch_size * num_heads, q_length, head_dim]\n\n            attn_output = jax.lax.batch_matmul(attention_scores, value_layer_)\n            attn_output = attn_output.reshape((attn_output.shape[1] * attn_output.shape[0],) + attn_output.shape[2:])\n            attn_output = self._merge_heads(attn_output)\n\n            output_tensor = self.dense(attn_output)\n\n            if output_attentions:\n                return output_tensor, attention_scores\n            else:\n                return output_tensor,\n\n\nclass FlaxFalconMlp(nn.Module):\n    config: FalconConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.dense_h_to_4h = nn.Dense(\n            features=self.config.hidden_size * 4,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.bias,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.dense_4h_to_h = nn.Dense(\n            features=self.config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.bias,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(self, x: chex.Array, deterministic: bool = True):\n        return self.dense_4h_to_h(nn.gelu(self.dense_h_to_4h(x)))\n\n\nclass FlaxFalconBlock(nn.Module):\n    config: FalconConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        config = self.config\n        self.input_layernorm = nn.LayerNorm(epsilon=config.layer_norm_epsilon,\n                                            dtype=self.dtype)\n        if not config.parallel_attn:\n            self.post_attention_layernorm = nn.LayerNorm(epsilon=config.layer_norm_epsilon,\n                                                         dtype=self.dtype)\n        if config.new_decoder_architecture:\n            self.ln_attn = nn.LayerNorm(epsilon=config.layer_norm_epsilon,\n                                        dtype=self.dtype)\n            self.ln_mlp = nn.LayerNorm(epsilon=config.layer_norm_epsilon,\n                                       dtype=self.dtype)\n        self.mlp = FlaxFalconMlp(\n            config=config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.self_attention = FlaxFalconAttention(\n            config=config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n        self.dropout = nn.Dropout(self.config.attention_dropout)\n        self.dropout_mlp = nn.Dropout(self.config.hidden_dropout)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            alibi: chex.Array,\n            attention_mask: chex.Array,\n            freq_cis: Tuple[chex.Array, chex.Array],\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            output_attentions: bool = False,\n            deterministic: bool = True\n    ):\n        residual = hidden_states\n        mlp_layernorm_out = None\n        if self.config.new_decoder_architecture:\n            attention_layernorm_out = self.ln_attn(hidden_states)\n            mlp_layernorm_out = self.ln_mlp(hidden_states)\n        else:\n            attention_layernorm_out = self.input_layernorm(hidden_states)\n\n        # Self attention.\n        attn_outputs = self.self_attention(\n            hidden_states=attention_layernorm_out,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=causal_mask,\n            alibi=alibi,\n            freq_cis=freq_cis,\n            output_attentions=output_attentions,\n        )\n\n        attention_output = attn_outputs[0]\n\n        if not self.config.new_decoder_architecture:\n            if self.config.parallel_attn:\n                mlp_layernorm_out = attention_layernorm_out\n            else:\n                residual = dropout_add(\n                    linen_drop=self.dropout,\n                    x=attention_output,\n                    residual=residual,\n                    deterministic=deterministic\n                )\n                mlp_layernorm_out = self.post_attention_layernorm(residual)\n\n        outputs = attn_outputs[1:]\n\n        mlp_output = self.mlp(mlp_layernorm_out, deterministic=deterministic)\n\n        if self.config.new_decoder_architecture or self.config.parallel_attn:\n            mlp_output += attention_output\n\n        output = dropout_add(\n            linen_drop=self.dropout_mlp,\n            x=mlp_output,\n            residual=residual,\n            deterministic=deterministic\n\n        )\n\n        return output, outputs\n\n\nclass FlaxFalconCollection(nn.Module):\n    config: FalconConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        # hidden_states: chex.Array,\n        # alibi: chex.Array,\n        # attention_mask: chex.Array,\n        # freq_cis: Tuple[chex.Array, chex.Array],\n        # position_ids: chex.Array,\n        # causal_mask: chex.Array,\n        # output_attentions: bool = False,\n        # deterministic: bool = True\n\n        block = FlaxFalconBlock\n        if self.config.gradient_checkpointing != '':\n            block = nn.remat(\n                block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing),\n                static_argnums=(-2, -1)\n            )\n        self.layers = [\n            block(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            )\n            for i in range(\n                self.config.num_hidden_layers\n            )\n        ]\n\n    def __call__(self,\n                 hidden_states: chex.Array,\n                 attention_mask: chex.Array,\n                 alibi: chex.Array,\n                 freq_cis: Tuple[chex.Array, chex.Array],\n                 position_ids: chex.Array,\n                 causal_mask: chex.Array,\n                 output_attentions: bool = False,\n                 deterministic: bool = True\n                 ):\n        for layer in self.layers:\n            out = layer(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                alibi=alibi,\n                freq_cis=freq_cis,\n                position_ids=position_ids,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n                output_attentions=output_attentions\n            )\n            hidden_states = out[0]\n        return hidden_states\n\n\nclass FlaxFalconModule(nn.Module):\n    config: FalconConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.word_embeddings = nn.Embed(\n            num_embeddings=self.config.vocab_size,\n            features=self.config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.h = FlaxFalconCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.ln_f = nn.LayerNorm(dtype=self.dtype, param_dtype=self.param_dtype, epsilon=self.config.layer_norm_epsilon)\n        self.causal_mask = nn.attention.make_causal_mask(jnp.ones((1, self.config.max_position_embeddings)))\n        if not self.config.alibi:\n            self.freq_cis: Tuple[chex.Array, chex.Array] = precompute_falcon_freq_cis(\n                max_position_embedding=self.config.max_position_embeddings,\n                head_dim=self.config.hidden_size // self.config.num_attention_heads\n            )\n        else:\n            self.freq_cis = None\n\n    def __call__(self,\n                 input_ids: chex.Array,\n                 attention_mask: Optional[chex.Array] = None,\n                 position_ids: Optional[chex.Array] = None,\n                 output_attentions: bool = False,\n                 deterministic: bool = True,\n                 use_cache: Optional[bool] = None,\n                 return_dict: Optional[bool] = False\n                 ):\n        batch, sequence_length = input_ids.shape\n        if position_ids is None:\n            position_ids = jnp.arange(0, sequence_length).reshape(1, -1)\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch, sequence_length))\n\n        hidden_states = self.word_embeddings(\n            inputs=input_ids.astype(jnp.int32)\n        )\n        alibi = None\n        if self.config.alibi:\n            alibi = built_bloom_alibi(attention_mask, self.config.num_attention_heads).astype(hidden_states.dtype)\n\n        if attention_mask.ndim == 2:\n            attention_mask = attention_mask[:, jnp.newaxis, jnp.newaxis, :]\n\n        out_layers = self.h(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            alibi=alibi,\n            freq_cis=self.freq_cis,\n            causal_mask=self.causal_mask,\n            output_attentions=output_attentions,\n            deterministic=deterministic\n        )\n\n        out = out_layers[0]\n        outputs = out_layers[1:]\n        output = self.ln_f(out)\n\n        if return_dict:\n            if output_attentions:\n                return FlaxBaseModelOutput(\n                    last_hidden_state=output,\n                    attentions=out_layers[1]\n                )\n            else:\n                return FlaxBaseModelOutput(\n                    last_hidden_state=output,\n                )\n        else:\n            return output, outputs\n\n\nclass FlaxFalconPretrainedModel(FlaxPreTrainedModel):\n    module_class: nn.Module = None\n    config_class = FalconConfig\n\n    def __init__(self, config,\n                 _do_init=False,\n                 dtype: jnp.dtype = jnp.float32,\n                 param_dtype: jnp.dtype = jnp.float32,\n                 input_shape: Tuple = (1, 1024),\n                 precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n                 ):\n        module = self.module_class(config=config, dtype=dtype, param_dtype=param_dtype, precision=precision)\n        super().__init__(_do_init=_do_init, module=module, config=config, dtype=dtype, input_shape=input_shape)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> Dict:\n        if params is None:\n            params = self.module.init(\n                rngs=rng,\n                input_ids=jnp.ones(input_shape),\n                attention_mask=jnp.ones(input_shape)\n            )\n        return params['params']\n\n    def __call__(self,\n                 input_ids: chex.Array,\n                 attention_mask: Optional[chex.Array] = None,\n                 position_ids: Optional[chex.Array] = None,\n                 past_key_values: Optional[nn.Module] = None,\n                 output_attentions: bool = False,\n                 deterministic: bool = True,\n                 use_cache: Optional[bool] = None,\n                 return_dict: Optional[bool] = False,\n                 params: FrozenDict = None,\n                 add_params_field: bool = False,\n                 ):\n        input_ids = jnp.asarray(input_ids, dtype=jnp.int32)\n        inputs = {'params': params or self.params} if add_params_field else params or self.params\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(input_ids.shape[1])[None, :],\n                                            (input_ids.shape[0], input_ids.shape[1]))\n        rngs = {}\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n        if attention_mask is None:\n            attention_mask = jnp.ones((input_ids.shape[0], input_ids.shape[1]))\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            output_attentions,\n            deterministic,\n            use_cache,\n            return_dict,\n            mutable=mutable,\n            rngs=rngs\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n        return outputs\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    @staticmethod\n    def update_inputs_for_generation(model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n\n\nclass FlaxFalconModel(FlaxFalconPretrainedModel):\n    module_class = FlaxFalconModule\n\n\nclass FlaxFalconForCausalLMModule(nn.Module):\n    config: FalconConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.transformer = FlaxFalconModule(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            use_bias=False,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(self,\n                 input_ids: chex.Array,\n                 attention_mask: Optional[chex.Array] = None,\n                 position_ids: Optional[chex.Array] = None,\n                 output_attentions: bool = False,\n                 deterministic: bool = True,\n                 use_cache: Optional[bool] = None,\n                 return_dict: Optional[bool] = False\n                 ):\n        transformer_output = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            return_dict=return_dict\n        )\n        if return_dict:\n            hidden_state = transformer_output.last_hidden_state\n        else:\n            hidden_state = transformer_output[0]\n        output = self.lm_head(hidden_state)\n        if return_dict:\n            if output_attentions:\n                return FlaxCausalLMOutput(\n                    logits=output,\n                    attentions=transformer_output.attentions\n                )\n            else:\n                return FlaxCausalLMOutput(\n                    logits=output,\n                )\n        else:\n            return (output, transformer_output[1]) if output_attentions else (output,)\n\n\nclass FlaxFalconForCausalLM(FlaxFalconPretrainedModel):\n    module_class = FlaxFalconForCausalLMModule\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py b/lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py
--- a/lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py	(date 1703667945155)
@@ -1,10 +1,9 @@
 import math
 from flax import linen as nn
 from flax.core import FrozenDict, unfreeze
-from typing import Optional, Dict, Union, Tuple, Sequence
+from typing import Optional, Dict, Union, Tuple
 
 from flax.linen import combine_masks
-from transformers import FlaxPreTrainedModel, PretrainedConfig
 from jax import numpy as jnp, lax
 import jax
 from jax.sharding import PartitionSpec
@@ -14,6 +13,7 @@
 import chex
 from fjformer.func import transpose
 from .falcon_configuration import FalconConfig
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 
 
 def built_bloom_alibi(attention_mask, num_attention_heads):
@@ -637,7 +637,7 @@
             return output, outputs
 
 
-class FlaxFalconPretrainedModel(FlaxPreTrainedModel):
+class FlaxFalconPretrainedModel(EasyDelFlaxPretrainedModel):
     module_class: nn.Module = None
     config_class = FalconConfig
 
@@ -752,6 +752,12 @@
 class FlaxFalconModel(FlaxFalconPretrainedModel):
     module_class = FlaxFalconModule
 
+    def get_input_embeddings(self):
+        return self.module.word_embeddings
+
+    def set_input_embeddings(self, value):
+        self.module.word_embeddings = value
+
 
 class FlaxFalconForCausalLMModule(nn.Module):
     config: FalconConfig
@@ -811,3 +817,21 @@
 
 class FlaxFalconForCausalLM(FlaxFalconPretrainedModel):
     module_class = FlaxFalconForCausalLMModule
+
+    def get_decoder(self):
+        return self.module.transformer
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
+    def get_input_embeddings(self):
+        return self.module.transformer.word_embeddings
+
+    def set_input_embeddings(self, value):
+        self.module.transformer.word_embeddings = value
+
+    def set_decoder(self, decoder):
+        self.module.transformer = decoder
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
Index: lib/python/EasyDel/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from .serve.torch_serve import (\n    PyTorchServer as PyTorchServer,\n    PytorchServerConfig as PytorchServerConfig\n)\nfrom .serve.jax_serve import (\n    JAXServer as JAXServer,\n    JAXServerConfig as JAXServerConfig\n)\nfrom .modules.llama import (\n    LlamaConfig as LlamaConfig,\n    FlaxLlamaForCausalLM as FlaxLlamaForCausalLM,\n    FlaxLlamaModel as FlaxLlamaModel\n)\nfrom .modules.gpt_j import (\n    GPTJConfig as GPTJConfig,\n    FlaxGPTJForCausalLM as FlaxGPTJForCausalLM,\n    FlaxGPTJModel as FlaxGPTJModel,\n)\nfrom .modules.t5 import (\n    T5Config as T5Config,\n    FlaxT5ForConditionalGeneration as FlaxT5ForConditionalGeneration,\n    FlaxT5Model as FlaxT5Model\n)\nfrom .modules.falcon import (\n    FalconConfig as FalconConfig,\n    FlaxFalconModel as FlaxFalconModel,\n    FlaxFalconForCausalLM as FlaxFalconForCausalLM\n)\nfrom .modules.opt import (\n    OPTConfig as OPTConfig,\n    FlaxOPTForCausalLM as FlaxOPTForCausalLM,\n    FlaxOPTModel as FlaxOPTModel\n)\nfrom .modules.mistral import (\n    MistralConfig as MistralConfig,\n    FlaxMistralForCausalLM as FlaxMistralForCausalLM,\n    FlaxMistralModel as FlaxMistralModel\n)\nfrom .modules.palm import (\n    FlaxPalmModel as FlaxPalmModel,\n    PalmConfig as PalmConfig,\n    FlaxPalmForCausalLM as FlaxPalmForCausalLM\n)\n\nfrom .modules.mosaic_mpt import (\n    MptConfig as MptConfig,\n    FlaxMptForCausalLM as FlaxMptForCausalLM,\n    FlaxMptModel as FlaxMptModel\n)\n\nfrom .modules.gpt_neo_x import (\n    GPTNeoXConfig as GPTNeoXConfig,\n    FlaxGPTNeoXModel as FlaxGPTNeoXModel,\n    FlaxGPTNeoXForCausalLM as FlaxGPTNeoXForCausalLM\n)\n\nfrom .modules.lucid_transformer import (\n    FlaxLTModel as FlaxLTModel,\n    FlaxLTConfig as FlaxLTConfig,\n    FlaxLTForCausalLM as FlaxLTForCausalLM\n)\n\nfrom .modules.gpt2 import (\n    # GPT2 code is from huggingface but in the version of huggingface they don't support gradient checkpointing\n    # and pjit attention force\n    GPT2Config as GPT2Config,\n    FlaxGPT2LMHeadModel as FlaxGPT2LMHeadModel,\n    FlaxGPT2Model as FlaxGPT2Model\n)\n\nfrom .modules.mixtral import (\n    FlaxMixtralForCausalLM as FlaxMixtralForCausalLM,\n    FlaxMixtralModel as FlaxMixtralModel,\n    MixtralConfig as MixtralConfig\n)\n\nfrom .modules.auto_models import (\n    AutoEasyDelModelForCausalLM as AutoEasyDelModelForCausalLM,\n    get_modules_by_type as get_modules_by_type\n)\n\nfrom .utils.utils import (\n    get_mesh as get_mesh,\n    names_in_mesh as names_in_mesh,\n    get_names_from_partition_spec as get_names_from_partition_spec,\n    make_shard_and_gather_fns as make_shard_and_gather_fns,\n    with_sharding_constraint as with_sharding_constraint,\n    RNG as RNG\n)\n\nfrom .trainer import (\n    CausalLanguageModelTrainer,\n    TrainArguments,\n    create_casual_language_model_evaluation_step,\n    create_casual_language_model_train_step,\n)\n\nfrom .linen import (\n    from_8bit as from_8bit,\n    Dense8Bit as Dense8Bit,\n    array_from_8bit as array_from_8bit,\n    array_to_bit8 as array_to_bit8,\n    to_8bit as to_8bit\n)\nfrom .smi import (\n    run as run,\n    initialise_tracking as initialise_tracking,\n    get_mem as get_mem\n)\n\nfrom .transform.llama import (\n    llama_from_pretrained as llama_from_pretrained,\n    llama_convert_flax_to_pt as llama_convert_flax_to_pt,\n    llama_convert_hf_to_flax_load as llama_convert_hf_to_flax_load,\n    llama_convert_hf_to_flax as llama_convert_hf_to_flax,\n    llama_easydel_to_hf as llama_easydel_to_hf\n)\nfrom .transform.mpt import (\n    mpt_convert_flax_to_pt_1b as mpt_convert_flax_to_pt_1b,\n    mpt_convert_pt_to_flax_1b as mpt_convert_pt_to_flax_1b,\n    mpt_convert_pt_to_flax_7b as mpt_convert_pt_to_flax_7b,\n    mpt_convert_flax_to_pt_7b as mpt_convert_flax_to_pt_7b,\n    mpt_from_pretrained as mpt_from_pretrained\n)\n\nfrom .transform.falcon import (\n    falcon_convert_pt_to_flax_7b as falcon_convert_pt_to_flax_7b,\n    falcon_convert_flax_to_pt_7b as falcon_convert_flax_to_pt_7b,\n    falcon_from_pretrained as falcon_from_pretrained,\n    falcon_convert_hf_to_flax as falcon_convert_hf_to_flax,\n    falcon_easydel_to_hf as falcon_easydel_to_hf\n)\nfrom .transform.mistral import (\n    mistral_convert_hf_to_flax as mistral_convert_hf_to_flax,\n    mistral_convert_hf_to_flax_load as mistral_convert_hf_to_flax_load,\n    mistral_convert_flax_to_pt as mistral_convert_flax_to_pt,\n    mistral_from_pretrained as mistral_from_pretrained,\n    mistral_convert_pt_to_flax as mistral_convert_pt_to_flax,\n    mistral_easydel_to_hf as mistral_easydel_to_hf\n)\n\nfrom .transform.easydel_transform import (\n    huggingface_to_easydel as huggingface_to_easydel\n)\nfrom .etils import (\n    EasyDelOptimizers,\n    EasyDelSchedulers,\n    EasyDelGradientCheckPointers\n)\n\n__version__ = \"0.0.41\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/__init__.py b/lib/python/EasyDel/__init__.py
--- a/lib/python/EasyDel/__init__.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/__init__.py	(date 1703671032661)
@@ -9,6 +9,7 @@
 from .modules.llama import (
     LlamaConfig as LlamaConfig,
     FlaxLlamaForCausalLM as FlaxLlamaForCausalLM,
+    FlaxLlamaForSequenceClassification as FlaxLlamaForSequenceClassification,
     FlaxLlamaModel as FlaxLlamaModel
 )
 from .modules.gpt_j import (
@@ -74,7 +75,7 @@
     MixtralConfig as MixtralConfig
 )
 
-from .modules.auto_models import (
+from .modules.auto_easydel_model import (
     AutoEasyDelModelForCausalLM as AutoEasyDelModelForCausalLM,
     get_modules_by_type as get_modules_by_type
 )
Index: lib/python/EasyDel/modules/llama/modelling_llama_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from functools import partial\nfrom typing import Optional, Tuple, Union\n\nfrom einops import einops\nimport jax\nimport jax.numpy as jnp\nfrom jax import lax\nfrom jax.sharding import PartitionSpec\nimport flax.linen as nn\nfrom jax.experimental.shard_map import shard_map\nfrom flax.traverse_util import flatten_dict, unflatten_dict\nfrom flax.linen import partitioning as nn_partitioning, dot_product_attention_weights\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze\nfrom flax.linen import combine_masks, make_causal_mask\nfrom transformers.modeling_flax_utils import FlaxPreTrainedModel\nfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput, FlaxSequenceClassifierOutput\n# EasyDel.modules\nfrom ..flax_modelling_utils import (\n    with_sharding_constraint,\n    get_gradient_checkpoint_policy,\n    repeat_kv_bnsh,\n    apply_rotary_pos_emb,\n    precompute_freq_cis,\n    smart_flash_attention, get_dot_general_by_bits\n)\nimport chex\nfrom .llama_configuration import LlamaConfig\n\n\nclass FlaxLlamaEmbedding(nn.Module):\n    dtype: jnp.dtype = jnp.float32\n\n    def __call__(self, query, key, freq_cis, position_ids):\n        sin, cos = freq_cis\n\n        sin = sin[position_ids][:, None, :, :]\n        cos = cos[position_ids][:, None, :, :]\n\n        key = apply_rotary_pos_emb(key, sin, cos)\n        query = apply_rotary_pos_emb(query, sin, cos)\n\n        return query.astype(self.dtype), key.astype(self.dtype)\n\n\ndef repeat_kv(x: chex.Array, n_rep: int) -> chex.Array:\n    bs, s, n_kv_heads, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    x = x[:, :, jnp.newaxis, :, :]\n    x = jnp.repeat(x, n_rep, axis=2)\n\n    return x.reshape(bs, s,\n                     n_kv_heads * n_rep,\n                     head_dim)\n\n\nclass RMSNorm(nn.Module):\n    dim: int\n    eps: float = 1e-6\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n\n    def setup(self) -> None:\n        self.weight = self.param(\n            'kernel',\n            nn.initializers.ones,\n            (self.dim,),\n            self.param_dtype,\n        )\n\n    def _norm(self, x: jnp.ndarray) -> jnp.ndarray:\n        return x * jax.lax.rsqrt(jnp.square(x).mean(-1, keepdims=True) + self.eps)\n\n    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n        x = x.astype(jnp.promote_types(self.dtype, jnp.float32))\n        output = self._norm(x).astype(self.dtype)\n        weight = jnp.asarray(self.weight, self.dtype)\n        return output * weight\n\n\nclass FlaxLlamaAttention(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.hidden_size // self.config.num_attention_heads\n        self.number_of_reps = self.config.num_attention_heads // self.config.num_key_value_heads\n\n        if self.number_of_reps == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n        self.q_proj = nn.Dense(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.k_proj = nn.Dense(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.v_proj = nn.Dense(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.o_proj = nn.Dense(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n        self.rotary = FlaxLlamaEmbedding(self.dtype)\n\n        self.resid_dropout = nn.Dropout(rate=config.resid_pdrop)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        \"\"\"\n        The _concatenate_to_cache function is used to concatenate the key and value vectors\n        of a query with those of previous queries. This allows for the attention mechanism to\n        look at all previous queries when computing its output. The function takes in three\n        arguments: key, value, and query. It also uses two variables that are stored in the cache:\n        cached_key and cached_value.\n\n        :param self: Access the variables stored in the cache\n        :param key: Store the keys of the encoder-decoder attention\n        :param value: Initialize the cached_value variable\n        :param query: Determine the number of cache vectors to update\n        :param attention_mask: Mask out the padded vectors in the cache\n        :return: The key, value and attention_mask\n\n        \"\"\"\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\n            \"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\n            \"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\n            \"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(\n                cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    @staticmethod\n    def _t(query, key, value):\n        \"\"\"\n        The _t function transposes the query, key and value matrices.\n\n        :param query: Get the attention weights for each of the heads\n        :param key: Determine the number of heads\n        :param value: Store the values of the input\n        :return: The transpose of the query, key and value matrices\n\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"\n        The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        :param self: Access variables that belong to the class\n        :param batch_size: Reshape the query, key and value tensors\n        :param sequence_length: Reshape the query, key and value tensors\n        :param query: Calculate the attention weights\n        :param key: Calculate the attention\n        :param value: Compute the attention weights\n        :param freq_cis: Calculate the frequency of each word in the vocabulary\n        :param position_ids: Identify the position of each token in the sequence\n        :return: A tuple of 3 tensors: query, key and value\n\n        \"\"\"\n        query = query.reshape(batch_size, sequence_length,\n                              self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length,\n                          self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length,\n                              self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._t(query, key, value)\n        query, key = self.rotary(\n            position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.number_of_reps)\n        value = repeat_kv_bnsh(value, self.number_of_reps)\n        return self._t(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n        \"\"\"\n\n        The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &quot;forward pass&quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        :param self: Access variables that belong to the class\n        :param hidden_states: chex.Array: Pass the hidden states of the previous layer\n        :param freq_cis: chex.Array: Pass in the frequency coefficients for each position\n        :param attention_mask: chex.Array: Mask out certain tokens in the input sequence\n        :param position_ids: chex.Array: Determine the position of each token in a sequence\n        :param causal_mask: chex.Array: Mask out the future tokens in the decoder\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :param fcm_mask: Mask out the attention weights between the input and output tokens\n        :param : Determine if the attention is causal or not\n        :return: A tuple of two arrays\n\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_state, key_state, value_state = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        if self.config.use_pjit_attention_force:\n            query_state = with_sharding_constraint(\n                query_state, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\"))\n            key_state = with_sharding_constraint(\n                key_state, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\"))\n            value_state = with_sharding_constraint(\n                value_state, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\"))\n\n        query_state = query_state.reshape(\n            batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_state = key_state.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_state = value_state.reshape(\n            batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_state, key_state, value_state = self.apply_rotary(\n            query=query_state,\n            key=key_state,\n            value=value_state,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with number_of_reps = {self.number_of_reps}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_state.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_state.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_state.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_state.shape[1], key_state.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1,\n                                                     query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(\n            causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(\n            attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        dropout_rng = None\n\n        if not deterministic and self.config.attention_dropout > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_state, value_state, attention_mask = self._concatenate_to_cache(\n                key_state,\n                value_state,\n                query_state,\n                attention_mask\n            )\n\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(\n                    self.config.num_attention_heads, 1, )\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(\n                    self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(query_state, rtp_axis),\n                k=jnp.transpose(key_state, rtp_axis),\n                v=jnp.transpose(value_state, rtp_axis),\n                q_ps=self.config.q_ps,\n                k_ps=self.config.k_ps,\n                v_ps=self.config.v_ps,\n                b_ps=self.config.b_ps,\n                a_ps=self.config.a_ps,\n                bias=attention_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=sequence_length,\n                kv_seq_len=key_length,\n                attn_pdrop=self.config.attention_dropout,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(\n                    self.dtype).min).astype(self.dtype),\n            )\n            if self.config.use_shard_map:\n                attn_weights = shard_map(\n                    partial(\n                        dot_product_attention_weights,\n                        dtype=jnp.promote_types(self.dtype, jnp.float32),\n                        deterministic=deterministic,\n                        dropout_rate=self.config.attention_dropout,\n                        precision=self.precision,\n                    ),\n                    mesh=self.config.jax_mesh(),\n                    in_specs=(\n                        self.config.q_ps,\n                        self.config.k_ps,\n                        self.config.b_ps\n                    ),\n                    out_specs=PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None),\n                    check_rep=False\n                )(\n                    query_state, key_state, attention_bias\n                )\n            else:\n                attn_weights = dot_product_attention_weights(\n                    query=query_state,\n                    key=key_state,\n                    bias=attention_bias,\n                    dtype=jnp.promote_types(self.dtype, jnp.float32),\n                    deterministic=deterministic,\n                    dropout_rate=self.config.attention_dropout,\n                    precision=self.precision,\n                )\n\n            if self.config.use_pjit_attention_force:\n                attn_weights = with_sharding_constraint(\n                    attn_weights, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", \"tp\", None))\n\n            attn_output = jnp.einsum(\n                \"...hqk,...khd->...qhd\",\n                attn_weights,\n                value_state,\n                precision=self.precision\n            )\n\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.o_proj(attn_output)\n\n        attn_output = self.resid_dropout(\n            attn_output, deterministic=deterministic)\n        outputs = (attn_output, attn_weights) if output_attentions else (\n            attn_output,)\n\n        return outputs\n\n\nclass FlaxLlamaMLP(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        config = self.config\n\n        self.gate_proj = nn.Dense(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.down_proj = nn.Dense(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.up_proj = nn.Dense(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n        self.dropout = nn.Dropout(rate=self.config.resid_pdrop)\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -> jnp.ndarray:\n        \"\"\"\n        The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        :param self: Represent the instance of the class\n        :param x: jnp.ndarray: Pass in the input to the layer\n        :param deterministic: bool: Determine whether to use dropout\n        :return: A tensor that is the result of applying a dropout function to x\n\n        \"\"\"\n        x = self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n        x = self.dropout(x, deterministic=deterministic)\n        return x\n\n\nclass FlaxLlamaBlock(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        attn_block = FlaxLlamaAttention\n        if self.config.gradient_checkpointing != '':\n            attn_block = nn_partitioning.remat(\n                FlaxLlamaAttention, static_argnums=(5, 6, 7),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing)\n            )\n\n        self.self_attn = attn_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        mlp_block = FlaxLlamaMLP\n\n        if self.config.gradient_checkpointing != '':\n            mlp_block = nn_partitioning.remat(\n                FlaxLlamaMLP, static_argnums=(1,),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing)\n            )\n\n        self.mlp = mlp_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n        self.input_layernorm = RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.post_attention_layernorm = RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask: Optional[jnp.ndarray] = None,\n    ):\n        \"\"\"\n        The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in hidden states, frequency-domain inputs, and masks as input. It then\n        applies self-attention to the hidden states using those inputs and returns an\n        output tensor with shape (batch_size, sequence_length, model_dim).\n\n        :param self: Refer to the class instance itself\n        :param hidden_states: chex.Array: Pass in the hidden state of the previous layer\n        :param freq_cis: chex.Array: Pass in the frequency information\n        :param attention_mask: chex.Array: Mask out the attention weights for padding tokens\n        :param position_ids: chex.Array: Determine the position of each token in the sequence\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param deterministic: bool: Control whether the dropout is applied or not\n        :param init_cache: bool: Initialize the cache in the attention layer\n        :param output_attentions: bool: Return the attention weights\n        :param fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n        :param : Control the dropout in the self attention layer\n        :return: A tuple of two items\n\n        \"\"\"\n        attn_outputs = self.self_attn(\n            self.input_layernorm(hidden_states),\n            freq_cis,\n            attention_mask,\n            position_ids,\n            causal_mask,\n            deterministic,\n            init_cache,\n            output_attentions,\n            fcm_mask,\n        )\n        attn_output = attn_outputs[0]\n        hidden_states = hidden_states + attn_output\n\n        feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n        if self.config.use_sacn_mlp:\n            feed_forward_input = einops.rearrange(\n                feed_forward_input,\n                '... (b s) d -> ... b s d',\n                b=self.config.scan_mlp_chunk_size\n            )\n\n            def mlp_forward(mlp, carry, x):\n                return None, mlp(x, deterministic)\n\n            scan_axis = feed_forward_input.ndim - 3\n\n            _, feed_forward_hidden_states = nn.scan(\n                mlp_forward,\n                variable_broadcast=\"params\",\n                split_rngs={\"params\": False, \"dropout\": True},\n                in_axes=scan_axis,\n                out_axes=scan_axis,\n            )(self.mlp, None, feed_forward_input)\n            feed_forward_hidden_states = einops.rearrange(\n                feed_forward_hidden_states,\n                '... b s d -> ... (b s) d'\n            )\n        else:\n            feed_forward_hidden_states = self.mlp(\n                feed_forward_input,\n                deterministic,\n            )\n\n        hidden_states = hidden_states + feed_forward_hidden_states\n\n        return (hidden_states,) + attn_outputs[1:]\n\n\nclass FlaxLlamaPreTrainedModel(FlaxPreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: LlamaConfig,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines what happens when it's created.\n        The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n\n        :param self: Refer to the object itself\n        :param config: LlamaConfig: Pass the configuration to the module\n        :param input_shape: Tuple: Specify the shape of the input to the model\n        :param seed: int: Set the seed for random number generation\n        :param dtype: jnp.dtype: Specify the data type of the input\n        :param _do_init: bool: Control whether the module is initialized or not\n        :param kwargs: Pass in any additional parameters that the module_class might need\n        :param : Specify the number of layers in the network\n        :return: The super() of the class\n\n        \"\"\"\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape,\n                         seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        \"\"\"\n        The init_weights function is used to initialize the weights of a model.\n\n        :param self: Access variables that belong to the class\n        :param rng: jax.random.PRNGKey: Initialize the weights of the model\n        :param input_shape: Tuple: Specify the shape of the input tensor\n        :param params: FrozenDict: Pass in the parameters of a pre-trained model\n        :return: A frozendict of parameters\n\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(\n                input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(\n                rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        \"\"\"\n        The init_cache function is used to initialize the cache for a given batch size and sequence length.\n        The cache is a dictionary that contains all the intermediate states from each layer in the model.\n        This allows us to run inference on multiple batches without having to re-run forward passes through every layer in\n        the model, which would be very slow.\n\n        :param self: Access the module\n        :param batch_size: Define the batch size of the input tensors\n        :param max_length: Set the length of the input sequence\n        :return: A dictionary with the following keys:\n\n        \"\"\"\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(\n            jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, but it also has some other important features:\n        - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n        - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n        :param self: Represent the instance of the class\n        :param input_ids: chex.Array: Pass in the input tokens\n        :param attention_mask: chex.Array: Mask out certain tokens in the input\n        :param position_ids: chex.Array: Create the positional embeddings\n        :param params: dict: Pass in the parameters of the model\n        :param past_key_values: dict: Pass in the past key values from a previous call to __call__\n        :param dropout_rng: jax.random.PRNGKey: Make sure that the dropout is applied in a random way\n        :param train: bool: Determine whether to use dropout or not\n        :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n        :param output_hidden_states: Optional[bool]: Return the hidden states of all layers\n        :param return_dict: Optional[bool]: Determine whether to return a dictionary or not\n        :param extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids\n        :param add_params_field: bool: Add the params field to the inputs dictionary\n        :return: A tuple of the following:\n\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length <= self.config.max_position_embeddings, (f'Position out of range '\n                                                                        f'(Model Support '\n                                                                        f'{self.config.max_position_embeddings} got'\n                                                                        f' {sequence_length})')\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\n                    \"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[\n                                            None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n\n        inputs = {\n            \"params\": params or self.params} if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            extra_embedding,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + \\\n                      (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxLlamaBlockCollection(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.blocks = [\n            FlaxLlamaBlock(\n                self.config,\n                name=str(i),\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision\n            )\n            for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX nn.Module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in training loops or inference scripts.\n        The __call__ method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.\n\n        :param self: Represent the instance of the class\n        :param hidden_states: chex.Array: Pass the input tensor to the encoder\n        :param freq_cis: chex.Array: Pass in the frequency of each token\n        :param attention_mask: chex.Array: Mask out certain tokens in the input sequence\n        :param position_ids: chex.Array: Specify the position of each token in a sequence\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param deterministic: bool: Determine whether the model is in training or evaluation mode\n        :param init_cache: bool: Initialize the cache for each layer\n        :param output_attentions: bool: Determine whether to output the attention weights\n        :param output_hidden_states: bool: Determine whether to return the hidden states of each layer\n        :param return_dict: bool: Return a dictionary of the outputs\n        :param : Determine whether to use the forgetful causal mask\n        :return: A tuple of 3 values\n\n        \"\"\"\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n\n        if not deterministic and self.config.fcm_max_ratio > 0:\n            # Apply forgetful causal mask\n            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n            fcm_ratio = jax.random.uniform(\n                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n                minval=self.config.fcm_min_ratio,\n                maxval=self.config.fcm_max_ratio\n            )\n            fcm_mask = jax.random.uniform(\n                self.make_rng('fcm'),\n                shape=(batch_size, 1, seq_length, seq_length)\n            ) > fcm_ratio\n            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n            fcm_mask = fcm_mask.astype('bool')\n        else:\n            fcm_mask = None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                freq_cis=freq_cis,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n                fcm_mask=fcm_mask,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n\n        return outputs\n\n\nclass FlaxLlamaModule(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.dropout = nn.Dropout(rate=self.config.embd_pdrop)\n        self.layers = FlaxLlamaBlockCollection(self.config, dtype=self.dtype, param_dtype=self.param_dtype,\n                                               precision=self.precision)\n        self.norm = RMSNorm(self.config.hidden_size, eps=self.config.rms_norm_eps, dtype=self.dtype,\n                            param_dtype=self.param_dtype)\n        config = self.config\n        self.causal_mask = make_causal_mask(\n            jnp.ones((1, config.max_position_embeddings)))\n\n        initial_rope_kwargs = dict(\n            rope_type=\"none\"\n        )\n        if config.rope_scaling is not None:\n            scaling_type = config.rope_scaling[\"type\"]\n            scaling_factor = config.rope_scaling[\"factor\"]\n            initial_rope_kwargs = dict(\n                scaling_factor=scaling_factor,\n                rope_type=scaling_type\n            )\n        self.freq_cis = precompute_freq_cis(\n            max_position_embeddings=config.max_position_embeddings,\n            dim=config.hidden_size // config.num_attention_heads,\n            base=config.rope_theta,\n            **initial_rope_kwargs\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            inputs_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"\n        The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n        and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n        the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n        calling a Flax model.\n\n        :param self: Represent the instance of the class\n        :param input_ids: chex.Array: Pass in the input token ids\n        :param attention_mask: chex.Array: Mask out the padding tokens\n        :param position_ids: chex.Array: Indicate the position of each token in a sequence\n        :param deterministic: bool: Control whether dropout is applied or not\n        :param inputs_embeds: chex.Array: Pass in the embeddings of the input tokens\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attentions or not\n        :param output_hidden_states: bool: Determine whether to return hidden states\n        :param return_dict: bool: Return a dictionary of the output or not\n        :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of the\n        :param None]]: Pass in the extra embedding\n        :return: A tuple of:\n\n        \"\"\"\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n        batch_size, sequence_length, _ = inputs_embeds.shape\n        assert sequence_length <= self.config.max_position_embeddings, (f'Position out of range '\n                                                                        f'(Model Support '\n                                                                        f'{self.config.max_position_embeddings} got'\n                                                                        f' {sequence_length})')\n        inputs_embeds = inputs_embeds + \\\n                        extra_embedding if extra_embedding is not None else inputs_embeds\n        hidden_states = self.dropout(\n            inputs_embeds, deterministic=deterministic)\n\n        outputs = self.layers(\n            hidden_states=hidden_states,\n            freq_cis=self.freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=self.causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n\n\nclass FlaxLlamaModel(FlaxLlamaPreTrainedModel):\n    module_class = FlaxLlamaModule\n\n\nclass FlaxLlamaForCausalLMModule(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model = FlaxLlamaModule(self.config,\n                                     dtype=self.dtype,\n                                     param_dtype=self.param_dtype,\n                                     precision=self.precision,\n                                     )\n\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"\n        The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n        :param self: Refer to the object itself\n        :param input_ids: chex.Array: Pass the input token ids to the model\n        :param attention_mask: chex.Array: Mask out the padding tokens\n        :param position_ids: chex.Array: Specify the position of each token in the input sequence\n        :param deterministic: bool: Control whether the model is trained or not\n        :param init_cache: bool: Initialize the cache for the decoder\n        :param output_attentions: bool: Return the attention weights\n        :param output_hidden_states: bool: Determine whether or not to return the hidden states\n        :param return_dict: bool: Return a dictionary of the outputs or not\n        :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict\n        :param None]]: Pass in the extra embedding\n        :return: The logits and the hidden states\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply(\n                {\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n\n\nclass FlaxLlamaForCausalLM(FlaxLlamaPreTrainedModel):\n    module_class = FlaxLlamaForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"\n        The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        :param self: Access variables that belong to the class\n        :param input_ids: Pass in the input tokens\n        :param max_length: Set the length of the sequence to be generated\n        :param attention_mask: Optional[chex.Array]: Mask the attention weights\n        :return: A dictionary of the past_key_values, attention_mask and position ids\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones(\n            (batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(\n                extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[\n                                            None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n\n\nclass FlaxLlamaForSequenceClassificationModule(nn.Module):\n    num_classes: int\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        \"\"\"\n        The setup function is called once at the beginning of training.\n        It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n        :param self: Access variables that belong to the class\n        :return: A tuple of the model and the classifier\n        \"\"\"\n        self.model = FlaxLlamaModule(self.config, dtype=self.dtype)\n        self.classifier = nn.Dense(\n            self.num_classes,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(\n                stddev=self.config.initializer_range),\n            precision=self.precision,\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"\n        The __call__ function is the main function of a Flax module.\n        It takes in all the inputs to the model and returns all outputs from it.\n        The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n            &gt;&gt;&gt; my_model = MyModel()  # instantiate your model class\n            &gt;&gt;&gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n        :param self: Refer to the class instance\n        :param input_ids: chex.Array: Pass the input to the model\n        :param attention_mask: chex.Array: Specify which tokens are masked\n        :param position_ids: chex.Array: Specify the position of each token in the sequence\n        :param deterministic: bool: Control whether the model is run in deterministic or stochastic mode\n        :param init_cache: bool: Initialize the cache for the transformer\n        :param output_attentions: bool: Return the attention weights\n        :param output_hidden_states: bool: Return the hidden states of all layers\n        :param return_dict: bool: Return a dictionary of outputs\n        :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of a new word\n        :param None]]: Pass the extra embedding to the model\n        :return: A tuple of logits and hidden_states\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n        prediction = self.classifier(hidden_states)\n        if return_dict:\n            return FlaxSequenceClassifierOutput(\n                logits=prediction,\n                hidden_states=hidden_states\n            )\n        else:\n            return prediction,\n\n\nclass FlaxLlamaForSequenceClassification(FlaxLlamaPreTrainedModel):\n    module_class = FlaxLlamaForSequenceClassificationModule\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/llama/modelling_llama_flax.py b/lib/python/EasyDel/modules/llama/modelling_llama_flax.py
--- a/lib/python/EasyDel/modules/llama/modelling_llama_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/llama/modelling_llama_flax.py	(date 1703669116108)
@@ -12,7 +12,6 @@
 from flax.linen import partitioning as nn_partitioning, dot_product_attention_weights
 from flax.core.frozen_dict import FrozenDict, freeze, unfreeze
 from flax.linen import combine_masks, make_causal_mask
-from transformers.modeling_flax_utils import FlaxPreTrainedModel
 from transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput, FlaxSequenceClassifierOutput
 # EasyDel.modules
 from ..flax_modelling_utils import (
@@ -23,6 +22,7 @@
     precompute_freq_cis,
     smart_flash_attention, get_dot_general_by_bits
 )
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 import chex
 from .llama_configuration import LlamaConfig
 
@@ -622,7 +622,7 @@
         return (hidden_states,) + attn_outputs[1:]
 
 
-class FlaxLlamaPreTrainedModel(FlaxPreTrainedModel):
+class FlaxLlamaPreTrainedModel(EasyDelFlaxPretrainedModel):
     config_class = LlamaConfig
     base_model_prefix = "model"
     module_class: nn.Module = None
@@ -1051,6 +1051,12 @@
 class FlaxLlamaModel(FlaxLlamaPreTrainedModel):
     module_class = FlaxLlamaModule
 
+    def set_input_embeddings(self, value):
+        self.module.embed_tokens = value
+
+    def get_input_embeddings(self):
+        return self.module.embed_tokens
+
 
 class FlaxLlamaForCausalLMModule(nn.Module):
     config: LlamaConfig
@@ -1145,6 +1151,24 @@
 class FlaxLlamaForCausalLM(FlaxLlamaPreTrainedModel):
     module_class = FlaxLlamaForCausalLMModule
 
+    def set_input_embeddings(self, value):
+        self.module.model.embed_tokens = value
+
+    def get_input_embeddings(self):
+        return self.module.model.embed_tokens
+
+    def set_decoder(self, decoder):
+        self.module.model = decoder
+
+    def get_decoder(self):
+        return self.module.model
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
     def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):
         """
         The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.
Index: lib/python/EasyDel/modules/gpt_j/modelling_gpt_j_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Note\n# the GPT-j implemented by Huggingface is not supporting Partition spec s and using\n# fully with pjit and required creating\n# parameters even if you want to load already trained model so this one is the same\n# but include those bugs / non-features fixed\n\n\n# coding=utf-8\n# Copyright 2021 The EleutherAI and HuggingFace Teams. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" GPT-J model configuration\"\"\"\n\nfrom functools import partial\nfrom typing import Optional, Tuple\n\nfrom einops import einops\nfrom fjformer import with_sharding_constraint\nfrom jax.sharding import PartitionSpec\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom flax.core.frozen_dict import FrozenDict, unfreeze\nfrom flax.linen import combine_masks, make_causal_mask\nfrom flax.linen.attention import dot_product_attention_weights\nfrom jax import lax\n\nfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput\nfrom transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel\nfrom transformers.utils import logging\n\nfrom fjformer.attention import efficient_attention\nfrom ..flax_modelling_utils import with_sharding_constraint\nimport chex\nfrom fjformer.bits import config as q_config, q_flax\nfrom .gpt_j_configuration import GPTJConfig, GPTJOnnxConfig\n\nlogger = logging.get_logger(__name__)\n\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)\n\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass FlaxGPTJAttention(nn.Module):\n    config: GPTJConfig\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    is_cross_attention: bool = False\n\n    def setup(self):\n        config = self.config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n\n        self.rotary_dim = config.rotary_dim\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            dot_general=dot_general_cls\n        )\n\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n\n        self.resid_dropout = nn.Dropout(rate=config.resid_pdrop)\n\n        self.causal_mask = make_causal_mask(jnp.ones((1, config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\")\n\n        pos_embd_dim = self.rotary_dim or self.embed_dim\n        self.embed_positions = create_sinusoidal_positions(config.max_position_embeddings, pos_embd_dim)\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: chex.Array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask,\n            position_ids,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n    ):\n        query = self.q_proj(hidden_states)\n        key = self.k_proj(hidden_states)\n        value = self.v_proj(hidden_states)\n\n        # Force A local Sharding\n        if self.config.use_pjit_attention_force:\n            query = with_sharding_constraint(query, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            key = with_sharding_constraint(key, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n            value = with_sharding_constraint(value, PartitionSpec((\"dp\", \"fsdp\"), None, \"sp\"))\n\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None:\n            k_rot = key[:, :, :, : self.rotary_dim]\n            k_pass = key[:, :, :, self.rotary_dim:]\n\n            q_rot = query[:, :, :, : self.rotary_dim]\n            q_pass = query[:, :, :, self.rotary_dim:]\n\n            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\n            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n        else:\n            key = apply_rotary_pos_emb(key, sincos)\n            query = apply_rotary_pos_emb(query, sincos)\n\n        query_length, key_length = query.shape[1], key.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                self.causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = self.causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask)\n\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n\n        # transform boolean mask into float mask\n        attention_bias = lax.select(\n            attention_mask > 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n        )\n\n        # usual dot product attention\n        if self.config.use_flash_attention:\n            attn_weights = None\n            attention_mask = einops.rearrange(\n                attention_bias,\n                '... s q k -> ... s 1 q k'\n            )\n            attn_output = efficient_attention(\n                query,\n                key,\n                value,\n                bias=attention_mask,\n                dropout_rng=dropout_rng,\n                attention_drop_rate=self.config.attn_pdrop,\n                deterministic=not deterministic and self.config.attn_pdrop > 0.0,\n                float32_logits=True,\n                causal=True,\n                dtype=self.dtype,\n                precision=self.precision,\n                query_chunk_size=self.config.flash_attn_query_chunk_size,\n                key_chunk_size=self.config.flash_attn_key_chunk_size,\n            )\n        else:\n            attn_weights = dot_product_attention_weights(\n                query,\n                key,\n                bias=attention_bias,\n                dropout_rng=dropout_rng,\n                dropout_rate=self.config.attn_pdrop,\n                deterministic=deterministic,\n                dtype=jnp.promote_types(self.dtype, jnp.bfloat16),\n                precision=self.precision,\n            )\n            if self.config.use_pjit_attention_force:\n                attn_weights = with_sharding_constraint(attn_weights, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None))\n\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value, precision=self.precision)\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.out_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n\n        outputs = (attn_output, attn_weights) if output_attentions else (attn_output,)\n        return outputs\n\n\nclass FlaxGPTJMLP(nn.Module):\n    config: GPTJConfig\n    intermediate_size: int\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        embed_dim = self.config.hidden_size\n        kernel_init = jax.nn.initializers.normal(self.config.initializer_range)\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n        self.fc_in = nn.Dense(\n            self.intermediate_size,\n            dtype=self.dtype,\n            kernel_init=kernel_init,\n            dot_general=dot_general_cls\n        )\n        self.fc_out = nn.Dense(\n            embed_dim,\n            dtype=self.dtype,\n            kernel_init=kernel_init,\n            dot_general=dot_general_cls\n        )\n\n        self.act = ACT2FN[self.config.activation_function]\n        self.dropout = nn.Dropout(rate=self.config.resid_pdrop)\n\n    def __call__(self, hidden_states, deterministic: bool = True):\n        hidden_states = self.fc_in(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc_out(hidden_states)\n        hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n        return hidden_states\n\n\nclass FlaxGPTJBlock(nn.Module):\n    config: GPTJConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        hidden_size = self.config.hidden_size\n        inner_dim = self.config.n_inner if self.config.n_inner is not None else 4 * hidden_size\n\n        self.ln_1 = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n        self.attn = FlaxGPTJAttention(self.config, dtype=self.dtype)\n\n        self.mlp = FlaxGPTJMLP(self.config, inner_dim, dtype=self.dtype)\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            position_ids=None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n    ):\n        residual = hidden_states\n        hidden_states = self.ln_1(hidden_states)\n        attn_outputs = self.attn(\n            hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n        )\n        attn_output = attn_outputs[0]\n\n        feed_forward_hidden_states = self.mlp(hidden_states, deterministic=deterministic)\n        # residual connection\n        hidden_states = attn_output + feed_forward_hidden_states + residual\n\n        return (hidden_states,) + attn_outputs[1:]\n\n\nclass FlaxGPTJPreTrainedModel(FlaxPreTrainedModel):\n    config_class = GPTJConfig\n    base_model_prefix = \"transformer\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: GPTJConfig,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # init input tensors\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"dropout\": dropout_rng}\n\n        if self.config.bits is not None:\n            rngs['params'] = jax.random.key(0)\n        if params is None:\n            if self.config.add_cross_attention:\n                encoder_hidden_states = jnp.zeros(input_shape + (self.config.n_embd,))\n                encoder_attention_mask = attention_mask\n                module_init_outputs = self.module.init(\n                    rngs,\n                    input_ids,\n                    attention_mask,\n                    position_ids,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    return_dict=False,\n                )\n            else:\n                module_init_outputs = self.module.init(rngs, input_ids, attention_mask, position_ids, return_dict=False)\n            return module_init_outputs\n        else:\n            return params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask=None,\n            position_ids=None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n        rngs['params'] = jax.random.key(0)\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        # add updated cache to model output\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxGPTJBlockCollection(nn.Module):\n    config: GPTJConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.blocks = [\n            FlaxGPTJBlock(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            position_ids=None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states,\n                attention_mask,\n                position_ids=position_ids,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n\n        return outputs\n\n\nclass FlaxGPTJModule(nn.Module):\n    config: GPTJConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.embed_dim = self.config.hidden_size\n\n        self.wte = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n        )\n        self.dropout = nn.Dropout(rate=self.config.embd_pdrop)\n        self.h = FlaxGPTJBlockCollection(self.config, dtype=self.dtype)\n        self.ln_f = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        inputs_embeds = self.wte(input_ids.astype(\"i4\"))\n\n        hidden_states = self.dropout(inputs_embeds, deterministic=deterministic)\n\n        outputs = self.h(\n            hidden_states,\n            attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.ln_f(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n\n\nclass FlaxGPTJModel(FlaxGPTJPreTrainedModel):\n    module_class = FlaxGPTJModule\n\n\nclass FlaxGPTJForCausalLMModule(nn.Module):\n    config: GPTJConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n        self.transformer = FlaxGPTJModule(self.config, dtype=self.dtype)\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dot_general=dot_general_cls\n        )\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        outputs = self.transformer(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n\n\nclass FlaxGPTJForCausalLM(FlaxGPTJPreTrainedModel):\n    module_class = FlaxGPTJForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/gpt_j/modelling_gpt_j_flax.py b/lib/python/EasyDel/modules/gpt_j/modelling_gpt_j_flax.py
--- a/lib/python/EasyDel/modules/gpt_j/modelling_gpt_j_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/gpt_j/modelling_gpt_j_flax.py	(date 1703669116100)
@@ -37,14 +37,14 @@
 from jax import lax
 
 from transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput
-from transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel
 from transformers.utils import logging
 
 from fjformer.attention import efficient_attention
-from ..flax_modelling_utils import with_sharding_constraint
+from ..flax_modelling_utils import with_sharding_constraint, ACT2FN
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 import chex
 from fjformer.bits import config as q_config, q_flax
-from .gpt_j_configuration import GPTJConfig, GPTJOnnxConfig
+from .gpt_j_configuration import GPTJConfig
 
 logger = logging.get_logger(__name__)
 
@@ -350,7 +350,7 @@
         return (hidden_states,) + attn_outputs[1:]
 
 
-class FlaxGPTJPreTrainedModel(FlaxPreTrainedModel):
+class FlaxGPTJPreTrainedModel(EasyDelFlaxPretrainedModel):
     config_class = GPTJConfig
     base_model_prefix = "transformer"
     module_class: nn.Module = None
@@ -586,6 +586,12 @@
 class FlaxGPTJModel(FlaxGPTJPreTrainedModel):
     module_class = FlaxGPTJModule
 
+    def get_input_embeddings(self):
+        return self.module.wte
+
+    def set_input_embeddings(self, value):
+        self.module.wte = value
+
 
 class FlaxGPTJForCausalLMModule(nn.Module):
     config: GPTJConfig
@@ -648,6 +654,24 @@
 class FlaxGPTJForCausalLM(FlaxGPTJPreTrainedModel):
     module_class = FlaxGPTJForCausalLMModule
 
+    def get_output_embeddings(self):
+        return self.module.lm_head
+
+    def get_decoder(self):
+        return self.module.transformer
+
+    def get_input_embeddings(self):
+        return self.module.transformer.wte
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
+    def set_decoder(self, decoder):
+        self.module.transformer = decoder
+
+    def set_input_embeddings(self, value):
+        self.module.transformer.wte = value
+
     def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):
 
         batch_size, seq_length = input_ids.shape
Index: lib/python/EasyDel/modules/palm/modelling_palm_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Union, Optional, Tuple, Any, Mapping, Sequence\nimport jax\nimport jax.numpy as jnp\nimport numpy as onp\nimport transformers.modeling_flax_outputs\nfrom einops import rearrange\nimport flax.linen as nn\nfrom flax.core import FrozenDict\nfrom jax import numpy as np\nfrom transformers.modeling_flax_outputs import FlaxCausalLMOutput\n\nfrom jax.sharding import PartitionSpec\nfrom ..flax_modelling_utils import get_gradient_checkpoint_policy, \\\n    with_sharding_constraint\nimport chex\nfrom .palm_configuration import PalmConfig\n\n\nclass RMSNorm(nn.Module):\n    dim: int\n    eps: float = 1e-6\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.weight = self.param(\n            'kernel',\n            nn.initializers.ones,\n            (self.dim,),\n            self.param_dtype,\n        )\n\n    def _norm(self, hidden_state: jnp.ndarray) -> jnp.ndarray:\n        return hidden_state * jax.lax.rsqrt(jnp.square(hidden_state).mean(-1, keepdims=True) + self.eps)\n\n    def __call__(self, hidden_state: jnp.ndarray) -> jnp.ndarray:\n        hidden_state = hidden_state.astype(jnp.promote_types(self.dtype, jnp.bfloat16))\n        output = self._norm(hidden_state).astype(self.dtype)\n        weight = jnp.asarray(self.weight, self.dtype)\n        return output * weight\n\n\ndef pre_compute_freq_cis(dim, max_length, theta: int = 10000.0, dtype=jnp.bfloat16):\n    freq_cis = 1 / (theta ** (jnp.arange(0, dim, 2).astype(dtype=dtype) / dim))\n    length = jnp.arange(max_length)\n    cis = jnp.outer(length, freq_cis).astype(dtype)\n    sin = jnp.sin(cis)\n    cos = jnp.cos(cis)\n    freq_cis = jnp.complex64(cos + 1j * sin)\n    return jnp.asarray(freq_cis)\n\n\ndef apply_rotary_embedding(xq, xk, freq_cis, dtype=jnp.bfloat16):\n    reshape_xq = xq.astype(jnp.flaot32).reshape(xq.shape[:-1], -1, 2)\n    reshape_xk = xk.astype(jnp.flaot32).reshape(xk.shape[:-1], -1, 2)\n\n    complex_q = jax.lax.complex(reshape_xq[..., 0], reshape_xq[..., 1])\n    complex_k = jax.lax.complex(reshape_xk[..., 0], reshape_xk[..., 1])\n\n    freq_cis = freq_cis.reshape(*freq_cis[:2], 1, *freq_cis[2:])\n    xq = complex_q * freq_cis\n    xk = complex_k * freq_cis\n    xq = jnp.stack([jnp.real(xq), jnp.imag(xq)], axis=-1).reshape(xq.shape[:-1], -1)\n    xk = jnp.stack([jnp.real(xk), jnp.imag(xk)], axis=-1).reshape(xk.shape[:-1], -1)\n    return xq.astype(dtype), xk.astype(dtype)\n\n\nclass ParallelPalmBlock(nn.Module):\n    config: PalmConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        attn_inner_dim = self.config.dim_head * self.config.num_attention_heads\n        ff_inner_dim = self.config.hidden_size * self.config.up_inner_dim\n        self.fused_dims = (attn_inner_dim, self.config.dim_head, self.config.dim_head, ff_inner_dim, ff_inner_dim)\n\n        # INPUT WEIGHTS\n        self.wi = self.param(\n            'kernel',\n            nn.initializers.normal,\n            (self.config.hidden_size, sum(self.fused_dims)),\n            self.param_dtype,\n        )\n\n        # ATTENTION WEIGHT OUTPUT\n        self.attn_wo = self.param(\n            'kernel',\n            nn.initializers.normal,\n            (attn_inner_dim, self.config.hidden_size),\n            self.param_dtype,\n        )\n\n        self.ff_wo = self.param(\n            'kernel',\n            nn.initializers.normal,\n            (attn_inner_dim, self.config.hidden_size),\n            self.param_dtype,\n        )\n\n        self.norm = RMSNorm(dim=self.config.hidden_size)\n        self.post_norm = RMSNorm(dim=self.config.hidden_size)\n\n        self.num_attention_heads: int = self.config.num_attention_heads\n        self.scale: float = self.config.dim_head ** -0.5\n\n    def __call__(self, hidden_state, freq_cis, causal_mask):\n        split_indices = onp.cumsum(self.fused_dims[:-1])\n\n        hidden_state = self.norm(hidden_state)\n\n        q, k, v, ff, ff_gate = np.split(hidden_state @ self.wi, split_indices, axis=-1)\n        q = rearrange(q, 'b s (h d)-> b s h d', h=self.num_attention_heads)\n        k = rearrange(k, 'b s (h d)-> b s h d', h=self.num_attention_heads)\n\n        q, k = apply_rotary_embedding(q, k, freq_cis, self.dtype)\n        q = rearrange(q, 'b s h d -> b s (h d)')\n        k = rearrange(k, 'b s h d -> b s (h d)')\n        q = rearrange(q, '... n (h d) -> ... h n d', h=self.num_attention_heads) * self.scale\n\n        sim = jnp.einsum('... h i d, ... j d -> ... h i j', q, k)\n        if self.config.use_pjit_attention_force:\n            sim = with_sharding_constraint(sim, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None))\n        mask_value = jnp.finfo(hidden_state).min\n        attn = nn.softmax(np.where(causal_mask, sim, mask_value), axis=-1)\n\n        out = jnp.einsum('... h i j, ... j d -> ... h i d', attn, v)\n        if self.config.use_pjit_attention_force:\n            out = with_sharding_constraint(out, PartitionSpec((\"dp\", \"fsdp\"), \"sp\", None, None))\n        attn_out = rearrange(out, '... h n hd -> ... n (h hd)') @ self.attn_wo\n\n        ff_out = (ff * nn.swish(ff_gate)) @ self.ff_wo\n\n        return attn_out + ff_out\n\n\nclass ParallelCollection(nn.Module):\n    config: PalmConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        block = ParallelPalmBlock\n        if self.config.gradient_checkpointing != '':\n            block = nn.remat(\n                block,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing)\n            )\n        self.blocks = [\n            block(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            )\n            for i in range(\n                self.config.num_hidden_layers\n            )\n        ]\n\n    def __call__(self, hidden_state, freq_cis, causal_mask, output_attention=False):\n        saves = []\n        for block in self.blocks:\n            hidden_state = block(\n                hidden_state=hidden_state,\n                freq_cis=freq_cis,\n                causal_mask=causal_mask\n            ) + hidden_state\n            if output_attention:\n                saves.append(hidden_state)\n        return hidden_state, saves\n\n\nclass PalmPretrainedModel(transformers.FlaxPreTrainedModel):\n    module_class: nn.Module\n    config_class = PalmConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def __init__(self, config: PalmConfig, input_shape=(1, 1), _do_init=False):\n        module = self.module_class(\n            config=config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        super().__init__(\n            config=config,\n            input_shape=input_shape,\n            _do_init=_do_init,\n            module=module\n        )\n\n    def init_weights(self, rng: jax.random.PRNGKey,\n                     input_shape: Tuple,\n                     params: FrozenDict = None\n                     ) -> [Mapping[str, Any], FrozenDict]:\n\n        if params is None:\n            return self.module.init(\n                rngs=rng,\n                input_ids=jnp.ones(input_shape, dtype='i4'),\n                attention_mask=jnp.ones(input_shape, dtype='i4'),\n\n            )['params']\n        else:\n            return params\n\n    def __call__(self, input_ids, attention_mask=None, params=None, add_params_field: bool = False,\n                 return_dict: bool = True, output_attention: bool = False):\n        params = {'params': params or self.params} if add_params_field else params or self.params\n        predict = self.module.apply(\n            params,\n            input_ids=jnp.asarray(input_ids, dtype='i4'),\n            attention_mask=jnp.asarray(attention_mask, dtype='i4') if attention_mask is not None else attention_mask,\n            return_dict=return_dict,\n            output_attention=output_attention\n        )\n        return predict\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        return {\n            \"attention_mask\": attention_mask,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        return model_kwargs\n\n\nclass FlaxPalmModule(nn.Module):\n    config: PalmConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.wte = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.dtype,\n            embedding_init=jax.nn.initializers.normal\n        )\n        self.block = ParallelCollection(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.freq_cis = pre_compute_freq_cis(\n            self.config.dim_head,\n            self.config.max_length,\n            dtype=self.dtype\n        )\n\n        self.ln_f = RMSNorm(\n            dim=self.config.hidden_size,\n            dtype=self.dtype,\n            precision=self.precision,\n            param_dtype=self.param_dtype,\n            eps=self.config.eps\n        )\n        self.causal_mask = nn.make_causal_mask(jnp.ones(\n            1, self.config.max_length\n        ))\n\n    def make_causal_mask(self, attention_mask=None):\n        assert attention_mask is not None\n        b, s = attention_mask.shape\n        mask = attention_mask + self.causal_mask\n        mask = jnp.where(\n            mask == 2,\n            1, 0\n        ).astype(jnp.bool_)\n        return mask.reshape(b, 1, 1, s)\n\n    def __call__(self,\n                 input_ids: chex.Array,\n                 attention_mask: chex.Array = None,\n                 return_dict: bool = True,\n                 output_attention: bool = False):\n        batch, seq_len = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones(\n                (batch, seq_len),\n                dtype=jnp.int32\n            )\n\n        mask = self.make_causal_mask(\n            attention_mask=attention_mask\n        )\n        hidden_state = self.wte(\n            inputs=input_ids\n        )\n        hidden_state, atn = self.block(\n            hidden_state=hidden_state,\n            causal_mask=mask,\n            output_attention=output_attention,\n            freq_cis=self.freq_cis[:seq_len].reshape(1, seq_len, -1)\n        )\n        hidden_state = self.ln_f(\n            hidden_state\n        )\n\n        if return_dict:\n            return transformers.modeling_flax_outputs.FlaxBaseModelOutput(\n                last_hidden_state=hidden_state,\n                hidden_states=atn\n            )\n        else:\n            return hidden_state, atn\n\n\nclass FlaxPalmModel(PalmPretrainedModel):\n    module_class = FlaxPalmModule\n\n\nclass FlaxPalmForCausalLMModule(nn.Module):\n    config: PalmConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -> None:\n        self.path_way = FlaxPalmModule(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        if not self.config.use_tie_word_embedding:\n            self.lm_head = self.param(\n                'kernel',\n                jax.nn.initializers.normal,\n                (self.config.hidden_size, self.config.vocab_size),\n                self.param_dtype\n            )\n\n    def __call__(self,\n                 input_ids: chex.Array,\n                 attention_mask: chex.Array = None,\n                 return_dict: bool = True,\n                 output_attention: bool = False):\n        out = self.path_way(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            output_attention=output_attention\n        )\n        last_state = out.last_hidden_state\n        if not self.config.use_tie_word_embedding:\n            last_state = last_state @ self.lm_head\n        else:\n            last_state = last_state @ self.path_way.wte.embedding.T\n\n        if return_dict:\n            return FlaxCausalLMOutput(\n                logits=last_state,\n                hidden_states=out.hidden_states\n            )\n        else:\n            return last_state, out.hidden_states if output_attention else last_state,\n\n\nclass FlaxPalmForCausalLM(PalmPretrainedModel):\n    module_class = FlaxPalmForCausalLMModule\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/palm/modelling_palm_flax.py b/lib/python/EasyDel/modules/palm/modelling_palm_flax.py
--- a/lib/python/EasyDel/modules/palm/modelling_palm_flax.py	(revision 410d9cae5c7e7995a4a192b1b53ad3d417051254)
+++ b/lib/python/EasyDel/modules/palm/modelling_palm_flax.py	(date 1703670261939)
@@ -1,4 +1,4 @@
-from typing import Union, Optional, Tuple, Any, Mapping, Sequence
+from typing import Union, Optional, Tuple, Any, Mapping
 import jax
 import jax.numpy as jnp
 import numpy as onp
@@ -14,6 +14,7 @@
     with_sharding_constraint
 import chex
 from .palm_configuration import PalmConfig
+from ..easydel_modelling_utils import EasyDelFlaxPretrainedModel
 
 
 class RMSNorm(nn.Module):
@@ -175,7 +176,7 @@
         return hidden_state, saves
 
 
-class PalmPretrainedModel(transformers.FlaxPreTrainedModel):
+class PalmPretrainedModel(EasyDelFlaxPretrainedModel):
     module_class: nn.Module
     config_class = PalmConfig
     dtype: jnp.dtype = jnp.bfloat16
@@ -319,6 +320,12 @@
 class FlaxPalmModel(PalmPretrainedModel):
     module_class = FlaxPalmModule
 
+    def get_input_embeddings(self):
+        return self.module.wte
+
+    def set_input_embeddings(self, value):
+        self.module.wte = value
+
 
 class FlaxPalmForCausalLMModule(nn.Module):
     config: PalmConfig
@@ -369,3 +376,21 @@
 
 class FlaxPalmForCausalLM(PalmPretrainedModel):
     module_class = FlaxPalmForCausalLMModule
+
+    def get_input_embeddings(self):
+        return self.module.path_way.wte
+
+    def get_decoder(self):
+        return self.module.path_way
+
+    def set_input_embeddings(self, value):
+        self.module.path_way.wte = value
+
+    def set_decoder(self, decoder):
+        self.module.path_way = decoder
+
+    def set_output_embeddings(self, new_embeddings):
+        self.module.lm_head = new_embeddings
+
+    def get_output_embeddings(self):
+        return self.module.lm_head
diff --git a/lib/python/EasyDel/modules/auto_models.py b/lib/python/EasyDel/modules/auto_easydel_model.py
rename from lib/python/EasyDel/modules/auto_models.py
rename to lib/python/EasyDel/modules/auto_easydel_model.py
