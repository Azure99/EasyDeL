Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># EasyDeL \uD83D\uDD2E\n\nEasyDeL, an open-source library, is specifically designed to enhance and streamline the training process of machine\nlearning models. It focuses primarily on Jax/Flax and aims to provide convenient and effective solutions for training\nFlax/Jax Models on TPU/GPU for both Serving and Training purposes. Additionally, EasyDeL will support mojo and will be\nrewritten for mojo as well.\n\nSome of the key features provided by EasyDeL include:\n\n- Support for 8, 6, and 4 BIT inference and training in JAX\n- Wide Range of models in Jax are supported which have never been implemented before such as _falcon_ \n- Integration of flashAttention in JAX for GPUs and TPUs\n- Automatic serving of LLMs with mid and high-level APIs in both JAX and PyTorch\n- LLM Trainer and fine-tuner in JAX\n- RLHF (Reinforcement Learning from Human Feedback) in Jax\n- And various other features to enhance the training process and optimize performance.\n\n> [!NOTE]\n> These features collectively aim to simplify and accelerate the training of machine learning models, making it more\n> efficient and accessible for developers working with Jax/Flax.\n\n## Documentation \uD83D\uDCAB\n\n> [!IMPORTANT]\n> Documents and Examples are ready at [Here](https://erfanzar.github.io/EasyDeL)\n> Please have that in mind that EasyDel is in the loop of fast-development\n> so we might have API changes\n\n## Serving\n\nyou can read docs or examples to see how `JAXServer` works but let me show you how you can simply host and serve a\nLLama2\nchat model (70B model is supported too)\n\n```shell\npython -m examples.serving.causal-lm.llama-2-chat \\\n  --repo_id='meta-llama/Llama-2-7b-chat-hf' --max_length=4096 \\\n  --max_new_tokens=2048 --max_stream_tokens=32 --temperature=0.6 \\\n  --top_p=0.95 --top_k=50 \\\n  --dtype='fp16' --use_prefix_tokenizer\n\n```\n\n> [!NOTE]\n> you can use all the llama models not just 'meta-llama/Llama-2-7b-chat-hf'\n> float16 or float32 , bfloat16 are supported dtype and make sure to use --use_prefix_tokenizer,\n> and you will get links or api to use model from gradio app chat/instruct or FastAPI apis\n\n## RLHF(Reinforcement Learning From Human Feedback)\n\n> RLHF or Reinforcement Learning From Human Feedback is Available At the moment, but it's still\n> under heavy development , because I don't have enough experience with Reinforcement Learning at the moment so its\n> still\n> in beta version but it's works and ill soon release a Tutorial For that\n\n## FineTuning\n\nwith using EasyDel FineTuning LLM (CausalLanguageModels) are easy as much as possible with using Jax and Flax\nand having the benefit of TPUs for the best speed here's a simple code to use in order to finetune your\nown Model\n\nDays Has Been Passed and now using easydel in Jax is way more similar to HF/PyTorch Style\nnow it's time to finetune our model\n\n```python\nimport jax.numpy\nfrom EasyDel import TrainArguments, CausalLMTrainer, AutoEasyDelModelForCausalLM, FlaxLlamaForCausalLM\nfrom datasets import load_dataset\nimport flax\nfrom jax import numpy as jnp\n\nmodel, params = AutoEasyDelModelForCausalLM.from_pretrained(\"\", )\n\nmax_length = 4096\n\nconfigs_to_init_model_class = {\n    'config': model.config,\n    'dtype': jnp.bfloat16,\n    'param_dtype': jnp.bfloat16,\n    'input_shape': (1, 1)\n}\n\ntrain_args = TrainArguments(\n    model_class=type(model),\n    model_name='my_first_model_to_train_using_easydel',\n    num_train_epochs=3,\n    learning_rate=5e-5,\n    learning_rate_end=1e-6,\n    optimizer='adamw',  # 'adamw', 'lion', 'adafactor' are supported\n    scheduler='linear',  # 'linear','cosine', 'none' ,'warm_up_cosine' and 'warm_up_linear'  are supported\n    weight_decay=0.01,\n    total_batch_size=64,\n    max_steps=None,  # None to let trainer Decide\n    do_train=True,\n    do_eval=False,  # it's optional but supported \n    backend='tpu',  # default backed is set to cpu, so you must define you want to use tpu cpu or gpu\n    max_length=max_length,  # Note that you have to change this in the model config too\n    gradient_checkpointing='nothing_saveable',\n    sharding_array=(1, -1, 1, 1),  # the way to shard model across gpu,cpu or TPUs using sharding array (1, -1, 1, 1)\n    # everything training will be in fully FSDP automatic and share data between devices\n    use_pjit_attention_force=False,\n    remove_ckpt_after_load=True,\n    gradient_accumulation_steps=8,\n    loss_remat='',\n    dtype=jnp.bfloat16\n)\ndataset = load_dataset('TRAIN_DATASET')\ndataset_train = dataset['train']\ndataset_eval = dataset['eval']\n\ntrainer = CausalLMTrainer(\n    train_args,\n    dataset_train,\n    ckpt_path=None\n)\n\noutput = trainer.train(flax.core.FrozenDict({'params': params}))\nprint(f'Hey ! , here\\'s where your model saved {output.last_save_file_name}')\n\n\n```\n\n> [!TIP]\n> you can then convert it to pytorch for better use I don't recommend jax/flax for hosting models since\n> pytorch is better option for gpus\n\n## LLMServe\n\nTo use EasyDeL in your project, you will need to import the library in your Python script and use its various functions\nand classes. Here is an example of how to import EasyDeL and use its Model class:\n\n```python\nfrom EasyDel.modules import AutoEasyDelModelForCausalLM\nfrom EasyDel.serve import JAXServer\nfrom transformers import AutoTokenizer\nimport jax\n\nmodel_id = 'meta-llama/Llama.md-2-7b-chat-hf'\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel, params = AutoEasyDelModelForCausalLM.from_pretrained(\n    model_id,\n    jax.devices('cpu')[0],\n    jax.numpy.float16,\n    jax.numpy.float16,\n    jax.lax.Precision('fastest'),\n    (1, -1, 1, 1),\n    device_map='auto'\n)\n\nserver = JAXServer.load_from_params(\n    model=model,\n    config_model=model.config,\n    tokenizer=tokenizer,\n    params=model.params,\n    add_params_field=True\n)\n\nresponse_printed = 0\nfor response, tokens_used in server.process(\n        'String To The Model', stream=True\n):\n    print(response[response_printed:], end='')\n    response_printed = len(response)\n``` \n\n## Contributing\n\nEasyDeL is an open-source project, and contributions are welcome. If you would like to contribute to EasyDeL, please\nfork the repository, make your changes, and submit a pull request. The team behind EasyDeL will review your changes and\nmerge them if they are suitable.\n\n## License \uD83D\uDCDC\n\nEasyDeL is released under the Apache v2 license. Please see the LICENSE file in the root directory of this project for\nmore information.\n\n## Contact\n\nIf you have any questions or comments about EasyDeL, you can reach out to me\n\n## Citing EasyDeL \uD83E\uDD76\n\nTo cite this repository:\n\n```misc\n@misc{Zare Chavoshi_2023,\n    title={EasyDeL, an open-source library, is specifically designed to enhance and streamline the training process of machine learning models. It focuses primarily on Jax/Flax and aims to provide convenient and effective solutions for training Flax/Jax Models on TPU/GPU for both Serving and Training purposes.},\n    url={https://github.com/erfanzar/EasyDel},\n    journal={EasyDeL Easy and Fast DeepLearning with JAX},\n    publisher={Erfan Zare Chavoshi},\n    author={Zare Chavoshi, Erfan},\n    year={2023}\n} \n```\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README.md b/README.md
--- a/README.md	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/README.md	(date 1703504083514)
@@ -8,7 +8,7 @@
 Some of the key features provided by EasyDeL include:
 
 - Support for 8, 6, and 4 BIT inference and training in JAX
-- Wide Range of models in Jax are supported which have never been implemented before such as _falcon_ 
+- Wide Range of models in Jax are supported which have never been implemented before such as _falcon_
 - Integration of flashAttention in JAX for GPUs and TPUs
 - Automatic serving of LLMs with mid and high-level APIs in both JAX and PyTorch
 - LLM Trainer and fine-tuner in JAX
@@ -64,7 +64,13 @@
 
 ```python
 import jax.numpy
-from EasyDel import TrainArguments, CausalLMTrainer, AutoEasyDelModelForCausalLM, FlaxLlamaForCausalLM
+from EasyDel import (TrainArguments,
+                     CausalLMTrainer,
+                     AutoEasyDelModelForCausalLM,
+                     EasyDelOptimizers,
+                     EasyDelSchedulers,
+                     EasyDelGradientCheckPointers
+                     )
 from datasets import load_dataset
 import flax
 from jax import numpy as jnp
@@ -86,8 +92,9 @@
     num_train_epochs=3,
     learning_rate=5e-5,
     learning_rate_end=1e-6,
-    optimizer='adamw',  # 'adamw', 'lion', 'adafactor' are supported
-    scheduler='linear',  # 'linear','cosine', 'none' ,'warm_up_cosine' and 'warm_up_linear'  are supported
+    optimizer=EasyDelOptimizers.ADAMW,  # 'adamw', 'lion', 'adafactor' are supported
+    scheduler=EasyDelSchedulers.LINEAR,
+    # 'linear','cosine', 'none' ,'warm_up_cosine' and 'warm_up_linear'  are supported
     weight_decay=0.01,
     total_batch_size=64,
     max_steps=None,  # None to let trainer Decide
@@ -95,7 +102,7 @@
     do_eval=False,  # it's optional but supported 
     backend='tpu',  # default backed is set to cpu, so you must define you want to use tpu cpu or gpu
     max_length=max_length,  # Note that you have to change this in the model config too
-    gradient_checkpointing='nothing_saveable',
+    gradient_checkpointing=EasyDelGradientCheckPointers.NOTHING_SAVEABLE,
     sharding_array=(1, -1, 1, 1),  # the way to shard model across gpu,cpu or TPUs using sharding array (1, -1, 1, 1)
     # everything training will be in fully FSDP automatic and share data between devices
     use_pjit_attention_force=False,
@@ -162,7 +169,7 @@
 ):
     print(response[response_printed:], end='')
     response_printed = len(response)
-``` 
+```
 
 ## Contributing
 
Index: .idea/vcs.xml
===================================================================
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
deleted file mode 100644
--- a/.idea/vcs.xml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,6 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<project version="4">
-  <component name="VcsDirectoryMappings">
-    <mapping directory="" vcs="Git" />
-  </component>
-</project>
\ No newline at end of file
Index: .idea/inspectionProfiles/profiles_settings.xml
===================================================================
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
deleted file mode 100644
--- a/.idea/inspectionProfiles/profiles_settings.xml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,6 +0,0 @@
-<component name="InspectionProjectProfileManager">
-  <settings>
-    <option name="USE_PROJECT_PROFILE" value="false" />
-    <version value="1.0" />
-  </settings>
-</component>
\ No newline at end of file
Index: .idea/modules.xml
===================================================================
diff --git a/.idea/modules.xml b/.idea/modules.xml
deleted file mode 100644
--- a/.idea/modules.xml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,8 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<project version="4">
-  <component name="ProjectModuleManager">
-    <modules>
-      <module fileurl="file://$PROJECT_DIR$/.idea/EasyDeL.iml" filepath="$PROJECT_DIR$/.idea/EasyDeL.iml" />
-    </modules>
-  </component>
-</project>
\ No newline at end of file
Index: .idea/misc.xml
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
deleted file mode 100644
--- a/.idea/misc.xml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,7 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<project version="4">
-  <component name="Black">
-    <option name="sdkName" value="Python 3.11 (venv)" />
-  </component>
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.11 (venv)" project-jdk-type="Python SDK" />
-</project>
\ No newline at end of file
Index: docs/lib-python-EasyDel-configs-configs.md
===================================================================
diff --git a/docs/lib-python-EasyDel-configs-configs.md b/docs/lib-python-EasyDel-configs-configs.md
deleted file mode 100644
--- a/docs/lib-python-EasyDel-configs-configs.md	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,2 +0,0 @@
-# configs.configs
-::: lib.python.EasyDel.configs.configs
\ No newline at end of file
Index: .idea/shelf/Uncommitted_changes_before_Update_at_12_16_23,_10_35_PM_[Changes]/shelved.patch
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_Update_at_12_16_23,_10_35_PM_[Changes]/shelved.patch b/.idea/shelf/Uncommitted_changes_before_Update_at_12_16_23,_10_35_PM_[Changes]/shelved.patch
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_Update_at_12_16_23,_10_35_PM_[Changes]/shelved.patch	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,99 +0,0 @@
-Index: lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
-<+>import functools\nimport typing\nfrom typing import Sequence\n\nimport fjformer.attention\nimport flax.core\nfrom jax import numpy as jnp, Array, lax\nfrom jax.experimental.shard_map import shard_map\nfrom jax.sharding import PartitionSpec as PS\nimport jax\nfrom flax import linen as nn\nfrom flax.traverse_util import unflatten_dict, flatten_dict\nfrom flax.core import freeze, unfreeze\nfrom typing import Union, Optional, Tuple\nfrom transformers import PretrainedConfig, FlaxPreTrainedModel\nfrom flax.linen import partitioning as nn_partitioning, combine_masks, dot_product_attention_weights\nfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput\n\nfrom ..flax_modelling_utils import (\n    ACT2FN,\n    with_sharding_constraint,\n    get_gradient_checkpoint_policy,\n    repeat_kv_bnsh,\n    apply_rotary_pos_emb,\n    precompute_freq_cis,\n    JaxBaseClassModel,\n    get_flash_attention,\n    smart_flash_attention, get_dot_general_by_bits\n)\nimport chex\nfrom fjformer.bits import config as q_config, q_flax\n\n\nclass MistralConfig(JaxBaseClassModel):\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=14336,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=8,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096 * 32,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            sliding_window=4096,\n            gradient_checkpointing: str = 'nothing_saveable',\n            use_pjit_attention_force: bool = False,\n            use_flash_attention: bool = False,\n            use_sacn_mlp: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 1024,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            attn_pdrop: float = 0.0,\n            c_max_position_embeddings: int = 4096,\n            freq_max_position_embeddings: int = 4096,\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It allows the class to initialize the attributes of a class.\n        The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n        :param self: Represent the instance of the class\n        :param vocab_size: Define the size of the vocabulary\n        :param hidden_size: Determine the size of the embedding layers\n        :param intermediate_size: Define the size of the intermediate layer in each transformer block\n        :param num_hidden_layers: Determine the number of layers in the encoder and decoder\n        :param num_attention_heads: Determine the number of attention heads in each layer\n        :param num_key_value_heads: Specify the number of heads for key and value\n        :param hidden_act: Specify the activation function used in the hidden layers\n        :param max_position_embeddings: Set the maximum length of the sequence\n        :param initializer_range: Initialize the weights of the model\n        :param rms_norm_eps: Avoid division by zero in the rms normalization\n        :param use_cache: Determine whether to use the cache in the decoder\n        :param pad_token_id: Specify the token id of the padding token\n        :param bos_token_id: Specify the beginning of sentence token id\n        :param eos_token_id: Specify the end of sentence token\n        :param tie_word_embeddings: Tie the word embeddings and the output layer\n        :param rope_theta: Control the number of tokens in a rope\n        :param sliding_window: Control the number of tokens that are processed in parallel\n        :param gradient_checkpointing: str: Specify whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Force the use of pjit attention\n        :param use_flash_attention: bool: Enable the flash attention mechanism\n        :param use_sacn_mlp: bool: Determine whether or not to use the scan_mlp function\n        :param flash_attn_query_chunk_size: int: Determine the number of rows in each chunk\n        :param flash_attn_key_chunk_size: int: Control the size of chunks that are used for the key matrix in flash attention\n        :param scan_mlp_chunk_size: int: Specify the chunk size of the scan mlp\n        :param number_rep_kv: int: Specify the number of times to repeat the key and value vectors\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of tokens in a sequence\n        :param freq_max_position_embeddings: int: Set the maximum number of frequency bins that can be used in the model\n        :param bits: Optional[int]: Specify the number of bits used for quantization\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Specify the names of each axis in the tensor\n        :param &quot;mp&quot;): Define the maximum position embeddings\n        :param **kwargs: Pass a variable number of keyword arguments to a function\n        :param : Define the number of layers in the model\n        :return: An instance of the class\n        \n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n        self.bits = bits\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = True):\n        \"\"\"\n        The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        :param fully_fsdp: bool: Determine whether to use the fully_fsdp partitioning scheme or not\n        :return: A list of tuples\n        \n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PS(\"dp\", \"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS(\"fsdp\", \"dp\")),\n            (\"self_attn/o_proj/kernel\", PS(\"dp\", \"fsdp\")),\n\n            (\"mlp/gate_proj/kernel\", PS(\"fsdp\", \"dp\")),\n            (\"mlp/down_proj/kernel\", PS(\"dp\", \"fsdp\")),\n            (\"mlp/up_proj/kernel\", PS(\"fsdp\", \"dp\")),\n\n            (\"input_layernorm/kernel\", PS(None)),\n            (\"post_attention_layernorm/kernel\", PS(None)),\n\n            (\"model/norm/kernel\", PS(None)),\n            (\"lm_head/kernel\", PS(\"fsdp\", \"dp\")),\n            ('.*', PS(None)),\n        ) if not fully_fsdp else (\n\n            (\"model/embed_tokens/embedding\", PS(\"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS(\"fsdp\")),\n            (\"self_attn/o_proj/kernel\", PS(\"fsdp\")),\n\n            (\"mlp/gate_proj/kernel\", PS(\"fsdp\")),\n            (\"mlp/down_proj/kernel\", PS(\"fsdp\")),\n            (\"mlp/up_proj/kernel\", PS(\"fsdp\")),\n\n            (\"input_layernorm/kernel\", PS(None)),\n            (\"post_attention_layernorm/kernel\", PS(None)),\n\n            (\"model/norm/kernel\", PS(None)),\n            (\"lm_head/kernel\", PS(\"fsdp\")),\n            ('.*', PS('fsdp')),\n        )\n\n    def add_jax_args(self,\n                     gradient_checkpointing: str = 'nothing_saveable',\n                     use_pjit_attention_force: bool = False,\n                     use_flash_attention: bool = False,\n                     use_sacn_mlp: bool = False,\n                     flash_attn_query_chunk_size: int = 1024,\n                     flash_attn_key_chunk_size: int = 1024,\n                     scan_mlp_chunk_size: int = 1024,\n                     number_rep_kv: int = 1,\n                     attn_pdrop: float = 0.0,\n                     c_max_position_embeddings: int = 4096,\n                     freq_max_position_embeddings: int = None,\n                     bits: Optional[int] = None,\n                     **kwargs,\n                     ):\n        \"\"\"\n        The add_jax_args function adds the following arguments to the model:\n\n        :param self: Bind the attributes and methods of a class to an instance of that class\n        :param gradient_checkpointing: str: Determine whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Determine whether to use the pjit_attention_force function\n        :param use_flash_attention: bool: Determine if the flash attention module is used or not\n        :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n        :param flash_attn_query_chunk_size: int: Specify the number of tokens that will be processed at a time\n        :param flash_attn_key_chunk_size: int: Chunk the keys for flash attention\n        :param scan_mlp_chunk_size: int: Chunk the input to the mlp\n        :param number_rep_kv: int: Control the number of times that the key and value vectors are repeated\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of positional embeddings for the causal axis\n        :param freq_max_position_embeddings: int: Set the maximum length of the frequency axis\n        :param bits: Optional[int]: Specify the number of bits to use for quantization\n        :param axis_dims: Sequence[int]: Specify the dimensions of each axis in the tensor\n        :param axis_names: Sequence[str]: Name the axes of the tensors\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Name the axes of the tensor\n        \n        :param backend: typing.Optional[str]: backend to use for model\n        :param : Enable gradient checkpointing\n        :return: A tuple of the following:\n\n        \"\"\"\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n\n\nre_mat = nn_partitioning.remat\n\n\ndef matmul_4d_loop(x, y):\n    \"\"\"Computes the matrix product of two 4D arrays x and y using a loop.\"\"\"\n    result = jnp.zeros(*x.shape[:-2] + x.shape[-2] + y.shape[-1])\n    for i in range(x.shape[0]):\n        for j in range(y.shape[1]):\n            for key in range(x.shape[2]):\n                for l in range(y.shape[3]):\n                    result[i, j, key, l] += x[i, j, key, :] * y[key, l, :, :]\n    return result\n\n\ndef _make_sliding_window_causal_mask(\n        input_ids_shape,\n        dtype: jnp.dtype,\n        past_key_values_length: int = 0,\n        sliding_window: int = 4096,\n):\n    \"\"\"\n    Make causal mask used for sliding window attention\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n\n    tensor = jnp.full(\n        (tgt_len, tgt_len),\n        fill_value=1,\n    )\n    mask = jnp.tril(tensor, 0)\n    mask = jnp.triu(mask, -sliding_window)\n    mask = jnp.log(mask).astype(dtype)\n\n    if past_key_values_length > 0:\n        mask = jnp.concatenate([jnp.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n    return mask[None, None, :, :].repeat(bsz, 0)\n\n\nclass MistralRMSNorm(nn.Module):\n    dim: int\n    eps: float = 1e-6\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n\n    def setup(self) -> None:\n        self.weight = self.param(\n            'kernel',\n            nn.initializers.ones,\n            (self.dim,),\n            self.param_dtype,\n        )\n\n    def _norm(self, x: jnp.ndarray) -> jnp.ndarray:\n        return x * jax.lax.rsqrt(jnp.square(x).mean(-1, keepdims=True) + self.eps)\n\n    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n        x = x.astype(jnp.promote_types(self.dtype, jnp.float32))\n        output = self._norm(x).astype(self.dtype)\n        weight = jnp.asarray(self.weight, self.dtype)\n        return output * weight\n\n\nclass FlaxMistralRotaryEmbedding(nn.Module):\n    dtype: jnp.dtype = jnp.float32\n\n    def __call__(self, key, query, freq_cis, position_ids):\n        sin, cos = freq_cis\n\n        sin = sin[position_ids][:, None, :, :]\n        cos = cos[position_ids][:, None, :, :]\n\n        key = apply_rotary_pos_emb(key, sin, cos)\n        query = apply_rotary_pos_emb(query, sin, cos)\n\n        return query.astype(self.dtype), key.astype(self.dtype)\n\n\nclass FlaxMistralMLP(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            dot_general=get_dot_general_by_bits(self.config.bits)\n        )\n        self.gate_proj = dense(self.config.intermediate_size)\n        self.up_proj = dense(self.config.intermediate_size)\n        self.down_proj = dense(self.config.hidden_size)\n        self.act_fn = ACT2FN[self.config.hidden_act]\n\n    def __call__(self, x: chex.Array):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass FlaxMistralAttention(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            dot_general=get_dot_general_by_bits(self.config.bits)\n        )\n\n        self.q_proj = dense(self.num_heads * self.head_dim)\n        self.k_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.v_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.o_proj = dense(self.hidden_size)\n        self.rotary = FlaxMistralRotaryEmbedding(self.dtype)\n\n    @nn.compact\n    def concatenate_to_cache_(self, query: chex.Array, key: chex.Array, value: chex.Array, attention_mask: chex.Array):\n        is_cache_available = self.has_variable('cache', 'key')\n        key_cache = self.variable('cache', 'key', jnp.zeros, key.shape, key.dtype)\n        value_cache = self.variable('cache', 'value', jnp.zeros, key.shape, value.dtype)\n        index_cache = self.variable('cache', 'index', lambda: jnp.array(0, dtype=jnp.int32))\n        if is_cache_available:\n            *bd, ml, nh, dph = key_cache.value.shape\n            indices = (0,) * len(bd) + (index_cache.value, 0, 0)\n            key = jax.lax.dynamic_update_slice(key_cache.value, key, indices)\n            value = jax.lax.dynamic_update_slice(value_cache.value, value, indices)\n            key_cache.value = key\n            value_cache.value = value\n            num_updated_cache_vector = query.shape[1]\n            index_cache.value = index_cache.value + num_updated_cache_vector\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(ml) < index_cache.value,\n                tuple(bd) + (1, num_updated_cache_vector, ml)\n            )\n            attention_mask = nn.combine_masks(pad_mask, attention_mask)\n        return query, key, value, attention_mask\n\n    @staticmethod\n    def _t(query, key, value):\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def t_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._t(query, key, value)\n        query, key = self.rotary(position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._t(query, key, value)\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n        The __call__ method takes an input tensor (x) and returns an output tensor (y).\n        In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n        :param self: Refer to the object itself\n        :param hidden_state: chex.Array: Pass in the hidden state of the model\n        :param freq_cis: chex.Array: Create the t_rotary variable\n        :param attention_mask: chex.Array: Mask the attention weights\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param position_ids: chex.Array: Specify the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attention weights\n        :return: A tuple of (out, attn_output)\n        \n        \"\"\"\n        batch_size, sequence_length = hidden_state.shape[:2]\n        query, key, value = self.q_proj(hidden_state), self.k_proj(hidden_state), self.v_proj(hidden_state)\n\n        if self.config.use_pjit_attention_force:\n            query = with_sharding_constraint(query, PS('fsdp', 'mp', None))\n            key = with_sharding_constraint(key, PS('fsdp', 'mp', None))\n            value = with_sharding_constraint(value, PS('fsdp', 'mp', None))\n        query, key, value = self.t_rotary(\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            query=query,\n            key=key,\n            value=value,\n            freq_cis=freq_cis,\n            position_ids=position_ids\n        )\n        if self.has_variable('cache', 'key') or init_cache:\n            query, key, value, attention_mask = self.concatenate_to_cache_(query, key, value, attention_mask)\n\n        q_l, k_l = query.shape[1], key.shape[1]\n        if self.has_variable('cache', 'key'):\n            mask_shift: int = self.variables['cache']['index']\n            dl = self.variables['cache']['key'].shape[1]\n            causal_mask = jax.lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, q_l, dl)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :q_l, :k_l]\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n\n        attention_mask = nn.combine_masks(attention_mask, causal_mask)\n\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(query, rtp_axis),\n                k=jnp.transpose(key, rtp_axis),\n                v=jnp.transpose(value, rtp_axis),\n                q_ps=self.config.q_ps,\n                k_ps=self.config.k_ps,\n                v_ps=self.config.v_ps,\n                b_ps=self.config.b_ps,\n                a_ps=self.config.a_ps,\n                bias=attention_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=q_l,\n                kv_seq_len=k_l,\n                attn_pdrop=self.config.attn_pdrop,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n            if self.config.use_shard_map:\n                attn_weights = shard_map(\n                    functools.partial(\n                        dot_product_attention_weights,\n                        dtype=jnp.promote_types(self.dtype, jnp.float32),\n                        deterministic=deterministic,\n                        dropout_rate=self.config.attn_pdrop,\n                        precision=self.precision,\n                    ),\n                    mesh=self.config.jax_mesh(),\n                    in_specs=(\n                        self.config.q_ps,\n                        self.config.k_ps,\n                        self.config.b_ps\n                    ),\n                    out_specs=PS((\"dp\", \"fsdp\"), None, None, None),\n                    check_rep=False\n                )(\n                    query, key, attention_bias\n                )\n            else:\n                attn_weights = dot_product_attention_weights(\n                    query=query,\n                    key=key,\n                    bias=attention_bias,\n                    dtype=jnp.promote_types(self.dtype, jnp.float32),\n                    deterministic=deterministic,\n                    dropout_rate=self.config.attn_pdrop,\n                    precision=self.precision,\n                )\n\n            if self.config.use_pjit_attention_force:\n                attn_weights = with_sharding_constraint(attn_weights, PS((\"dp\", \"fsdp\"), \"mp\", None, None))\n\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n\n        out = self.o_proj(attn_output.reshape(batch_size, sequence_length, self.hidden_size))\n        outputs = (out, attn_weights) if output_attentions else (out,)\n        return outputs\n\n\nclass FlaxMistralDecoderLayer(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        self.self_attn = FlaxMistralAttention(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.mlp = FlaxMistralMLP(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.input_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attention_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_state (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        :param self: Represent the instance of the class\n        :param hidden_state: chex.Array: Represent the input to the encoder layer\n        :param freq_cis: chex.Array: Pass the frequency information to the attention layer\n        :param attention_mask: chex.Array: Mask out the attention weights for certain positions\n        :param causal_mask: chex.Array: Mask the future tokens\n        :param position_ids: chex.Array: Indicate the position of each token in the sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache for the self-attention layer\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :return: A tuple of hidden_state and attention_output\n        \n        \"\"\"\n        residual = hidden_state\n        attention_output = self.self_attn(\n            hidden_state=self.input_layernorm(hidden_state),\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            causal_mask=causal_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions\n        )\n\n        hidden_state = attention_output[0] + residual\n\n        hidden_state = self.mlp(self.post_attention_layernorm(hidden_state)) + hidden_state\n        outputs = (hidden_state,)\n        if output_attentions:\n            outputs += attention_output[1]\n        return outputs\n\n\nclass FlaxMistralPretrainedModel(FlaxPreTrainedModel):\n    config_class = MistralConfig\n    base_model_prefix = 'mistral'\n    module_class: nn.Module = None\n\n    def __init__(self,\n                 config: MistralConfig,\n                 input_shape: Tuple = (1, 1),\n                 seed: int = 0,\n                 dtype: jnp.dtype = jnp.bfloat16,\n                 _do_init: bool = True,\n                 **kwargs\n                 ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: flax.core.FrozenDict = None\n    ) -> flax.core.FrozenDict:\n\n        \"\"\"\n        The init_weights function is used to initialize the weights of a model.\n        It takes in an rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        :param self: Access variables that belong to the class\n        :param rng: jax.random.PRNGKey: Initialize the weights of the model\n        :param input_shape: Tuple: Initialize the input_ids, attention_mask and position_ids\n        :param params: flax.core.FrozenDict: Pass in the parameters of a pre-trained model\n        :return: A frozendict of parameters\n        \n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rng_s,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(rng_s, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask=None,\n            position_ids=None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input sequence to the model\n        :param attention_mask: Mask out the padding tokens\n        :param position_ids: Specify the position of each token in the sequence\n        :param params: dict: Pass in the parameters of the model\n        :param past_key_values: dict: Pass the past key values to the model\n        :param dropout_rng: jax.random.PRNGKey: Pass in a random number generator key to the model\n        :param train: bool: Determine whether to use dropout or not\n        :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n        :param output_hidden_states: Optional[bool]: Determine whether to return the hidden states of all layers\n        :param return_dict: Optional[bool]: Return a dictionary of the outputs\n        :param add_params_field: bool: Add a params field to the inputs dictionary\n        :return: A tuple of (last_hidden_state, past_key_values)\n        \n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            None,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxMistralDecoratorCollection(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        block = FlaxMistralDecoderLayer\n        if self.config.gradient_checkpointing != '':\n            block = re_mat(\n                block,\n                static_argnums=(5, 6, 7),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing\n                )\n            )\n        self.layers = [\n            block(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            ) for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False\n    ):\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n        for layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_state,)\n            output = layer(\n                hidden_state,\n                freq_cis,\n                attention_mask,\n                causal_mask,\n                position_ids,\n                deterministic,\n                init_cache,\n                output_attentions\n            )\n            hidden_state = output[0]\n\n            if output_attentions:\n                output_attentions += (output[1],)\n\n        return hidden_state, all_hidden_states, all_attentions\n\n\nclass FlaxMistralModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n\n        self.layers = FlaxMistralDecoratorCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm = MistralRMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n        self.freq_cis = precompute_freq_cis(\n            max_position_embedding=self.config.freq_max_position_embeddings if self.config.freq_max_position_embeddings is not None else self.config.max_position_embeddings,\n            head_dim=self.config.hidden_size // self.config.num_attention_heads\n        )\n        self.causal_mask = nn.make_causal_mask(jnp.ones((1, self.config.c_max_position_embeddings), dtype='i4'))\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            input_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n\n    ) -> typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n        \"\"\"\n        The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n        The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n\n        :param self: Represent the instance of the class\n        :param input_ids: chex.Array: Pass in the input ids\n        :param attention_mask: chex.Array: Mask out the attention weights for certain tokens\n        :param position_ids: chex.Array: Determine the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param input_embeds: chex.Array: Pass in the embedding of the input_ids\n        :param init_cache: bool: Initialize the cache for the decoder\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :param output_hidden_states: bool: Return all hidden states or just the last one\n        :param return_dict: bool: Return a dictionary of the outputs or not\n        :param : Determine whether the model is in training mode or not\n        :return: A tuple of the hidden states, all hidden states, and attentions\n        \n        \"\"\"\n        if input_embeds is None:\n            input_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n        if attention_mask.ndim == 2:\n            b, s = attention_mask.shape\n            attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n        outputs = self.layers(\n            hidden_state=input_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            freq_cis=self.freq_cis,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            causal_mask=self.causal_mask\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(value for value in outputs if value is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n\n\nclass FlaxMistralModel(FlaxMistralPretrainedModel):\n    module_class = FlaxMistralModule\n\n\nclass FlaxMistralForCausalLMModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model: FlaxMistralModule = FlaxMistralModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n            dot_general=get_dot_general_by_bits(self.config.bits)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            input_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"\n            The __call__ function is the main function of a Flax module. It defines how the model will be called,\n            and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n            as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n            the call function: deterministic (whether to use dropout), input_embeds (if you want to pass your own embeddings),\n            output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n            :param self: Refer to the object itself\n            :param input_ids: chex.Array: Pass in the input tokens\n            :param attention_mask: chex.Array: Mask out the padding tokens\n            :param position_ids: chex.Array: Specify the position of each token in the sequence\n            :param deterministic: bool: Determine whether to use dropout in the model\n            :param input_embeds: chex.Array: Pass in the embeddings of the input tokens\n            :param init_cache: bool: Initialize the cache for the decoder\n            :param output_attentions: bool: Return the attention weights\n            :param output_hidden_states: bool: Return the hidden states of all layers\n            :param return_dict: bool: Return a dictionary of the outputs or just the logits\n            :param : Determine whether to return the logits or not\n            :return: A tuple of (lm_logits, hidden_states, attentions)\n            \n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        if attention_mask is None: attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            input_embeds=input_embeds,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        # lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n\n\nclass FlaxMistralForCausalLM(FlaxMistralPretrainedModel):\n    module_class = FlaxMistralForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = jax.lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    @staticmethod\n    def update_inputs_for_generation(model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py b/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py
---- a/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py	(revision 3031818c8b802088e00f0bddd69481ee48ecc180)
-+++ b/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py	(date 1702753526171)
-@@ -176,22 +176,21 @@
-             ("lm_head/kernel", PS("fsdp", "dp")),
-             ('.*', PS(None)),
-         ) if not fully_fsdp else (
-+            ("model/embed_tokens/embedding", PS(("fsdp", "sp"))),
- 
--            ("model/embed_tokens/embedding", PS("fsdp")),
-+            ("self_attn/(q_proj|k_proj|v_proj)/kernel", PS(("fsdp", "sp"))),
-+            ("self_attn/o_proj/kernel", PS(("fsdp", "sp"))),
- 
--            ("self_attn/(q_proj|k_proj|v_proj)/kernel", PS("fsdp")),
--            ("self_attn/o_proj/kernel", PS("fsdp")),
--
--            ("mlp/gate_proj/kernel", PS("fsdp")),
--            ("mlp/down_proj/kernel", PS("fsdp")),
--            ("mlp/up_proj/kernel", PS("fsdp")),
-+            ("mlp/gate_proj/kernel", PS(("fsdp", "sp"))),
-+            ("mlp/down_proj/kernel", PS(("fsdp", "sp"))),
-+            ("mlp/up_proj/kernel", PS(("fsdp", "sp"))),
- 
-             ("input_layernorm/kernel", PS(None)),
-             ("post_attention_layernorm/kernel", PS(None)),
- 
-             ("model/norm/kernel", PS(None)),
--            ("lm_head/kernel", PS("fsdp")),
--            ('.*', PS('fsdp')),
-+            ("lm_head/kernel", PS(("fsdp", "sp"))),
-+            ('.*', PS(("fsdp", "sp"))),
-         )
- 
-     def add_jax_args(self,
-@@ -348,7 +347,7 @@
-             param_dtype=self.param_dtype,
-             precision=self.precision,
-             kernel_init=nn.initializers.normal(),
--            dot_general=get_dot_general_by_bits(self.config.bits)
-+            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)
-         )
-         self.gate_proj = dense(self.config.intermediate_size)
-         self.up_proj = dense(self.config.intermediate_size)
-@@ -381,7 +380,7 @@
-             param_dtype=self.param_dtype,
-             precision=self.precision,
-             kernel_init=nn.initializers.normal(),
--            dot_general=get_dot_general_by_bits(self.config.bits)
-+            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)
-         )
- 
-         self.q_proj = dense(self.num_heads * self.head_dim)
-@@ -460,9 +459,9 @@
-         query, key, value = self.q_proj(hidden_state), self.k_proj(hidden_state), self.v_proj(hidden_state)
- 
-         if self.config.use_pjit_attention_force:
--            query = with_sharding_constraint(query, PS('fsdp', 'mp', None))
--            key = with_sharding_constraint(key, PS('fsdp', 'mp', None))
--            value = with_sharding_constraint(value, PS('fsdp', 'mp', None))
-+            query = with_sharding_constraint(query, PS("fsdp", "sp", None))
-+            key = with_sharding_constraint(key, PS("fsdp", "sp", None))
-+            value = with_sharding_constraint(value, PS("fsdp", "sp", None))
-         query, key, value = self.t_rotary(
-             batch_size=batch_size,
-             sequence_length=sequence_length,
-@@ -555,7 +554,7 @@
-                         self.config.k_ps,
-                         self.config.b_ps
-                     ),
--                    out_specs=PS(("dp", "fsdp"), None, None, None),
-+                    out_specs=PS(("dp", "fsdp"), "sp", "tp", None),
-                     check_rep=False
-                 )(
-                     query, key, attention_bias
-@@ -572,7 +571,7 @@
-                 )
- 
-             if self.config.use_pjit_attention_force:
--                attn_weights = with_sharding_constraint(attn_weights, PS(("dp", "fsdp"), "mp", None, None))
-+                attn_weights = with_sharding_constraint(attn_weights, PS(("dp", "fsdp"), "sp", "tp", None))
- 
-             attn_output = jnp.einsum("...hqk,...khd->...qhd", attn_weights, value)
- 
-@@ -1027,7 +1026,7 @@
-             use_bias=False,
-             kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),
-             precision=self.precision,
--            dot_general=get_dot_general_by_bits(self.config.bits)
-+            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)
-         )
- 
-     def __call__(
Index: .idea/shelf/Uncommitted_changes_before_Update_at_12_18_23__11_49_AM__Changes_.xml
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_Update_at_12_18_23__11_49_AM__Changes_.xml b/.idea/shelf/Uncommitted_changes_before_Update_at_12_18_23__11_49_AM__Changes_.xml
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_Update_at_12_18_23__11_49_AM__Changes_.xml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,4 +0,0 @@
-<changelist name="Uncommitted_changes_before_Update_at_12_18_23,_11_49_AM_[Changes]" date="1702887584441" recycled="true" deleted="true">
-  <option name="PATH" value="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Update_at_12_18_23,_11_49_AM_[Changes]/shelved.patch" />
-  <option name="DESCRIPTION" value="Uncommitted changes before Update at 12/18/23, 11:49 AM [Changes]" />
-</changelist>
\ No newline at end of file
Index: .idea/inspectionProfiles/Project_Default.xml
===================================================================
diff --git a/.idea/inspectionProfiles/Project_Default.xml b/.idea/inspectionProfiles/Project_Default.xml
deleted file mode 100644
--- a/.idea/inspectionProfiles/Project_Default.xml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,31 +0,0 @@
-<component name="InspectionProjectProfileManager">
-  <profile version="1.0">
-    <option name="myName" value="Project Default" />
-    <inspection_tool class="DuplicatedCode" enabled="true" level="WEAK WARNING" enabled_by_default="true">
-      <Languages>
-        <language minSize="73" name="Python" />
-      </Languages>
-    </inspection_tool>
-    <inspection_tool class="PyPackageRequirementsInspection" enabled="true" level="WARNING" enabled_by_default="true">
-      <option name="ignoredPackages">
-        <value>
-          <list size="5">
-            <item index="0" class="java.lang.String" itemvalue="typing" />
-            <item index="1" class="java.lang.String" itemvalue="ipython" />
-            <item index="2" class="java.lang.String" itemvalue="setuptools" />
-            <item index="3" class="java.lang.String" itemvalue="numpy" />
-            <item index="4" class="java.lang.String" itemvalue="flet" />
-          </list>
-        </value>
-      </option>
-    </inspection_tool>
-    <inspection_tool class="PyPep8NamingInspection" enabled="true" level="WEAK WARNING" enabled_by_default="true">
-      <option name="ignoredErrors">
-        <list>
-          <option value="N803" />
-          <option value="N806" />
-        </list>
-      </option>
-    </inspection_tool>
-  </profile>
-</component>
\ No newline at end of file
Index: .idea/shelf/Uncommitted_changes_before_Checkout_at_12_16_23,_10_35_PM_[Changes]/shelved.patch
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_Checkout_at_12_16_23,_10_35_PM_[Changes]/shelved.patch b/.idea/shelf/Uncommitted_changes_before_Checkout_at_12_16_23,_10_35_PM_[Changes]/shelved.patch
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_Checkout_at_12_16_23,_10_35_PM_[Changes]/shelved.patch	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,27 +0,0 @@
-Index: .idea/workspace.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
-<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"AutoImportSettings\">\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\n  </component>\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"e9058a88-3ea4-4b63-a9f8-ea56f3a88630\" name=\"Changes\" comment=\"\" />\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\n      <map>\n        <entry key=\"$PROJECT_DIR$\" value=\"main\" />\n      </map>\n    </option>\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n  </component>\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProjectColorInfo\">{\n  &quot;customColor&quot;: &quot;&quot;,\n  &quot;associatedIndex&quot;: 6\n}</component>\n  <component name=\"ProjectId\" id=\"2ZXHOtjPa3CdcVdlW1mCznNWdgB\" />\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\">{\n  &quot;keyToString&quot;: {\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\n    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,\n    &quot;git-widget-placeholder&quot;: &quot;beta&quot;,\n    &quot;last_opened_file_path&quot;: &quot;/home/erfan/PycharmProjects/EasyDeL&quot;,\n    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,\n    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,\n    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,\n    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,\n    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,\n    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;\n  }\n}</component>\n  <component name=\"RunManager\" selected=\"Python.env\">\n    <configuration name=\"env\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/lib/python/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/env.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"llama_compration\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/python_test\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/python_test/llama_compration.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <recent_temporary>\n      <list>\n        <item itemvalue=\"Python.env\" />\n        <item itemvalue=\"Python.llama_compration\" />\n        <item itemvalue=\"Python.env\" />\n      </list>\n    </recent_temporary>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"e9058a88-3ea4-4b63-a9f8-ea56f3a88630\" name=\"Changes\" comment=\"\" />\n      <created>1702561475154</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1702561475154</updated>\n      <workItem from=\"1702561476600\" duration=\"2879000\" />\n      <workItem from=\"1702564997855\" duration=\"1585000\" />\n      <workItem from=\"1702570696725\" duration=\"5776000\" />\n      <workItem from=\"1702625797888\" duration=\"1904000\" />\n      <workItem from=\"1702627837344\" duration=\"3368000\" />\n    </task>\n    <servers />\n  </component>\n  <component name=\"TypeScriptGeneratedFilesManager\">\n    <option name=\"version\" value=\"3\" />\n  </component>\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\n    <SUITE FILE_PATH=\"coverage/EasyDeL$env.coverage\" NAME=\"env Coverage Results\" MODIFIED=\"1702644128255\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/lib/python/\" />\n  </component>\n</project>
-===================================================================
-diff --git a/.idea/workspace.xml b/.idea/workspace.xml
---- a/.idea/workspace.xml	
-+++ b/.idea/workspace.xml	
-@@ -4,7 +4,9 @@
-     <option name="autoReloadType" value="SELECTIVE" />
-   </component>
-   <component name="ChangeListManager">
--    <list default="true" id="e9058a88-3ea4-4b63-a9f8-ea56f3a88630" name="Changes" comment="" />
-+    <list default="true" id="e9058a88-3ea4-4b63-a9f8-ea56f3a88630" name="Changes" comment="">
-+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-+    </list>
-     <option name="SHOW_DIALOG" value="false" />
-     <option name="HIGHLIGHT_CONFLICTS" value="true" />
-     <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
-@@ -111,6 +113,7 @@
-       <workItem from="1702570696725" duration="5776000" />
-       <workItem from="1702625797888" duration="1904000" />
-       <workItem from="1702627837344" duration="3368000" />
-+      <workItem from="1702647727341" duration="1293000" />
-     </task>
-     <servers />
-   </component>
Index: .idea/workspace.xml
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
deleted file mode 100644
--- a/.idea/workspace.xml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,190 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<project version="4">
-  <component name="AutoImportSettings">
-    <option name="autoReloadType" value="SELECTIVE" />
-  </component>
-  <component name="ChangeListManager">
-    <list default="true" id="e9058a88-3ea4-4b63-a9f8-ea56f3a88630" name="Changes" comment="" />
-    <option name="SHOW_DIALOG" value="false" />
-    <option name="HIGHLIGHT_CONFLICTS" value="true" />
-    <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
-    <option name="LAST_RESOLUTION" value="IGNORE" />
-  </component>
-  <component name="FileTemplateManagerImpl">
-    <option name="RECENT_TEMPLATES">
-      <list>
-        <option value="Python Script" />
-      </list>
-    </option>
-  </component>
-  <component name="Git.Settings">
-    <option name="RECENT_BRANCH_BY_REPOSITORY">
-      <map>
-        <entry key="$PROJECT_DIR$" value="main" />
-      </map>
-    </option>
-    <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
-  </component>
-  <component name="MarkdownSettingsMigration">
-    <option name="stateVersion" value="1" />
-  </component>
-  <component name="ProjectColorInfo">{
-  &quot;customColor&quot;: &quot;&quot;,
-  &quot;associatedIndex&quot;: 6
-}</component>
-  <component name="ProjectId" id="2ZXHOtjPa3CdcVdlW1mCznNWdgB" />
-  <component name="ProjectLevelVcsManager">
-    <ConfirmationsSetting value="2" id="Add" />
-  </component>
-  <component name="ProjectViewState">
-    <option name="hideEmptyMiddlePackages" value="true" />
-    <option name="showLibraryContents" value="true" />
-  </component>
-  <component name="PropertiesComponent">{
-  &quot;keyToString&quot;: {
-    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
-    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
-    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,
-    &quot;git-widget-placeholder&quot;: &quot;main&quot;,
-    &quot;last_opened_file_path&quot;: &quot;/home/erfan/PycharmProjects/EasyDeL&quot;,
-    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,
-    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,
-    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,
-    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,
-    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,
-    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;
-  }
-}</component>
-  <component name="RunManager" selected="Python.mixtral_flax">
-    <configuration name="env" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
-      <module name="EasyDeL" />
-      <option name="INTERPRETER_OPTIONS" value="" />
-      <option name="PARENT_ENVS" value="true" />
-      <envs>
-        <env name="PYTHONUNBUFFERED" value="1" />
-      </envs>
-      <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/lib/python/" />
-      <option name="IS_MODULE_SDK" value="true" />
-      <option name="ADD_CONTENT_ROOTS" value="true" />
-      <option name="ADD_SOURCE_ROOTS" value="true" />
-      <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/env.py" />
-      <option name="PARAMETERS" value="" />
-      <option name="SHOW_COMMAND_LINE" value="false" />
-      <option name="EMULATE_TERMINAL" value="false" />
-      <option name="MODULE_MODE" value="false" />
-      <option name="REDIRECT_INPUT" value="false" />
-      <option name="INPUT_FILE" value="" />
-      <method v="2" />
-    </configuration>
-    <configuration name="llama_compration" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
-      <module name="EasyDeL" />
-      <option name="INTERPRETER_OPTIONS" value="" />
-      <option name="PARENT_ENVS" value="true" />
-      <envs>
-        <env name="PYTHONUNBUFFERED" value="1" />
-      </envs>
-      <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/python_test" />
-      <option name="IS_MODULE_SDK" value="true" />
-      <option name="ADD_CONTENT_ROOTS" value="true" />
-      <option name="ADD_SOURCE_ROOTS" value="true" />
-      <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/python_test/llama_compration.py" />
-      <option name="PARAMETERS" value="" />
-      <option name="SHOW_COMMAND_LINE" value="false" />
-      <option name="EMULATE_TERMINAL" value="false" />
-      <option name="MODULE_MODE" value="false" />
-      <option name="REDIRECT_INPUT" value="false" />
-      <option name="INPUT_FILE" value="" />
-      <method v="2" />
-    </configuration>
-    <configuration name="llama_flax" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
-      <module name="EasyDeL" />
-      <option name="INTERPRETER_OPTIONS" value="" />
-      <option name="PARENT_ENVS" value="true" />
-      <envs>
-        <env name="PYTHONUNBUFFERED" value="1" />
-      </envs>
-      <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/python_test" />
-      <option name="IS_MODULE_SDK" value="true" />
-      <option name="ADD_CONTENT_ROOTS" value="true" />
-      <option name="ADD_SOURCE_ROOTS" value="true" />
-      <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/python_test/llama_flax.py" />
-      <option name="PARAMETERS" value="" />
-      <option name="SHOW_COMMAND_LINE" value="false" />
-      <option name="EMULATE_TERMINAL" value="false" />
-      <option name="MODULE_MODE" value="false" />
-      <option name="REDIRECT_INPUT" value="false" />
-      <option name="INPUT_FILE" value="" />
-      <method v="2" />
-    </configuration>
-    <configuration name="mixtral_flax" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
-      <module name="EasyDeL" />
-      <option name="INTERPRETER_OPTIONS" value="" />
-      <option name="PARENT_ENVS" value="true" />
-      <envs>
-        <env name="PYTHONUNBUFFERED" value="1" />
-      </envs>
-      <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/python_test" />
-      <option name="IS_MODULE_SDK" value="true" />
-      <option name="ADD_CONTENT_ROOTS" value="true" />
-      <option name="ADD_SOURCE_ROOTS" value="true" />
-      <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/python_test/mixtral_flax.py" />
-      <option name="PARAMETERS" value="" />
-      <option name="SHOW_COMMAND_LINE" value="false" />
-      <option name="EMULATE_TERMINAL" value="false" />
-      <option name="MODULE_MODE" value="false" />
-      <option name="REDIRECT_INPUT" value="false" />
-      <option name="INPUT_FILE" value="" />
-      <method v="2" />
-    </configuration>
-    <recent_temporary>
-      <list>
-        <item itemvalue="Python.mixtral_flax" />
-        <item itemvalue="Python.llama_flax" />
-        <item itemvalue="Python.env" />
-        <item itemvalue="Python.llama_compration" />
-        <item itemvalue="Python.env" />
-      </list>
-    </recent_temporary>
-  </component>
-  <component name="SpellCheckerSettings" RuntimeDictionaries="0" Folders="0" CustomDictionaries="0" DefaultDictionary="application-level" UseSingleDictionary="true" transferred="true" />
-  <component name="TaskManager">
-    <task active="true" id="Default" summary="Default task">
-      <changelist id="e9058a88-3ea4-4b63-a9f8-ea56f3a88630" name="Changes" comment="" />
-      <created>1702561475154</created>
-      <option name="number" value="Default" />
-      <option name="presentableId" value="Default" />
-      <updated>1702561475154</updated>
-      <workItem from="1702561476600" duration="2879000" />
-      <workItem from="1702564997855" duration="1585000" />
-      <workItem from="1702570696725" duration="5776000" />
-      <workItem from="1702625797888" duration="1904000" />
-      <workItem from="1702627837344" duration="3368000" />
-      <workItem from="1702795436603" duration="757000" />
-      <workItem from="1702799337712" duration="11633000" />
-      <workItem from="1702811433240" duration="3570000" />
-      <workItem from="1702830071828" duration="8103000" />
-    </task>
-    <servers />
-  </component>
-  <component name="TypeScriptGeneratedFilesManager">
-    <option name="version" value="3" />
-  </component>
-  <component name="com.intellij.coverage.CoverageDataManagerImpl">
-    <SUITE FILE_PATH="coverage/EasyDeL$generate_documentations.coverage" NAME="generate_documentations Coverage Results" MODIFIED="1702887955454" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/EasyDeL$llama_compration.coverage" NAME="llama_compration Coverage Results" MODIFIED="1701936876438" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/python_test" />
-    <SUITE FILE_PATH="coverage/EasyDeL$env.coverage" NAME="env Coverage Results" MODIFIED="1702474767871" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/lib/python" />
-    <SUITE FILE_PATH="coverage/EasyDeL$mistral_flax.coverage" NAME="mistral_flax Coverage Results" MODIFIED="1701867908351" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/python_test" />
-    <SUITE FILE_PATH="coverage/EasyDeL$llama_flax.coverage" NAME="llama_flax Coverage Results" MODIFIED="1701943680350" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/python_test" />
-    <SUITE FILE_PATH="coverage/EasyDeL$mixtral_flax.coverage" NAME="mixtral_flax Coverage Results" MODIFIED="1702977128554" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/python_test" />
-    <SUITE FILE_PATH="coverage/EasyDeL$falcon_flax.coverage" NAME="falcon_flax Coverage Results" MODIFIED="1701860359932" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/python_test" />
-    <SUITE FILE_PATH="coverage/EasyDeL$phi_flax.coverage" NAME="phi_flax Coverage Results" MODIFIED="1701867632398" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/python_test" />
-  </component>
-</project>
\ No newline at end of file
Index: .idea/EasyDeL.iml
===================================================================
diff --git a/.idea/EasyDeL.iml b/.idea/EasyDeL.iml
deleted file mode 100644
--- a/.idea/EasyDeL.iml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,14 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<module type="PYTHON_MODULE" version="4">
-  <component name="NewModuleRootManager">
-    <content url="file://$MODULE_DIR$">
-      <sourceFolder url="file://$MODULE_DIR$/lib/python" isTestSource="false" />
-    </content>
-    <orderEntry type="inheritedJdk" />
-    <orderEntry type="sourceFolder" forTests="false" />
-  </component>
-  <component name="PyDocumentationSettings">
-    <option name="format" value="PLAIN" />
-    <option name="myDocStringFormat" value="Plain" />
-  </component>
-</module>
\ No newline at end of file
Index: .idea/shelf/Uncommitted_changes_before_Update_at_12_18_23,_11_49_AM_[Changes]/shelved.patch
===================================================================
diff --git a/.idea/shelf/Uncommitted_changes_before_Update_at_12_18_23,_11_49_AM_[Changes]/shelved.patch b/.idea/shelf/Uncommitted_changes_before_Update_at_12_18_23,_11_49_AM_[Changes]/shelved.patch
deleted file mode 100644
--- a/.idea/shelf/Uncommitted_changes_before_Update_at_12_18_23,_11_49_AM_[Changes]/shelved.patch	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ /dev/null	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
@@ -1,65 +0,0 @@
-Index: .idea/workspace.xml
-IDEA additional info:
-Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
-<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"AutoImportSettings\">\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\n  </component>\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"e9058a88-3ea4-4b63-a9f8-ea56f3a88630\" name=\"Changes\" comment=\"\">\n      <change afterPath=\"$PROJECT_DIR$/lib/python/EasyDel/modules/mixtral/__init__.py\" afterDir=\"false\" />\n      <change afterPath=\"$PROJECT_DIR$/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py\" afterDir=\"false\" />\n    </list>\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"FileTemplateManagerImpl\">\n    <option name=\"RECENT_TEMPLATES\">\n      <list>\n        <option value=\"Python Script\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\n      <map>\n        <entry key=\"$PROJECT_DIR$\" value=\"main\" />\n      </map>\n    </option>\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n  </component>\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProjectColorInfo\">{\n  &quot;customColor&quot;: &quot;&quot;,\n  &quot;associatedIndex&quot;: 6\n}</component>\n  <component name=\"ProjectId\" id=\"2ZXHOtjPa3CdcVdlW1mCznNWdgB\" />\n  <component name=\"ProjectLevelVcsManager\">\n    <ConfirmationsSetting value=\"2\" id=\"Add\" />\n  </component>\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\"><![CDATA[{\n  \"keyToString\": {\n    \"RunOnceActivity.OpenProjectViewOnStart\": \"true\",\n    \"RunOnceActivity.ShowReadmeOnStart\": \"true\",\n    \"WebServerToolWindowFactoryState\": \"false\",\n    \"git-widget-placeholder\": \"main\",\n    \"last_opened_file_path\": \"/home/erfan/PycharmProjects/EasyDeL\",\n    \"node.js.detected.package.eslint\": \"true\",\n    \"node.js.detected.package.tslint\": \"true\",\n    \"node.js.selected.package.eslint\": \"(autodetect)\",\n    \"node.js.selected.package.tslint\": \"(autodetect)\",\n    \"nodejs_package_manager_path\": \"npm\",\n    \"vue.rearranger.settings.migration\": \"true\"\n  }\n}]]></component>\n  <component name=\"RunManager\" selected=\"Python.env\">\n    <configuration name=\"env\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/lib/python/\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/env.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <configuration name=\"llama_compration\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\n      <module name=\"EasyDeL\" />\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\n      <option name=\"PARENT_ENVS\" value=\"true\" />\n      <envs>\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\n      </envs>\n      <option name=\"SDK_HOME\" value=\"\" />\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/python_test\" />\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/python_test/llama_compration.py\" />\n      <option name=\"PARAMETERS\" value=\"\" />\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\n      <option name=\"MODULE_MODE\" value=\"false\" />\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\n      <option name=\"INPUT_FILE\" value=\"\" />\n      <method v=\"2\" />\n    </configuration>\n    <recent_temporary>\n      <list>\n        <item itemvalue=\"Python.env\" />\n        <item itemvalue=\"Python.llama_compration\" />\n        <item itemvalue=\"Python.env\" />\n      </list>\n    </recent_temporary>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"e9058a88-3ea4-4b63-a9f8-ea56f3a88630\" name=\"Changes\" comment=\"\" />\n      <created>1702561475154</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1702561475154</updated>\n      <workItem from=\"1702561476600\" duration=\"2879000\" />\n      <workItem from=\"1702564997855\" duration=\"1585000\" />\n      <workItem from=\"1702570696725\" duration=\"5776000\" />\n      <workItem from=\"1702625797888\" duration=\"1904000\" />\n      <workItem from=\"1702627837344\" duration=\"3368000\" />\n    </task>\n    <servers />\n  </component>\n  <component name=\"TypeScriptGeneratedFilesManager\">\n    <option name=\"version\" value=\"3\" />\n  </component>\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\n    <SUITE FILE_PATH=\"coverage/EasyDeL$llama_compration.coverage\" NAME=\"llama_compration Coverage Results\" MODIFIED=\"1701936876438\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$env.coverage\" NAME=\"env Coverage Results\" MODIFIED=\"1702474767871\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/lib/python\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$mistral_flax.coverage\" NAME=\"mistral_flax Coverage Results\" MODIFIED=\"1701867908351\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$llama_flax.coverage\" NAME=\"llama_flax Coverage Results\" MODIFIED=\"1701943680350\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$falcon_flax.coverage\" NAME=\"falcon_flax Coverage Results\" MODIFIED=\"1701860359932\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n    <SUITE FILE_PATH=\"coverage/EasyDeL$phi_flax.coverage\" NAME=\"phi_flax Coverage Results\" MODIFIED=\"1701867632398\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/python_test\" />\n  </component>\n</project>
-Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
-<+>UTF-8
-===================================================================
-diff --git a/.idea/workspace.xml b/.idea/workspace.xml
---- a/.idea/workspace.xml	(revision 75357673a7522f641b09ac8d228f424910c3132d)
-+++ b/.idea/workspace.xml	(date 1702887567226)
-@@ -5,10 +5,7 @@
-   </component>
-   <component name="ChangeListManager">
-     <list default="true" id="e9058a88-3ea4-4b63-a9f8-ea56f3a88630" name="Changes" comment="">
--      <change afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mixtral/__init__.py" afterDir="false" />
--      <change afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mixtral/modelling_mixtral_flax.py" afterDir="false" />
-       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
--      <change beforePath="$PROJECT_DIR$/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py" beforeDir="false" afterPath="$PROJECT_DIR$/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py" afterDir="false" />
-     </list>
-     <option name="SHOW_DIALOG" value="false" />
-     <option name="HIGHLIGHT_CONFLICTS" value="true" />
-@@ -45,21 +42,21 @@
-     <option name="hideEmptyMiddlePackages" value="true" />
-     <option name="showLibraryContents" value="true" />
-   </component>
--  <component name="PropertiesComponent"><![CDATA[{
--  "keyToString": {
--    "RunOnceActivity.OpenProjectViewOnStart": "true",
--    "RunOnceActivity.ShowReadmeOnStart": "true",
--    "WebServerToolWindowFactoryState": "false",
--    "git-widget-placeholder": "main",
--    "last_opened_file_path": "/home/erfan/PycharmProjects/EasyDeL",
--    "node.js.detected.package.eslint": "true",
--    "node.js.detected.package.tslint": "true",
--    "node.js.selected.package.eslint": "(autodetect)",
--    "node.js.selected.package.tslint": "(autodetect)",
--    "nodejs_package_manager_path": "npm",
--    "vue.rearranger.settings.migration": "true"
-+  <component name="PropertiesComponent">{
-+  &quot;keyToString&quot;: {
-+    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
-+    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
-+    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,
-+    &quot;git-widget-placeholder&quot;: &quot;main&quot;,
-+    &quot;last_opened_file_path&quot;: &quot;/home/erfan/PycharmProjects/EasyDeL&quot;,
-+    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,
-+    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,
-+    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,
-+    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,
-+    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,
-+    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;
-   }
--}]]></component>
-+}</component>
-   <component name="RunManager" selected="Python.env">
-     <configuration name="env" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
-       <module name="EasyDeL" />
-@@ -126,6 +123,7 @@
-       <workItem from="1702570696725" duration="5776000" />
-       <workItem from="1702625797888" duration="1904000" />
-       <workItem from="1702627837344" duration="3368000" />
-+      <workItem from="1702886999328" duration="568000" />
-     </task>
-     <servers />
-   </component>
Index: docs/etils-etils.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/docs/etils-etils.md b/docs/etils-etils.md
new file mode 100644
--- /dev/null	(date 1703504083514)
+++ b/docs/etils-etils.md	(date 1703504083514)
@@ -0,0 +1,2 @@
+# etils.etils
+::: lib.python.EasyDel.etils.etils
\ No newline at end of file
Index: docs/etils-configs.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/docs/etils-configs.md b/docs/etils-configs.md
new file mode 100644
--- /dev/null	(date 1703504083514)
+++ b/docs/etils-configs.md	(date 1703504083514)
@@ -0,0 +1,2 @@
+# etils.configs
+::: lib.python.EasyDel.etils.configs
\ No newline at end of file
Index: python_test/mixtral_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import copy\n\nimport os\n\nos.environ[\"JAX_TRACEBACK_FILTERING\"] = 'off'\nimport jax\n\ntry:\n    from lib.python.EasyDel import MixtralConfig, FlaxMixtralForCausalLM\n    from lib.python.EasyDel.transform.easydel_transform import huggingface_to_easydel\nexcept ModuleNotFoundError:\n    import sys\n    from pathlib import Path\n\n    cp = Path.cwd().__str__()\n    sys.path.append(cp)\n    from lib.python.EasyDel import MixtralConfig, FlaxMixtralForCausalLM\n    from lib.python.EasyDel.transform.easydel_transform import huggingface_to_easydel\nfrom jax import numpy as jnp\nfrom transformers import MixtralForCausalLM\nimport torch\nimport numpy as np\n\n\ndef main():\n    torch.manual_seed(42)\n    seq_len = 128\n    config = MixtralConfig(\n        hidden_size=128,\n        num_attention_heads=8,\n        num_key_value_heads=4,\n        num_hidden_layers=1,\n        intermediate_size=256,\n        gradient_checkpointing='',\n        max_position_embeddings=seq_len\n    )\n    batch_size = len(jax.devices())\n\n    torch_model = MixtralForCausalLM(\n        config=copy.deepcopy(config)\n    )\n    params = {\"params\":\n        huggingface_to_easydel(\n            torch_model.state_dict(),\n            embedding_layer_names=[\"embed_tokens\"],\n            device=jax.devices('cpu')[0]\n        )\n    }\n\n    np_random_input_ids = np.random.randint(0, config.vocab_size, (batch_size, seq_len))\n    input_ids = torch.from_numpy(np_random_input_ids).reshape(batch_size, -1).to(torch.long)\n    flax_input_ids = jnp.asarray(np_random_input_ids, dtype=jnp.int32).reshape(batch_size, -1)\n    torch_output = torch_model(\n        input_ids=input_ids\n    )\n    torch_output = torch_output.logits.cpu().detach().numpy()\n    config.add_jax_args()\n    config.add_partitions(\n        use_shard_map=True\n    )\n    try:\n        flax_model = FlaxMixtralForCausalLM(\n            config=config,\n            dtype=jnp.float32,\n            param_dtype=jnp.float32,\n            _do_init=False,\n            input_shape=(batch_size, seq_len)\n        )\n        flax_output = flax_model(\n            input_ids=flax_input_ids,\n            params=params,\n\n        )\n        res = jnp.allclose(torch_output, flax_output.logits, atol=1e-5)\n        print('Mistral Huggingface Predictions :\\n', torch_output,\n              '\\nEasyDel Predictions: \\n', flax_output.logits)\n        if res:  # A Little Bit of humor\n            print('\\033[1;36mTest Passed Unfortunately \uD83E\uDD73')\n        else:\n            print('\\033[1;31mTest Failed Successfully  \uD83E\uDD15')\n        error = jnp.mean(torch_output - flax_output.logits)\n        print(\"Error : \", error)\n    except TypeError as e:\n        print(e.__str__())\n\n\nif __name__ == '__main__':\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/python_test/mixtral_flax.py b/python_test/mixtral_flax.py
--- a/python_test/mixtral_flax.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/python_test/mixtral_flax.py	(date 1703504083518)
@@ -58,6 +58,7 @@
     config.add_partitions(
         use_shard_map=True
     )
+    print(params['params']['model']['layers']["0"])
     try:
         flax_model = FlaxMixtralForCausalLM(
             config=config,
Index: docs/etils-errors.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/docs/etils-errors.md b/docs/etils-errors.md
new file mode 100644
--- /dev/null	(date 1703504083514)
+++ b/docs/etils-errors.md	(date 1703504083514)
@@ -0,0 +1,2 @@
+# etils.errors
+::: lib.python.EasyDel.etils.errors
\ No newline at end of file
Index: lib/python/EasyDel/rl_trainer/models/modelling_value_head.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import typing\n\nimport jax.lax\nfrom flax import linen as nn\nfrom transformers import FlaxPreTrainedModel\nfrom EasyDel.modules import FlaxLlamaForCausalLM\nfrom typing import Type\n\nfrom transformers.modeling_flax_outputs import FlaxCausalLMOutput\nfrom ...modules.auto_models import AutoEasyDelModelForCausalLM\nfrom .modelling_base import FlaxPreTrainedModelWrapper\nfrom jax import numpy as jnp\nimport chex\n\n\nclass ValueHead(nn.Module):\n    config: typing.Any\n    summary_dropout_prob: float = 0.0\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: typing.Optional[jax.lax.Precision] = jax.lax.Precision('fastest')\n\n    def setup(self):\n        \"\"\"\n        The setup function is called by the model's constructor.\n        It initializes all the layers in your model, and assigns them to member variables.\n        The setup function should be used for any initialization that needs to happen before running forward().\n        This includes things like loading weights from a file, or setting up an optimizer.\n\n        :param self: Represent the instance of the class\n        :return: A tuple of the following:\n        \n        \"\"\"\n        config = self.config\n\n        self.dropout = nn.Dropout(self.summary_dropout_prob)\n\n        if hasattr(config, \"hidden_size\"):\n            hidden_size = config.hidden_size\n        if hasattr(config, \"word_embed_proj_dim\"):\n            hidden_size = config.word_embed_proj_dim\n        elif hasattr(config, \"is_encoder_decoder\"):\n            if config.is_encoder_decoder and hasattr(config, \"decoder\"):\n                if hasattr(config.decoder, \"hidden_size\"):\n                    hidden_size = config.decoder.hidden_size\n\n        self.summary = nn.Dense(\n            1,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(self, hidden_states: chex.Array, deterministic: bool = True):\n        \"\"\"\n        The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, e.g., x(arg).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        :param self: Represent the instance of the class\n        :param hidden_states: chex.Array: Pass the hidden states of the previous layer\n        :param deterministic: bool: Determine whether to use dropout\n        :return: A tensor of shape (batch_size, num_classes)\n        \n        \"\"\"\n        output = self.dropout(hidden_states, deterministic=deterministic)\n        if output.dtype != self.summary.weight.dtype:\n            output = output.to(self.summary.weight.dtype)\n        return self.summary(output)\n\n\nclass FlaxAutoModelForCausalLMWithValueHead(FlaxPreTrainedModelWrapper):\n    pretrained_model: Type[FlaxPreTrainedModel] = FlaxLlamaForCausalLM\n    transformers_parent_class: Type[FlaxPreTrainedModel] = AutoEasyDelModelForCausalLM\n    lm_head_namings = [\"lm_head\", \"embed_out\"]\n    supported_args = (\n        \"summary_dropout_prob\",\n        \"v_head_initializer_range\",\n        \"v_head_init_strategy\",\n    )\n\n    def setup(self):\n        if not any(hasattr(self.pretrained_model, attribute) for attribute in self.lm_head_namings):\n            raise ValueError(\"The model does not have a language model head, please use a model that has one.\")\n\n        self.v_head = ValueHead(self.pretrained_model.config)\n\n    def __call__(\n            self,\n            input_ids=None,\n            past_key_values=None,\n            attention_mask=None,\n            **kwargs,\n    ):\n        \"\"\"\n        The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and past_key_values as arguments.\n        The output is a tuple containing lm logits and value.\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input to the model\n        :param past_key_values: Pass the past key values to the model\n        :param attention_mask: Mask out the padding tokens\n        :param **kwargs: Pass in the past_key_values parameter\n        :param : Pass the past key values to the model\n        :return: The logits and the value\n        \n        \"\"\"\n        kwargs[\"output_hidden_states\"] = True\n        kwargs[\"past_key_values\"] = past_key_values\n\n        base_model_output: FlaxCausalLMOutput = self.pretrained_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            **kwargs,\n        )\n\n        last_hidden_state = base_model_output.hidden_states[-1]\n        lm_logits = base_model_output.logits\n\n        value = self.v_head(last_hidden_state).squeeze(-1)\n\n        return lm_logits, value\n\n    def generate(self, *args, **kwargs):\n        return self.pretrained_model.generate(*args, **kwargs)\n\n    def push_to_hub(self, *args, **kwargs):\n        \"\"\"\n        The push_to_hub function is used to push the model to a remote location.\n\n        :param self: Represent the instance of the class\n        :param *args: Send a non-keyworded variable length argument list to the function\n        :param **kwargs: Pass keyworded, variable-length argument list to a function\n        :return: The pretrained model\n        \n        \"\"\"\n        setattr(self.pretrained_model, \"v_head\", self.v_head)\n\n        return self.pretrained_model.push_to_hub(*args, **kwargs)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/rl_trainer/models/modelling_value_head.py b/lib/python/EasyDel/rl_trainer/models/modelling_value_head.py
--- a/lib/python/EasyDel/rl_trainer/models/modelling_value_head.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/rl_trainer/models/modelling_value_head.py	(date 1703504083518)
@@ -11,6 +11,8 @@
 from .modelling_base import FlaxPreTrainedModelWrapper
 from jax import numpy as jnp
 import chex
+import flax
+from flax.traverse_util import unflatten_dict, flatten_dict
 
 
 class ValueHead(nn.Module):
@@ -18,7 +20,8 @@
     summary_dropout_prob: float = 0.0
     dtype: jnp.dtype = jnp.float32
     param_dtype: jnp.dtype = jnp.float32
-    precision: typing.Optional[jax.lax.Precision] = jax.lax.Precision('fastest')
+    precision: typing.Optional[jax.lax.Precision] = jax.lax.Precision(
+        "fastest")
 
     def setup(self):
         """
@@ -29,7 +32,7 @@
 
         :param self: Represent the instance of the class
         :return: A tuple of the following:
-        
+
         """
         config = self.config
 
@@ -61,7 +64,7 @@
         :param hidden_states: chex.Array: Pass the hidden states of the previous layer
         :param deterministic: bool: Determine whether to use dropout
         :return: A tensor of shape (batch_size, num_classes)
-        
+
         """
         output = self.dropout(hidden_states, deterministic=deterministic)
         if output.dtype != self.summary.weight.dtype:
@@ -69,7 +72,7 @@
         return self.summary(output)
 
 
-class FlaxAutoModelForCausalLMWithValueHead(FlaxPreTrainedModelWrapper):
+class FlaxAutoModelForCausalLMWithValueHead(FlaxPreTrainedModelWrapper,flax.linen.Module):
     pretrained_model: Type[FlaxPreTrainedModel] = FlaxLlamaForCausalLM
     transformers_parent_class: Type[FlaxPreTrainedModel] = AutoEasyDelModelForCausalLM
     lm_head_namings = ["lm_head", "embed_out"]
@@ -78,34 +81,54 @@
         "v_head_initializer_range",
         "v_head_init_strategy",
     )
+    summary_dropout_prob: float = 0.0
+    dtype: jnp.dtype = jnp.float32
+    param_dtype: jnp.dtype = jnp.float32
+    precision: typing.Optional[jax.lax.Precision] = jax.lax.Precision(
+        "fastest")
 
     def setup(self):
         if not any(hasattr(self.pretrained_model, attribute) for attribute in self.lm_head_namings):
-            raise ValueError("The model does not have a language model head, please use a model that has one.")
+            raise ValueError(
+                "The model does not have a language model head, please use a model that has one.")
 
-        self.v_head = ValueHead(self.pretrained_model.config)
+        self.v_head = ValueHead(
+            self.pretrained_model.config,
+            dtype=self.dtype,
+            param_dtype=self.param_dtype,
+            precision=self.precision,
+        )
 
     def __call__(
+
             self,
+            module_params: dict,
             input_ids=None,
             past_key_values=None,
             attention_mask=None,
             **kwargs,
     ):
         """
-        The __call__ function is the main function of a Flax model.
-        It takes in input_ids, attention_mask, and past_key_values as arguments.
-        The output is a tuple containing lm logits and value.
+        The function takes in module parameters, input IDs, past key values, and attention mask, and
+        returns language model logits and a value.
 
-        :param self: Represent the instance of the class
-        :param input_ids: Pass the input to the model
-        :param past_key_values: Pass the past key values to the model
-        :param attention_mask: Mask out the padding tokens
-        :param **kwargs: Pass in the past_key_values parameter
-        :param : Pass the past key values to the model
-        :return: The logits and the value
-        
+        :param module_params: The `module_params` parameter is a dictionary that contains the parameters
+        for the module. It is used to pass any additional parameters that are specific to the module
+        being called
+        :type module_params: dict
+        :param input_ids: The `input_ids` parameter is used to specify the input sequence of token IDs
+        for the model. These token IDs represent the input text that the model will process
+        :param past_key_values: The `past_key_values` parameter is used for autoregressive decoding. It
+        allows you to pass the previous key-value pairs from the attention mechanism to the model, which
+        can be used to generate the next token in the sequence. This is useful when generating text one
+        token at a time
+        :param attention_mask: The `attention_mask` parameter is used to mask certain tokens in the
+        input sequence. It is a binary tensor of shape `(batch_size, sequence_length)` where each
+        element is either 0 or 1. A value of 0 indicates that the corresponding token should be masked,
+        while a value of
+        :return: two values: `lm_logits` and `value`.
         """
+
         kwargs["output_hidden_states"] = True
         kwargs["past_key_values"] = past_key_values
 
@@ -113,29 +136,34 @@
             input_ids=input_ids,
             attention_mask=attention_mask,
             return_dict=True,
+            params=module_params,
             **kwargs,
         )
 
         last_hidden_state = base_model_output.hidden_states[-1]
         lm_logits = base_model_output.logits
 
-        value = self.v_head(last_hidden_state).squeeze(-1)
+        value = self.v_head(last_hidden_state)
 
         return lm_logits, value
 
     def generate(self, *args, **kwargs):
-        return self.pretrained_model.generate(*args, **kwargs)
+        raise NotImplementedError()
 
     def push_to_hub(self, *args, **kwargs):
-        """
-        The push_to_hub function is used to push the model to a remote location.
+        raise NotImplementedError()
 
-        :param self: Represent the instance of the class
-        :param *args: Send a non-keyworded variable length argument list to the function
-        :param **kwargs: Pass keyworded, variable-length argument list to a function
-        :return: The pretrained model
-        
-        """
-        setattr(self.pretrained_model, "v_head", self.v_head)
-
-        return self.pretrained_model.push_to_hub(*args, **kwargs)
+    def post_init(self, params: dict, config) -> dict:
+        self.setup()
+        has_v_head = True in set(
+            [key for key, _ in flatten_dict(params).items()]
+        )
+        if not has_v_head:
+            vh_params = self.v_head.init(
+                jnp.ones((1, config.vocab_size))
+            )
+            print(
+                vh_params
+            )
+            
+        return params
Index: lib/python/EasyDel/rl_trainer/models/modelling_base.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import json\nimport logging\nimport os\nimport jax\n# from jax import numpy as jnp, lax\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub.utils import EntryNotFoundError, HFValidationError, LocalEntryNotFoundError\nfrom transformers import FlaxPreTrainedModel\nfrom flax import linen as nn\nfrom flax.serialization import from_bytes\n# from flax.traverse_util import flatten_dict, unflatten_dict\nimport msgpack\nfrom safetensors.torch import load_file\n\nLAYER_PATTERNS = [\n    \"transformer.h.{layer}\",\n    \"model.decoder.layers.{layer}\",\n    \"gpt_neox.layers.{layer}\",\n    \"model.layers.{layer}\",\n]\n\n\ndef match_keywords(string, ts, ns):\n    for t in ts:\n        if t not in string:\n            return False\n    for n in ns:\n        if n in string:\n            return False\n    return True\n\n\nclass FlaxPreTrainedModelWrapper(nn.Module):\n    pretrained_model: FlaxPreTrainedModel\n    transformers_parent_class = None\n    supported_args = None\n    supported_modules = (\"v_head\",)\n    supported_rm_modules = (\"score\",)\n    supported_pretrained_model_architectures = FlaxPreTrainedModel\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, from_pt: bool = True, *model_args, **kwargs):\n\n        \"\"\"\n        The from_pretrained function is used to instantiate a model from a pretrained checkpoint.\n\n        :param cls: Refer to the class that called this function\n        :param pretrained_model_name_or_path: Specify the path to the pretrained model\n        :param from_pt: bool: Determine whether to load the model from a pytorch checkpoint or not\n        :param model_args: Pass the positional arguments of the model\n        :param kwargs: Pass keyworded, variable-length argument list\n        :return: A model with the state_dict loaded from a file\n        \n        \"\"\"\n        if kwargs is not None:\n            reward_adapter = kwargs.pop(\"reward_adapter\", None)\n            trl_model_args, pretrained_kwargs, peft_quantization_kwargs = cls._split_kwargs(kwargs)\n            token = pretrained_kwargs.get(\"token\", None)\n        else:\n            reward_adapter = None\n            trl_model_args = {}\n            pretrained_kwargs = {}\n            token = None\n\n        if reward_adapter is not None and not isinstance(reward_adapter, str):\n            raise ValueError(\n                \"The `reward_adapter` argument should be \"\n                \"a string representing the name of local path or the\"\n                \" Hub id to the Reward Modeling adapter.\"\n            )\n\n        if isinstance(pretrained_model_name_or_path, str):\n            pretrained_model = cls.transformers_parent_class.from_pretrained(\n                pretrained_model_name_or_path, *model_args, **pretrained_kwargs\n            )\n\n        elif isinstance(pretrained_model_name_or_path, cls.supported_pretrained_model_architectures):\n            pretrained_model = pretrained_model_name_or_path\n\n        else:\n            raise ValueError(\n                \"pretrained_model_name_or_path should be a string or a PreTrainedModel, \"\n                f\"but is {type(pretrained_model_name_or_path)}\"\n            )\n\n        model = cls(pretrained_model, **trl_model_args)\n        is_resuming_training = True\n        if isinstance(pretrained_model_name_or_path, str):\n            safe_filename = os.path.join(pretrained_model_name_or_path, \"model.safetensors\")\n            filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")\n\n            sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin.index.json\")\n            safe_sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"model.safetensors.index.json\")\n            is_sharded = False\n            use_safe = os.path.exists(safe_filename)\n\n            if not (os.path.exists(filename) or os.path.exists(safe_filename)):\n                filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(\n                    pretrained_model,\n                    pretrained_model_name_or_path,\n                    sharded_index_filename,\n                    token=token,\n                )\n                if filename is None and files_to_download is None:\n                    safe_filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(\n                        pretrained_model,\n                        pretrained_model_name_or_path,\n                        safe_sharded_index_filename,\n                        token=token,\n                        model_name=\"model.safetensors\",\n                        model_index_name=\"model.safetensors.index.json\",\n                    )\n                    use_safe = True\n                else:\n                    use_safe = False\n            if from_pt:\n                loading_func = load_file\n                load_kwargs = {}\n            else:\n                def loading_func(file_name: str, *args, **kwargs_):\n                    tensors = {}\n                    with open(file_name, 'rb') as stream:\n                        unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n                        for key, value in unpacker:\n                            key = tuple(key)\n                            tensor = from_bytes(None, value)\n                            tensors[key] = tensor\n                    return tensors\n\n            if is_resuming_training:\n                if is_sharded:\n                    state_dict = {}\n\n                    for shard_file in files_to_download:\n                        filename = hf_hub_download(\n                            pretrained_model_name_or_path,\n                            shard_file,\n                            token=token,\n                        )\n                        state_dict.update(loading_func(filename, **load_kwargs))\n                else:\n                    state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n\n        else:\n            state_dict = pretrained_model_name_or_path.state_dict()\n\n        if from_pt:\n            lw = len('.weight')\n            with jax.default_device(cls._get_current_device()):\n                flax_dict = {}\n                for key, tensor in state_dict.items():\n                    if match_keywords(key, ['kernel'], ['none']):\n                        if len(tensor.shape) == 2:\n                            tensor = tensor.transpose(0, 1)\n                    if key.endswith('.weight'):\n                        key = key[:-lw] + '.kernel'\n                    key_tuple = key.split('.')\n                    key_names = ()\n                    tensor = tensor.detach().cpu().numpy()\n                    for k in key_tuple:\n                        key_names += k,\n                    flax_dict[key_names] = tensor\n\n        model.is_peft_model = False\n        model.current_device = cls._get_current_device()\n\n        if is_resuming_training:\n            model.post_init(state_dict=state_dict)\n\n        model.supports_rm_adapter = False\n\n        return model\n\n    @classmethod\n    def _get_checkpoint_from_hub(\n            cls,\n            pretrained_model,\n            pretrained_model_name_or_path,\n            index_filename,\n            token=None,\n            model_name=\"pytorch_model.bin\",\n            model_index_name=\"pytorch_model.bin.index.json\",\n    ):\n        \"\"\"\n        The _get_checkpoint_from_hub function is used to download a pretrained model from the Hugging Face Hub.\n        It will first attempt to download the entire model, and if that fails it will try downloading just the v_head weights.\n        If neither of those attempts succeed, it will return None for all outputs.\n\n        :param cls: Specify the class of the model\n        :param pretrained_model: Load the pretrained model\n        :param pretrained_model_name_or_path: Load the pretrained model from a checkpoint\n        :param index_filename: Load the index file for sharded models\n        :param token: Authenticate with the hugging face model hub\n        :param model_name: Specify the name of the model file to be downloaded\n        :param model_index_name: Specify the name of the index file\n        :param : Load the pretrained model\n        :return: A tuple of four elements:\n        \n        \"\"\"\n        files_to_download = None\n        filename = None\n        is_resuming_training = True\n        is_sharded = False\n\n        try:\n            filename = hf_hub_download(\n                pretrained_model_name_or_path,\n                model_name,\n                token=token,\n            )\n        # sharded\n        except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):\n            index_file_name = ''\n            if os.path.exists(index_filename):\n                index_file_name = index_filename\n            else:\n                try:\n                    index_file_name = hf_hub_download(\n                        pretrained_model_name_or_path,\n                        model_index_name,\n                        token=token,\n                    )\n                except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):\n                    # not continue training, do not have v_head weight\n                    is_resuming_training = False\n                    logging.warning(\n                        f\"A {type(pretrained_model)} model is loaded from '{pretrained_model_name_or_path}', \"\n                        f\"and no v_head weight is found. This IS expected if you are not resuming PPO training.\"\n                    )\n            # load json\n            if is_resuming_training:\n                with open(index_file_name, \"r\") as f:\n                    index = json.load(f)\n                files_to_download = set()\n                for k, v in index[\"weight_map\"].items():\n                    if any([module in k for module in cls.supported_modules]):\n                        files_to_download.add(v)\n                is_sharded = True\n\n        return filename, files_to_download, is_sharded, is_resuming_training\n\n    @classmethod\n    def _get_current_device(cls):\n        \"\"\"\n        The _get_current_device function is a class method that returns the current device.\n\n        :param cls: Indicate that the function is a method of the class\n        :return: The current device\n        \n        \"\"\"\n        return jax.devices()[0]\n\n    @classmethod\n    def _split_kwargs(cls, kwargs):\n        \"\"\"\n        The _split_kwargs function is used to split the kwargs into three categories:\n            1. supported_kwargs - These are the arguments that are supported by this class and will be passed on to the parent class.\n            2. unsupported_kwargs - These are arguments that aren't supported by this class, but may be useful for other classes in a chain of inheritance (e.g., if you're using multiple mixins).\n            3. peft_kwargs - These are arguments specific to PEFT and will not be passed on to any other classes.\n\n        :param cls: Refer to the class itself\n        :param kwargs: Pass keyword arguments to the function\n        :return: A tuple of three dictionaries\n        \n        \"\"\"\n        supported_kwargs = {}\n        unsupported_kwargs = {}\n        peft_kwargs = {}\n\n        for key, value in kwargs.items():\n            if key in cls.supported_args:\n                supported_kwargs[key] = value\n            else:\n                unsupported_kwargs[key] = value\n\n        return supported_kwargs, unsupported_kwargs, peft_kwargs\n\n    def push_to_hub(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def save_pretrained(self, *args, **kwargs):\n        state_dict = kwargs.get(\"state_dict\")\n        if state_dict is None:\n            state_dict = self.state_dict()\n            kwargs[\"state_dict\"] = state_dict\n\n        return self.pretrained_model.save_pretrained(*args, **kwargs)\n\n    def state_dict(self, *args, **kwargs):\n        r\"\"\"\n        Return the state_dict of the pretrained model.\n        \"\"\"\n        raise NotImplementedError\n\n    def post_init(self, *args, **kwargs):\n        r\"\"\"\n        Post initialization method. This method is called after the model is\n        instantiated and loaded from a checkpoint. It can be used to perform\n        additional operations such as loading the state_dict.\n        \"\"\"\n        raise NotImplementedError\n\n    def compute_reward_score(self, input_ids, attention_mask=None, ppo_adapter_name=\"default\", **kwargs):\n\n        \"\"\"\n        The compute_reward_score function is used to compute the reward score for a given input.\n        The function takes in an input_ids tensor and returns a tensor of scores. The shape of the returned\n        tensor will be (batch_size, sequence_length). The higher the score, the more likely that token should be kept.\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input tokens to the model\n        :param attention_mask: Indicate which tokens are padding\n        :param ppo_adapter_name: Set the adapter back to its original state\n        :param **kwargs: Pass a variable number of arguments to a function\n        :return: The scores for the given input_ids\n        \n        \"\"\"\n        if not self.supports_rm_adapter:\n            raise ValueError(\"This model does not support reward modeling adapter.\")\n\n        # enable rm adapter\n        self.pretrained_model.set_adapter(self.rm_adapter_name)\n        self.pretrained_model.eval()\n\n        base_model_output = self.pretrained_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True,\n            return_dict=True,\n            **kwargs,\n        )\n\n        last_hidden_states = base_model_output.hidden_states[-1]\n        scores = self.score(last_hidden_states)\n\n        self.pretrained_model.set_adapter(ppo_adapter_name)\n        self.pretrained_model.train()\n\n        return scores\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/rl_trainer/models/modelling_base.py b/lib/python/EasyDel/rl_trainer/models/modelling_base.py
--- a/lib/python/EasyDel/rl_trainer/models/modelling_base.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/rl_trainer/models/modelling_base.py	(date 1703504083518)
@@ -1,6 +1,7 @@
 import json
 import logging
 import os
+from typing import Sequence, Tuple
 import jax
 # from jax import numpy as jnp, lax
 from huggingface_hub import hf_hub_download
@@ -11,6 +12,10 @@
 # from flax.traverse_util import flatten_dict, unflatten_dict
 import msgpack
 from safetensors.torch import load_file
+from jax import numpy as jnp
+from ...transform.easydel_transform import huggingface_to_easydel
+from ...modules.auto_models import AutoEasyDelModelForCausalLM
+
 
 LAYER_PATTERNS = [
     "transformer.h.{layer}",
@@ -32,144 +37,66 @@
 
 class FlaxPreTrainedModelWrapper(nn.Module):
     pretrained_model: FlaxPreTrainedModel
-    transformers_parent_class = None
+    transformers_parent_class = AutoEasyDelModelForCausalLM
     supported_args = None
     supported_modules = ("v_head",)
     supported_rm_modules = ("score",)
     supported_pretrained_model_architectures = FlaxPreTrainedModel
 
     @classmethod
-    def from_pretrained(cls, pretrained_model_name_or_path, from_pt: bool = True, *model_args, **kwargs):
-
-        """
-        The from_pretrained function is used to instantiate a model from a pretrained checkpoint.
-
-        :param cls: Refer to the class that called this function
-        :param pretrained_model_name_or_path: Specify the path to the pretrained model
-        :param from_pt: bool: Determine whether to load the model from a pytorch checkpoint or not
-        :param model_args: Pass the positional arguments of the model
-        :param kwargs: Pass keyworded, variable-length argument list
-        :return: A model with the state_dict loaded from a file
-        
-        """
-        if kwargs is not None:
-            reward_adapter = kwargs.pop("reward_adapter", None)
-            trl_model_args, pretrained_kwargs, peft_quantization_kwargs = cls._split_kwargs(kwargs)
-            token = pretrained_kwargs.get("token", None)
-        else:
-            reward_adapter = None
-            trl_model_args = {}
-            pretrained_kwargs = {}
-            token = None
-
-        if reward_adapter is not None and not isinstance(reward_adapter, str):
-            raise ValueError(
-                "The `reward_adapter` argument should be "
-                "a string representing the name of local path or the"
-                " Hub id to the Reward Modeling adapter."
-            )
-
-        if isinstance(pretrained_model_name_or_path, str):
-            pretrained_model = cls.transformers_parent_class.from_pretrained(
-                pretrained_model_name_or_path, *model_args, **pretrained_kwargs
-            )
-
-        elif isinstance(pretrained_model_name_or_path, cls.supported_pretrained_model_architectures):
-            pretrained_model = pretrained_model_name_or_path
-
-        else:
-            raise ValueError(
-                "pretrained_model_name_or_path should be a string or a PreTrainedModel, "
-                f"but is {type(pretrained_model_name_or_path)}"
-            )
-
-        model = cls(pretrained_model, **trl_model_args)
-        is_resuming_training = True
-        if isinstance(pretrained_model_name_or_path, str):
-            safe_filename = os.path.join(pretrained_model_name_or_path, "model.safetensors")
-            filename = os.path.join(pretrained_model_name_or_path, "pytorch_model.bin")
-
-            sharded_index_filename = os.path.join(pretrained_model_name_or_path, "pytorch_model.bin.index.json")
-            safe_sharded_index_filename = os.path.join(pretrained_model_name_or_path, "model.safetensors.index.json")
-            is_sharded = False
-            use_safe = os.path.exists(safe_filename)
-
-            if not (os.path.exists(filename) or os.path.exists(safe_filename)):
-                filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(
-                    pretrained_model,
-                    pretrained_model_name_or_path,
-                    sharded_index_filename,
-                    token=token,
-                )
-                if filename is None and files_to_download is None:
-                    safe_filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(
-                        pretrained_model,
-                        pretrained_model_name_or_path,
-                        safe_sharded_index_filename,
-                        token=token,
-                        model_name="model.safetensors",
-                        model_index_name="model.safetensors.index.json",
-                    )
-                    use_safe = True
-                else:
-                    use_safe = False
-            if from_pt:
-                loading_func = load_file
-                load_kwargs = {}
-            else:
-                def loading_func(file_name: str, *args, **kwargs_):
-                    tensors = {}
-                    with open(file_name, 'rb') as stream:
-                        unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)
-                        for key, value in unpacker:
-                            key = tuple(key)
-                            tensor = from_bytes(None, value)
-                            tensors[key] = tensor
-                    return tensors
-
-            if is_resuming_training:
-                if is_sharded:
-                    state_dict = {}
-
-                    for shard_file in files_to_download:
-                        filename = hf_hub_download(
-                            pretrained_model_name_or_path,
-                            shard_file,
-                            token=token,
-                        )
-                        state_dict.update(loading_func(filename, **load_kwargs))
-                else:
-                    state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)
-
-        else:
-            state_dict = pretrained_model_name_or_path.state_dict()
-
-        if from_pt:
-            lw = len('.weight')
-            with jax.default_device(cls._get_current_device()):
-                flax_dict = {}
-                for key, tensor in state_dict.items():
-                    if match_keywords(key, ['kernel'], ['none']):
-                        if len(tensor.shape) == 2:
-                            tensor = tensor.transpose(0, 1)
-                    if key.endswith('.weight'):
-                        key = key[:-lw] + '.kernel'
-                    key_tuple = key.split('.')
-                    key_names = ()
-                    tensor = tensor.detach().cpu().numpy()
-                    for k in key_tuple:
-                        key_names += k,
-                    flax_dict[key_names] = tensor
-
-        model.is_peft_model = False
-        model.current_device = cls._get_current_device()
-
-        if is_resuming_training:
-            model.post_init(state_dict=state_dict)
-
-        model.supports_rm_adapter = False
-
-        return model
+    def from_pretrained(
+        cls,
+        pretrained_model_name_or_path,
+        summary_dropout_prob: float = 0.0,
+        device=jax.devices('cpu')[0],
+        dtype: jax.numpy.dtype = jax.numpy.float32,
+        param_dtype: jax.numpy.dtype = jax.numpy.float32,
+        precision: jax.lax.Precision = jax.lax.Precision('fastest'),
+        sharding_axis_dims: Sequence[int] = (1, -1, 1, 1),
+        sharding_axis_names: Sequence[str] = ("dp", "fsdp", "tp", "sp"),
+        q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+            ("dp", "fsdp"), "sp", "tp", None),
+        k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+            ("dp", "fsdp"), "sp", "tp", None),
+        v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+            ("dp", "fsdp"), "sp", "tp", None),
+        b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+            ("dp", "fsdp"), None, None, None),
+        a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+            ("dp", "fsdp"), "sp", "tp", None),
+        use_shard_map: bool = False,
+        input_shape: Sequence[int] = (1, 1),
+    ) -> Tuple[FlaxPreTrainedModel, dict]:
+        model, params = cls.transformers_parent_class.from_pretrained(
+            pretrained_model_name_or_path,
+            dtype=dtype,
+            param_dtype=param_dtype,
+            device=device,
+            precision=precision,
+            sharding_axis_dims=sharding_axis_dims,
+            sharding_axis_names=sharding_axis_names,
+            q_ps=q_ps,
+            k_ps=k_ps,
+            v_ps=v_ps,
+            b_ps=b_ps,
+            a_ps=a_ps,
+            use_shard_map=use_shard_map,
+            input_shape=input_shape
+        )
+        config = model.config
+        model_type = getattr(config, "model_type", "UNKNOWN")
+        rl_model = cls(
+            pretrained_model=model,
+            transformers_parent_class=cls.transformers_parent_class,
+            summary_dropout_prob=summary_dropout_prob,
+            dtype=dtype,
+            param_dtype=param_dtype,
+            precision=precision,
+        )
+        params = rl_model.post_init(
+            params, config
+        )
+        return rl_model, params
 
     @classmethod
     def _get_checkpoint_from_hub(
@@ -195,7 +122,7 @@
         :param model_index_name: Specify the name of the index file
         :param : Load the pretrained model
         :return: A tuple of four elements:
-        
+
         """
         files_to_download = None
         filename = None
@@ -246,7 +173,7 @@
 
         :param cls: Indicate that the function is a method of the class
         :return: The current device
-        
+
         """
         return jax.devices()[0]
 
@@ -261,7 +188,7 @@
         :param cls: Refer to the class itself
         :param kwargs: Pass keyword arguments to the function
         :return: A tuple of three dictionaries
-        
+
         """
         supported_kwargs = {}
         unsupported_kwargs = {}
@@ -301,7 +228,6 @@
         raise NotImplementedError
 
     def compute_reward_score(self, input_ids, attention_mask=None, ppo_adapter_name="default", **kwargs):
-
         """
         The compute_reward_score function is used to compute the reward score for a given input.
         The function takes in an input_ids tensor and returns a tensor of scores. The shape of the returned
@@ -313,10 +239,11 @@
         :param ppo_adapter_name: Set the adapter back to its original state
         :param **kwargs: Pass a variable number of arguments to a function
         :return: The scores for the given input_ids
-        
+
         """
         if not self.supports_rm_adapter:
-            raise ValueError("This model does not support reward modeling adapter.")
+            raise ValueError(
+                "This model does not support reward modeling adapter.")
 
         # enable rm adapter
         self.pretrained_model.set_adapter(self.rm_adapter_name)
Index: lib/python/EasyDel/transform/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from .llama import llama_from_pretrained, llama_convert_flax_to_pt, llama_convert_hf_to_flax_load, \\\n    llama_convert_hf_to_flax, llama_easydel_to_hf\nfrom .mpt import mpt_convert_flax_to_pt_1b, mpt_convert_pt_to_flax_1b, mpt_convert_pt_to_flax_7b, \\\n    mpt_convert_flax_to_pt_7b, mpt_from_pretrained\nfrom .falcon import falcon_convert_pt_to_flax_7b, falcon_convert_flax_to_pt_7b, falcon_from_pretrained, \\\n    falcon_convert_hf_to_flax, falcon_easydel_to_hf\nfrom .mistral import mistral_convert_hf_to_flax, mistral_convert_hf_to_flax_load, \\\n    mistral_convert_flax_to_pt, \\\n    mistral_from_pretrained, mistral_convert_pt_to_flax, mistral_easydel_to_hf\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/transform/__init__.py b/lib/python/EasyDel/transform/__init__.py
--- a/lib/python/EasyDel/transform/__init__.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/transform/__init__.py	(date 1703504083518)
@@ -1,9 +1,34 @@
-from .llama import llama_from_pretrained, llama_convert_flax_to_pt, llama_convert_hf_to_flax_load, \
-    llama_convert_hf_to_flax, llama_easydel_to_hf
-from .mpt import mpt_convert_flax_to_pt_1b, mpt_convert_pt_to_flax_1b, mpt_convert_pt_to_flax_7b, \
-    mpt_convert_flax_to_pt_7b, mpt_from_pretrained
-from .falcon import falcon_convert_pt_to_flax_7b, falcon_convert_flax_to_pt_7b, falcon_from_pretrained, \
-    falcon_convert_hf_to_flax, falcon_easydel_to_hf
-from .mistral import mistral_convert_hf_to_flax, mistral_convert_hf_to_flax_load, \
-    mistral_convert_flax_to_pt, \
-    mistral_from_pretrained, mistral_convert_pt_to_flax, mistral_easydel_to_hf
+from .llama import (
+    llama_from_pretrained,
+    llama_convert_flax_to_pt,
+    llama_convert_hf_to_flax_load,
+    llama_convert_hf_to_flax,
+    llama_easydel_to_hf
+)
+from .mpt import (
+    mpt_convert_flax_to_pt_1b,
+    mpt_convert_pt_to_flax_1b,
+    mpt_convert_pt_to_flax_7b,
+    mpt_convert_flax_to_pt_7b,
+    mpt_from_pretrained
+)
+from .falcon import (
+    falcon_convert_pt_to_flax_7b,
+    falcon_convert_flax_to_pt_7b,
+    falcon_from_pretrained,
+    falcon_convert_hf_to_flax,
+    falcon_easydel_to_hf
+)
+from .mistral import (
+    mistral_convert_hf_to_flax,
+    mistral_convert_hf_to_flax_load,
+    mistral_convert_flax_to_pt,
+    mistral_from_pretrained,
+    mistral_convert_pt_to_flax,
+    mistral_easydel_to_hf
+)
+
+from .easydel_transform import (
+    easydel_to_torch_dict,
+    huggingface_to_easydel,
+)
Index: lib/python/EasyDel/transform/easydel_transform.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import flax.traverse_util\nimport jax\n\nfrom flax.traverse_util import flatten_dict\nfrom flax.serialization import from_bytes, to_bytes, to_state_dict\nimport msgpack\nimport os\n\nfrom typing import List\n\n\ndef get_float_dtype_by_name(dtype):\n    \"\"\"\n    The get_float_dtype_by_name function is a helper function that returns the JAX float dtype\n    corresponding to the string name of a floating point type.  This is useful for converting\n    between strings and JAX float types, which are used in many places throughout this codebase.\n\n\n    :param dtype: Specify the type of data that is being passed into the function\n    :return: The float dtype of the input string\n    \n    \"\"\"\n    return {\n        'bf16': jax.numpy.bfloat16,\n        'bfloat16': jax.numpy.bfloat16,\n        'fp16': jax.numpy.float16,\n        'float16': jax.numpy.float16,\n        'fp32': jax.numpy.float32,\n        'float32': jax.numpy.float32,\n        'fp64': jax.numpy.float64,\n        'float64': jax.numpy.float64,\n    }[dtype]\n\n\ndef float_tensor_to_dtype(tensor, dtype):\n    \"\"\"\n    The float_tensor_to_dtype function is used to convert a tensor's dtype to the specified dtype.\n\n    :param tensor: Convert the tensor to a float dtype\n    :param dtype: Convert the tensor to a specific dtype\n    :return: A tensor with the specified dtype\n    \n    \"\"\"\n    if dtype is None or dtype == '':\n        return tensor\n    if isinstance(dtype, str):\n        dtype = get_float_dtype_by_name(dtype)\n    float_dtypes = (jax.numpy.bfloat16, jax.numpy.float16, jax.numpy.float32, jax.numpy.float64)\n    if getattr(tensor, 'dtype', None) in float_dtypes:\n        tensor = tensor.astype(dtype)\n    return tensor\n\n\ndef match_keywords(string, ts, ns):\n    \"\"\"\n    The match_keywords function takes a string, and two lists of strings.\n    The first list is the &quot;must-have&quot; keywords, and the second list is the &quot;not-allowed&quot; keywords.\n    It returns True if all of the must-have keywords are in string, but none of not allowed are in it.\n\n    :param string: Pass in the text that is being searched\n    :param ts: Specify the required keywords and ns is used to specify the non-required keywords\n    :param ns: Specify a list of negative keywords\n    :return: True if all the keywords in ts are present and none of the\n    \n    \"\"\"\n    for t in ts:\n        if t not in string:\n            return False\n    for n in ns:\n        if n in string:\n            return False\n    return True\n\n\ndef huggingface_to_easydel(\n        state_dict,\n        *,\n        embedding_layer_names: List[str],\n        device,\n        dtype: jax.numpy.dtype = jax.numpy.float16,\n        **kwargs\n):\n    \"\"\"\n    The huggingface_to_easydel function takes a huggingface model's state_dict and converts it to an easydel\n    model's flax_dict. The function is designed to be used in conjunction with the load_huggingface function, which\n    loads a huggingface model from disk. The embedding layer name must be specified as well as the device on which\n    the conversion will take place.\n\n    :param state_dict: Load the weights from a huggingface model\n    :param embedding_layer_names: List[str]: Identify the embedding layer in the huggingface model\n    :param device: Determine which device the model will be loaded on\n    :param dtype: jax.numpy.dtype: Specify the data type of the tensors\n    :return: A dictionary of the weights and biases in a format that can be used by flax (it's an UnFlattenDict)\n    \n    \"\"\"\n    if isinstance(embedding_layer_names, str):\n        embedding_layer_names = [embedding_layer_names]\n    _l = len('.weight')\n    with jax.default_device(device):\n        flax_dict = {}\n        for key, tensor in state_dict.items():\n            for embedding_layer_name in embedding_layer_names:\n                if embedding_layer_name in key:\n                    key = key[:-_l] + '.embedding'\n                elif match_keywords(key, ['weight'], ['none']):\n                    if len(tensor.shape) == 2:\n                        tensor = tensor.transpose(0, 1)\n                    if key.endswith('.weight'):\n                        key = key[:-_l] + '.kernel'\n            key_tuple = key.split('.')\n            key_names = ()\n            tensor = tensor.detach().cpu().numpy()\n            for k in key_tuple:\n                key_names += k,\n            flax_dict[key_names] = tensor.astype(dtype)\n        return flax.traverse_util.unflatten_dict(flax_dict)\n\n\ndef read_ckpt(path: [str, os.PathLike], shard_fns=None, add_extra_past_fix: list = None):\n    \"\"\"\n    The read_ckpt function reads a checkpoint file and returns the tensors in it.\n\n    :param path: [str: Specify the path to the checkpoint file\n    :param os.PathLike]: Specify the path to the checkpoint file\n    :param shard_fns: Shard the tensors\n    :param add_extra_past_fix: list: Add an extra past to the key\n    :return: A dictionary of tensors\n    \n    \"\"\"\n    tensors = {}\n    with open(path, 'rb') as stream:\n        unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n        for key, value in unpacker:\n            if add_extra_past_fix is not None:\n                key = add_extra_past_fix + key\n            key = tuple(key)\n            tensor = from_bytes(None, value)\n            if shard_fns is not None:\n                tensor = shard_fns[key](tensor)\n            tensors[key] = tensor\n    return tensors\n\n\ndef save_ckpt(train_state, path, gather_fns=None, float_dtype=None):\n    \"\"\"\n    The save_ckpt function saves the state of a training run to disk.\n\n    :param train_state: Store the current state of the training process\n    :param path: Specify the location of the checkpoint file\n    :param gather_fns: Specify a function that will be used to convert the tensor to bytes\n    :param float_dtype: Convert the tensor to a specific dtype\n    :return: Nothing\n    \n    \"\"\"\n\n    train_state = to_state_dict(train_state)\n    packer = msgpack.Packer()\n    flatten_train_state = flatten_dict(train_state)\n    if gather_fns is not None:\n        gather_fns = flatten_dict(to_state_dict(gather_fns))\n\n    with open(path, \"wb\") as stream:\n        for key, value in flatten_train_state.items():\n            if gather_fns is not None:\n                value = gather_fns[key](value)\n            value = float_tensor_to_dtype(value, float_dtype)\n            stream.write(packer.pack((key, to_bytes(value))))\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/transform/easydel_transform.py b/lib/python/EasyDel/transform/easydel_transform.py
--- a/lib/python/EasyDel/transform/easydel_transform.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/transform/easydel_transform.py	(date 1703504083518)
@@ -5,6 +5,7 @@
 from flax.serialization import from_bytes, to_bytes, to_state_dict
 import msgpack
 import os
+from transformers import PretrainedConfig
 
 from typing import List
 
@@ -18,17 +19,17 @@
 
     :param dtype: Specify the type of data that is being passed into the function
     :return: The float dtype of the input string
-    
+
     """
     return {
-        'bf16': jax.numpy.bfloat16,
-        'bfloat16': jax.numpy.bfloat16,
-        'fp16': jax.numpy.float16,
-        'float16': jax.numpy.float16,
-        'fp32': jax.numpy.float32,
-        'float32': jax.numpy.float32,
-        'fp64': jax.numpy.float64,
-        'float64': jax.numpy.float64,
+        "bf16": jax.numpy.bfloat16,
+        "bfloat16": jax.numpy.bfloat16,
+        "fp16": jax.numpy.float16,
+        "float16": jax.numpy.float16,
+        "fp32": jax.numpy.float32,
+        "float32": jax.numpy.float32,
+        "fp64": jax.numpy.float64,
+        "float64": jax.numpy.float64,
     }[dtype]
 
 
@@ -39,14 +40,15 @@
     :param tensor: Convert the tensor to a float dtype
     :param dtype: Convert the tensor to a specific dtype
     :return: A tensor with the specified dtype
-    
+
     """
-    if dtype is None or dtype == '':
+    if dtype is None or dtype == "":
         return tensor
     if isinstance(dtype, str):
         dtype = get_float_dtype_by_name(dtype)
-    float_dtypes = (jax.numpy.bfloat16, jax.numpy.float16, jax.numpy.float32, jax.numpy.float64)
-    if getattr(tensor, 'dtype', None) in float_dtypes:
+    float_dtypes = (jax.numpy.bfloat16, jax.numpy.float16,
+                    jax.numpy.float32, jax.numpy.float64)
+    if getattr(tensor, "dtype", None) in float_dtypes:
         tensor = tensor.astype(dtype)
     return tensor
 
@@ -61,7 +63,7 @@
     :param ts: Specify the required keywords and ns is used to specify the non-required keywords
     :param ns: Specify a list of negative keywords
     :return: True if all the keywords in ts are present and none of the
-    
+
     """
     for t in ts:
         if t not in string:
@@ -72,10 +74,52 @@
     return True
 
 
+def easydel_to_torch_dict(
+
+    params: dict | flax.core.FrozenDict,
+    dtype: jax.numpy.dtype = jax.numpy.float16
+) -> dict:
+    """
+    The function `easydel_to_torch_dict` converts a dictionary of parameters from the Flax library to a
+    PyTorch state dictionary, with some specific transformations applied to the keys.
+
+    :param params: The `params` parameter is a dictionary or a `flax.core.FrozenDict` object that
+    contains the parameters of a model
+    :type params: dict | flax.core.FrozenDict
+    :param dtype: The `dtype` parameter specifies the data type of the tensors in the resulting Torch
+    state dictionary. By default, it is set to `jax.numpy.float16`, which is a 16-bit floating-point
+    data type
+    :type dtype: jax.numpy.dtype
+    :return: a dictionary containing the state dictionary for the PyTorch model.
+    """
+    import torch
+    if isinstance(params, flax.core.FrozenDict):
+        params = flax.core.unfreeze(params)
+    state_dict = flax.traverse_util.flatten_dict(
+        params,
+        sep="."
+    )
+    state_dict = {}
+
+    kernel_word_length = len("kernel")
+    embedding_word_length = len("embedding")
+
+    for key, tensor in params.items():
+        key_len = len(key)
+        if match_keywords(key, ["kernel"], ["none"]):
+            tensor = tensor.T
+            key[key_len-kernel_word_length:] = "weight"
+        elif match_keywords(key, ["embedding"], ["none"]):
+            key[key_len-embedding_word_length:] = "weight"
+
+        state_dict[key] = torch.from_numpy(tensor.astype(dtype=dtype))
+    return state_dict
+
+
 def huggingface_to_easydel(
         state_dict,
         *,
-        embedding_layer_names: List[str],
+        embedding_layer_names: List[str] | str,
         device,
         dtype: jax.numpy.dtype = jax.numpy.float16,
         **kwargs
@@ -91,23 +135,23 @@
     :param device: Determine which device the model will be loaded on
     :param dtype: jax.numpy.dtype: Specify the data type of the tensors
     :return: A dictionary of the weights and biases in a format that can be used by flax (it's an UnFlattenDict)
-    
+
     """
     if isinstance(embedding_layer_names, str):
         embedding_layer_names = [embedding_layer_names]
-    _l = len('.weight')
+    _l = len(".weight")
     with jax.default_device(device):
         flax_dict = {}
         for key, tensor in state_dict.items():
             for embedding_layer_name in embedding_layer_names:
                 if embedding_layer_name in key:
-                    key = key[:-_l] + '.embedding'
-                elif match_keywords(key, ['weight'], ['none']):
+                    key = key[:-_l] + ".embedding"
+                elif match_keywords(key, ["weight"], ["none"]):
                     if len(tensor.shape) == 2:
                         tensor = tensor.transpose(0, 1)
-                    if key.endswith('.weight'):
-                        key = key[:-_l] + '.kernel'
-            key_tuple = key.split('.')
+                    if key.endswith(".weight"):
+                        key = key[:-_l] + ".kernel"
+            key_tuple = key.split(".")
             key_names = ()
             tensor = tensor.detach().cpu().numpy()
             for k in key_tuple:
@@ -125,11 +169,12 @@
     :param shard_fns: Shard the tensors
     :param add_extra_past_fix: list: Add an extra past to the key
     :return: A dictionary of tensors
-    
+
     """
     tensors = {}
-    with open(path, 'rb') as stream:
-        unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)
+    with open(path, "rb") as stream:
+        unpacker = msgpack.Unpacker(
+            stream, read_size=83886080, max_buffer_size=0)
         for key, value in unpacker:
             if add_extra_past_fix is not None:
                 key = add_extra_past_fix + key
@@ -150,7 +195,7 @@
     :param gather_fns: Specify a function that will be used to convert the tensor to bytes
     :param float_dtype: Convert the tensor to a specific dtype
     :return: Nothing
-    
+
     """
 
     train_state = to_state_dict(train_state)
Index: lib/python/EasyDel/etils/etils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/etils/etils.py b/lib/python/EasyDel/etils/etils.py
new file mode 100644
--- /dev/null	(date 1703504083518)
+++ b/lib/python/EasyDel/etils/etils.py	(date 1703504083518)
@@ -0,0 +1,43 @@
+from dataclasses import dataclass
+
+
+@dataclass
+class EasyDelOptimizers:
+    """
+    The code snippet is defining a data class called `EasyDelOptimizers` using the `@dataclass`
+    decorator. A data class is a class that is primarily used to store data, and it automatically
+    generates special methods such as `__init__`, `__repr__`, and `__eq__` based on the class
+    attributes.
+    """
+    ADAFACTOR: str = "adafactor"
+    LION: str = "lion"
+    ADAMW: str = 'adamw'
+
+
+@dataclass
+class EasyDelSchedulers:
+    """
+    The code snippet is defining a data class called `EasyDelSchedulers` using the `@dataclass`
+    decorator. A data class is a class that is primarily used to store data, and it automatically
+    generates special methods such as `__init__`, `__repr__`, and `__eq__` based on the class
+    attributes.
+    """
+    LINEAR: str = "linear"
+    COSINE: str = "cosine"
+    NONE: str = "none"
+    WARM_UP_COSINE: str = "warm_up_cosine"
+    WARM_UP_LINEAR: str = "warm_up_linear"
+
+
+@dataclass
+class EasyDelGradientCheckPointers:
+    """
+    The code snippet is defining a data class called `EasyDelGradientCheckPointers` using the `@dataclass`
+    decorator. A data class is a class that is primarily used to store data, and it automatically
+    generates special methods such as `__init__`, `__repr__`, and `__eq__` based on the class
+    attributes.
+    """
+    EVERYTHING_SAVEABLE: str = "everything_saveable"
+    NOTHING_SAVEABLE: str = "nothing_saveable"
+    CHECKPOINT_DOTS: str = "checkpoint_dots"
+    CHECKPOINT_DOTS_WITH_NO_BATCH_DMIS: str = "checkpoint_dots_with_no_batch_dims"
Index: generate_documentations.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport sys\n\n\ndef get_inner(path: str):\n    return [os.path.join(path, o) for o in os.listdir(path) if os.path.exists(os.path.join(path, o))]\n\n\ndef get_dirs(path: str):\n    return [os.path.join(path, o) for o in os.listdir(path) if\n            os.path.exists(os.path.join(path, o)) and os.path.isdir(os.path.join(path, o))]\n\n\ndef get_files(path: str):\n    return [os.path.join(path, o) for o in os.listdir(path) if\n            os.path.exists(os.path.join(path, o)) and not os.path.isdir(os.path.join(path, o))]\n\n\ndef run(project_locations=\"lib/python/EasyDel\"):\n    try:\n        for ps in get_inner(project_locations):\n            is_file = not os.path.isdir(ps)\n            if not ps.endswith(\"__init__.py\") and is_file and ps.endswith('.py'):\n                name = ps.replace(\".py\", \"\").replace(\"/\", \".\")\n                _pr = \"lib/python/EasyDel\".replace('/', '.') + '.'\n                md_doc = f\"# {name.replace(_pr, '')}\\n::: {name}\"\n                md_file = name.replace(\".\", \"-\") + '.md'\n                with open(\"docs/\" + md_file, 'w') as buffer:\n                    buffer.write(md_doc)\n\n                print(f\" - {name.replace('.', '/')} : {md_file}\")\n            else:\n                run(ps)\n    except NotADirectoryError:\n        ...\n\n\ndef main():\n    run()\n\n\nif __name__ == \"__main__\":\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/generate_documentations.py b/generate_documentations.py
--- a/generate_documentations.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/generate_documentations.py	(date 1703504083518)
@@ -20,12 +20,13 @@
     try:
         for ps in get_inner(project_locations):
             is_file = not os.path.isdir(ps)
-            if not ps.endswith("__init__.py") and is_file and ps.endswith('.py'):
+            if not ps.endswith("__init__.py") and is_file and ps.endswith(".py"):
                 name = ps.replace(".py", "").replace("/", ".")
-                _pr = "lib/python/EasyDel".replace('/', '.') + '.'
+                _pr = "lib/python/EasyDel".replace("/", ".") + "."
                 md_doc = f"# {name.replace(_pr, '')}\n::: {name}"
-                md_file = name.replace(".", "-") + '.md'
-                with open("docs/" + md_file, 'w') as buffer:
+                md_file = (name.replace(".", "-") +
+                           ".md").replace("lib-python-EasyDel-", "")
+                with open("docs/" + md_file, "w") as buffer:
                     buffer.write(md_doc)
 
                 print(f" - {name.replace('.', '/')} : {md_file}")
Index: lib/python/EasyDel/trainer/fsdp_train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import dataclasses\nimport os\nimport time\nimport typing\n\nimport IPython.display\nimport fjformer.func.loss_func\nfrom fjformer.func.loss_func import fused_cross_entropy_loss_and_accuracy, cross_entropy_loss_and_accuracy\nimport wandb\nfrom datasets import Dataset\n\nfrom .config import TrainArguments\n\nimport jax\nimport flax\nfrom transformers import FlaxAutoModelForCausalLM, AutoConfig\nfrom tqdm import tqdm\nfrom ..utils.utils import Timers, prefix_print\nfrom ..smi import initialise_tracking, get_mem\nfrom jax.experimental.pjit import pjit, with_sharding_constraint\nfrom jax.sharding import PartitionSpec\nfrom flax.training import train_state\nfrom jax import numpy as jnp\nfrom torch.utils.data import DataLoader\nfrom fjformer import match_partition_rules, make_shard_and_gather_fns, StreamingCheckpointer\nfrom ..erros import EasyDelTimer\nimport chex\n\n\ndef calculate_accuracy(predictions: chex.Array, targets: chex.Array):\n    \"\"\"\n    The calculate_accuracy function takes in two arrays, predictions and targets.\n    The function then calculates the accuracy of the model by comparing the predicted classes to\n    the target classes. The predicted class is determined by taking argmax along axis - 1 of predictions.\n    The correct_predictions variable is an array containing True or False values depending on whether or not\n    the prediction was correct for each example in a batch. The total number of examples that were correctly\n    predicted are summed up and divided by the total number of examples to get an accuracy value between 0 and 1.\n\n    :param predictions: chex.Array: Pass in the predictions from the model\n    :param targets: chex.Array: Calculate the accuracy of the model\n    :return: A single value, the accuracy\n\n    \"\"\"\n    predicted_classes = jnp.argmax(predictions, axis=-1)\n    correct_predictions = (predicted_classes == targets).sum()\n    total_predictions = targets.shape[0]\n    accuracy = correct_predictions / total_predictions\n    return accuracy\n\n\ndef create_fsdp_train_step(partition_spec=PartitionSpec((\"dp\", \"fsdp\"), \"sp\")):\n    \"\"\"\n    The create_fsdp_train_step function is a training step function that takes in the current state of the model,\n    and a batch of data. It then calculates the loss and accuracy for this batch, and returns an updated state\n    with new parameters based on these gradients.\n\n    :param partition_spec: Specify which devices the model will be split across\n    :return: A fsdp_train_step function that takes in the current state of the model,\n\n    \"\"\"\n\n    def fsdp_train_step(state, batch):\n        \"\"\"\n        The fsdp_train_step function is a training step function that takes in the current state of the model,\n        and a batch of data. It then calculates the loss and accuracy for this batch, and returns an updated state\n        with new parameters based on these gradients.\n\n        :param state: Store the model parameters\n        :param batch: Pass the data to the model\n        :return: A tuple of (state, loss, accuracy)\n\n        \"\"\"\n        batch = with_sharding_constraint(batch, partition_spec)\n\n        def calculate_loss(params):\n            labels = batch.pop('labels')\n            logits = state.apply_fn(params=params, **batch,\n                                    return_dict=True).logits\n\n            loss, accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :], labels, batch['attention_mask'].astype(jnp.float32)[:, 1:]\n            )\n            return loss, accuracy\n\n        grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n        (loss__, accuracy__), grad = grad_fn(state.params)\n        state = state.apply_gradients(grads=grad)\n        return state, loss__, accuracy__\n\n    return fsdp_train_step\n\n\ndef create_fsdp_eval_step(partition_spec=PartitionSpec((\"dp\", \"fsdp\"), \"sp\")):\n    \"\"\"\n    The create_fsdp_eval_step function is used to create a function that calculates the loss and accuracy of a model.\n    It takes in a set of parameters, which are then passed into the state.apply_fn function\n    to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.\n\n    :param partition_spec: Specify the partitioning of the model parameters\n    :return: A function that can be used to calculate the loss and accuracy of a model\n\n    \"\"\"\n\n    def fsdp_eval_step(state, batch_eval):\n        \"\"\"\n        The fsdp_eval_step function is used to calculate the loss and accuracy of a model.\n        It takes in a set of parameters, which are then passed into the state.apply_fn function\n        to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.\n\n        :param state: Store the model parameters and other information about the training process\n        :param batch_eval: Pass the batch of data to the function\n        :return: The loss and accuracy of the model\n\n        \"\"\"\n        batch_eval = with_sharding_constraint(\n            batch_eval, partition_spec\n        )\n\n        def calculate_loss(params):\n            \"\"\"\n            The calculate_loss function is used to calculate the loss and accuracy of a model.\n            It takes in a set of parameters, which are then passed into the state.apply_fn function\n            to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.\n\n            :param params: Pass the model parameters to the function\n            :return: The loss and the accuracy\n\n            \"\"\"\n            labels = batch_eval.pop('labels')\n            logits = state.apply_fn(params=params, **batch_eval,\n                                    return_dict=True).logits\n\n            loss, accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :], labels, batch_eval['attention_mask'].astype(jnp.float32)[:, 1:]\n            )\n            return loss, accuracy\n\n        loss__, accuracy__ = calculate_loss(state.params)\n        return loss__, accuracy__\n\n    return fsdp_eval_step\n\n\ndef predict(state, input_ids):\n    \"\"\"\n    The predict function takes in a state and input_ids, and returns the next token.\n\n    :param state: Store the model parameters and the input_ids parameter is used to pass in a batch of token ids\n    :param input_ids: Pass the input to the model\n    :return: The next input_ids\n\n    \"\"\"\n    input_ids = with_sharding_constraint(input_ids, PartitionSpec((\"dp\", \"fsdp\")))\n    pred = state.apply_fn(params=state.params, input_ids=input_ids, return_dict=True)\n    token = jnp.argmax(jax.nn.softmax(pred.logits)[:, -1, :])\n    input_ids = jnp.concatenate([input_ids, token.reshape(1, -1)], axis=-1)\n    return input_ids\n\n\n@dataclasses.dataclass\nclass OutputFineTuner:\n    train_state: typing.Any\n    predict_fun: typing.Any\n    mesh: typing.Any\n    ckpt_stream: typing.Any\n    gather_fns: typing.Any\n    shard_fns: typing.Any\n    last_save_file_name: str\n\n\nclass CausalLMTrainer:\n    def __init__(self,\n                 arguments: TrainArguments,\n                 dataset_train: Dataset,\n                 dataset_eval: Dataset = None,\n                 finetune: bool = True,\n                 ckpt_path: typing.Union[str, os.PathLike] = None,\n                 _do_init_fns: bool = True\n                 ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up all the variables that are needed for training, including:\n        - The timer to keep track of how long each epoch takes.\n        - The dataloaders for both training and evaluation (if provided).\n        - The model itself, which will be created from a checkpoint if one was provided.  Otherwise,\n         it will be created from scratch using the arguments passed in by the user.\n         Note that this function also handles creating a mesh if one was not already specified in arguments\n         or loaded from a checkpoint file (see below).   This means that you can pass in either\n\n        :param self: Represent the instance of the class\n        :param arguments: TrainArguments: Pass the arguments to the trainer\n        :param dataset_train: Dataset: Pass the training dataset to the trainer\n        :param dataset_eval: Dataset: Pass the validation dataset\n        :param finetune: bool: Load the model from a checkpoint\n        :param ckpt_path: typing.Union[str,os.PathLike] : Load the checkpoint path\n        :param _do_init_fns: bool: Initialize the functions\n        :return: Nothing, it just initializes the class\n\n        \"\"\"\n        self.timer = None\n        self.dataloader_train = None\n        self.dataloader_eval = None\n        self.model = None\n        self.wandb_runtime = None\n        self.max_steps_train = None\n        self.max_steps_eval = None\n        self.config = None\n        self.scheduler = None\n        self.tx = None\n        self.sharded_create_from_params_fn = None\n        self.sharded_train_step_fn = None\n        self.sharded_predict = None\n        self.mesh = None\n        self.ckpt_streamer = None\n        self.init_fn = None\n        self.train_state_shape = None\n        self.train_state_partition_spec = None\n        self.arguments = arguments\n        self.dataset_train = dataset_train\n        self.dataset_eval = dataset_eval\n        self.finetune = finetune\n        self.ckpt_path = ckpt_path\n        self.dtype = arguments.dtype\n        self.param_dtype = arguments.param_dtype\n        if finetune:\n            if ckpt_path is None:\n                prefix_print(\n                    'Warning',\n                    'In case of using finetune = True and Passing ckpt_path = None you should pass parameters'\n                    'in train function'\n                )\n        if _do_init_fns:\n            self.init_functions()\n        else:\n            prefix_print('Warning', 'you have set _do_init_fns to False so function will not me initialized you have '\n                                    f'to do in manually (simply with  trainer.init_functions() )')\n\n    def __str__(self):\n        string = f'CausalLMTrainer('\n        for k, v in self.__dict__.items():\n            if isinstance(v, typing.Callable):\n                def string_func(it_self):\n\n                    string_ = f'{it_self.__class__.__name__}(\\n'\n                    for k_, v_ in it_self.__dict__.items():\n                        string_ += f'\\t\\t{k_} : {v_}\\n'\n                    string_ += '\\t)'\n                    return string_\n\n                try:\n                    v.__str__ = string_func\n                    v = v.__str__(v)\n                except RuntimeError:\n                    pass\n\n            string += f'\\n\\t{k} : {v}'\n        string += ')'\n        return string\n\n    def __repr__(self):\n        return self.__str__()\n\n    @staticmethod\n    def finish():\n        \"\"\"\n        The finish function is called when the experiment ends.\n        It can be used to save data, upload files, or do any other cleanup tasks.\n\n        :return: A dictionary of the run's metadata\n\n        \"\"\"\n        wandb.finish()\n\n    def init_functions(self):\n        \"\"\"\n        The init_functions function is responsible for initializing the following:\n            - wandb_runtime (if you use_wandb is True)\n            - timer object (for logging time taken by various functions)\n            - dataloader objects for training and evaluation data, along with max steps per epoch.\n              The configure_dataloader function accomplishes this task.\n\n        :param self: Represent the instance of the class\n        :return: A tuple of functions\n\n        \"\"\"\n        self.wandb_runtime = self.arguments.get_wandb_init() if self.arguments.use_wandb else None\n        self.timer = Timers(\n            use_wandb=False,\n            tensorboard_writer=self.arguments.get_board()\n        )\n        self.timer(\n            'configure dataloaders'\n        ).start()\n        self.dataloader_train, self.max_steps_train, \\\n            self.dataloader_eval, self.max_steps_eval = self.configure_dataloader()\n        self.timer(\n            'configure dataloaders'\n        ).stop()\n\n        self.timer.log(['configure dataloaders'])\n\n        self.timer(\n            'configure Model ,Optimizer ,Scheduler and Config'\n        ).start()\n        self.model, self.tx, self.scheduler, self.config = self.configure_model()\n        self.timer(\n            'configure Model ,Optimizer ,Scheduler and Config'\n        ).stop()\n        self.timer.log(['configure Model ,Optimizer ,Scheduler and Config'])\n\n        self.timer(\n            'configure functions and sharding them'\n        ).start()\n        funcs = self.configure_functions()\n        self.sharded_create_from_params_fn = funcs[0]\n        self.sharded_train_step_fn = funcs[1]\n        self.sharded_predict = funcs[2]\n        self.mesh = funcs[3]\n        self.ckpt_streamer = funcs[4]\n        self.init_fn = funcs[5]\n        self.timer(\n            'configure functions and sharding them'\n        ).stop()\n        self.timer.log(['configure functions and sharding them'])\n\n    def configure_dataloader(self):\n\n        \"\"\"\n        The configure_dataloader function is used to configure the dataloader for training and evaluation.\n\n        :param self: Refer to the class instance itself\n        :return: A dataloader_train, max_steps_train, dataloader_eval and max steps eval\n\n        \"\"\"\n\n        def collate_fn(batch):\n            rs = {}\n            for key in batch[0].keys():\n                if self.arguments.is_left_padded:\n                    ssp = [jnp.array(f[key])[..., -self.arguments.max_length:] for f in batch]\n                else:\n                    ssp = [jnp.array(f[key])[..., :self.arguments.max_length] for f in batch]\n                rs[key] = jnp.stack(ssp).reshape(-1, ssp[0].shape[-1])\n            return rs\n\n        dataloader_train = DataLoader(self.dataset_train, collate_fn=collate_fn,\n                                      batch_size=self.arguments.total_batch_size, drop_last=True)\n        max_steps_train = self.arguments.num_train_epochs * len(\n            dataloader_train) if self.arguments.max_steps is None else self.arguments.max_steps\n        if self.dataset_eval is not None and self.arguments.do_eval:\n            dataloader_eval = DataLoader(self.dataset_eval, collate_fn=collate_fn,\n                                         batch_size=self.arguments.total_batch_size, drop_last=True)\n            max_steps_eval = len(\n                dataloader_eval) if self.arguments.max_steps is None else self.arguments.max_steps\n        else:\n            dataloader_eval, max_steps_eval = None, 0\n        return dataloader_train, max_steps_train, dataloader_eval, max_steps_eval\n\n    def configure_model(self):\n        \"\"\"\n        The configure_model function is responsible for creating the model, optimizer and scheduler.\n\n        :param self: Represent the instance of the class\n        :return: A model, optimizer, scheduler and config\n\n        \"\"\"\n        extra_configs = {} if self.arguments.extra_configs is None else self.arguments.extra_configs\n        if self.arguments.model_class is None:\n            config = AutoConfig.from_pretrained(self.arguments.model_id, trust_remote_code=True\n                                                , gradient_checkpointing=self.arguments.gradient_checkpointing,\n                                                use_pjit_attention_force=self.arguments.use_pjit_attention_force,\n                                                **extra_configs\n                                                )\n\n            assert hasattr(config, 'get_partition_rules')\n            model = FlaxAutoModelForCausalLM.from_config(config, trust_remote_code=True, dtype=self.arguments.dtype,\n                                                         param_dtype=self.arguments.param_dtype,\n                                                         _do_init=False)\n\n        else:\n            if not hasattr(self.arguments.configs_to_init_model_class['config'], 'get_partition_rules'):\n                assert self.arguments.custom_rule is not None, 'if you are using custom model to init you must' \\\n                                                               ' pass custom_rule for partition rules '\n\n            self.arguments.configs_to_init_model_class[\n                'config'\n            ].use_pjit_attention_force = self.arguments.use_pjit_attention_force\n\n            self.arguments.configs_to_init_model_class['config'].axis_dims = self.arguments.sharding_array\n\n            model = self.arguments.model_class(\n                **self.arguments.configs_to_init_model_class,\n                _do_init=False\n            )\n\n            config = self.arguments.configs_to_init_model_class['config']\n\n        tx, scheduler = self.arguments.get_optimizer_and_scheduler(self.max_steps_train)\n        return model, tx, scheduler, config\n\n    def configure_functions(self):\n        \"\"\"\n        The configure_functions function is responsible for configuring the functions that will be used in training.\n        It does this by first defining a function called init_fn, which initializes the model parameters and returns them as a\n        TrainState object. The TrainState object contains all of the information needed to train or evaluate on a batch of data,\n        including:\n\n        :param self: Access the class attributes\n        :return: A tuple of functions\n\n        \"\"\"\n\n        def init_fn():\n            params__ = self.model.init_weights(\n                jax.random.PRNGKey(0), self.arguments.init_input_shape\n            )\n            if self.arguments.dtype == jnp.bfloat16:\n                params__ = self.model.to_bf16(params__)\n            elif self.arguments.dtype == jnp.float16:\n                params__ = self.model.to_fp16(params__)\n            return train_state.TrainState.create(\n                tx=self.tx,\n                params=flax.core.freeze({'params': params__}),\n                apply_fn=self.model.__call__\n            )\n\n        def create_train_state_from_params(params_):\n            return train_state.TrainState.create(\n                tx=self.tx,\n                apply_fn=self.model.__call__,\n                params=params_\n            )\n\n        if self.arguments.loss_remat == 'OHA':\n            loss_fn = fjformer.func.loss_func.cross_entropy_with_logits\n        elif self.arguments.loss_remat != '':\n            loss_fn = fused_cross_entropy_loss_and_accuracy\n        else:\n            loss_fn = cross_entropy_loss_and_accuracy\n\n        def fsdp_train_step_(state, batch):\n            batch = with_sharding_constraint(batch, self.arguments.step_partition_spec)\n\n            def calculate_loss(params):\n                labels = batch.pop('labels')\n                logits = state.apply_fn(params=params, **batch,\n                                        return_dict=True).logits[:, :-1, :]\n\n                loss, accuracy = loss_fn(\n                    logits, labels, batch['attention_mask'].astype(jnp.float32)[:, 1:]\n                )\n                return loss, accuracy\n\n            grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n            (loss__, accuracy__), grad = grad_fn(state.params)\n            state = state.apply_gradients(grads=grad)\n            return state, loss__, accuracy__\n\n        train_state_shape = jax.eval_shape(init_fn)\n        train_state_partition_spec = match_partition_rules(\n            self.config.get_partition_rules(\n                fully_fsdp=self.arguments.fully_fsdp\n            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n            train_state_shape\n        )\n        sharded_create_from_params_fn = pjit(\n            create_train_state_from_params,\n            in_shardings=(train_state_partition_spec.params,),\n            out_shardings=train_state_partition_spec,\n            donate_argnums=(0,)\n        )\n        sharded_train_step_fn = pjit(\n            fsdp_train_step_,\n            in_shardings=(train_state_partition_spec, PartitionSpec()),\n            out_shardings=(train_state_partition_spec, PartitionSpec(), PartitionSpec()),\n            donate_argnums=(0, 0),\n        )\n        sharded_predict = pjit(predict, out_shardings=PartitionSpec(),\n                               in_shardings=(train_state_partition_spec, PartitionSpec()))\n        mesh = self.arguments.get_mesh()\n        self.arguments.ckpt_path_exists()\n        ckpt_streamer = self.arguments.get_streaming_checkpointer()\n        self.train_state_partition_spec = train_state_partition_spec\n        self.train_state_shape = train_state_shape\n        return sharded_create_from_params_fn, sharded_train_step_fn, sharded_predict, mesh, ckpt_streamer, init_fn\n\n    def train(self, model_parameters: flax.core.FrozenDict = None) -> OutputFineTuner:\n        \"\"\"\n        The train function is the main function of this module.\n        It takes a model_parameters argument which can be used to load a pretrained model and finetune it.\n        The train function returns an OutputFineTuner object that contains the last saved file name, predict func,\n        train state, mesh and checkpoint streamer.\n\n\n        :param self: Make the class methods aware of other methods and attributes within the class\n        :param model_parameters: flax.core.FrozenDict: Load a pre-trained model\n        :return: An object of type \"OutputFineTuner\"\n\n        \"\"\"\n\n        def count_params(_p):\n            print('\\033[1;31mModel Contain : ',\n                  sum(i.size for i in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9,\n                  ' Billion Parameters')\n\n        dir_prefix: str = '/dev/shm'\n        if self.arguments.track_memory:\n            initialise_tracking(dir_prefix=dir_prefix)\n        start_time = time.time()\n        with self.mesh:\n            if self.finetune:\n                shard_fns, gather_fns = make_shard_and_gather_fns(self.train_state_partition_spec,\n                                                                  dtype_specs=self.dtype)\n\n                if model_parameters is None:\n                    prefix_print(\n                        'Action', f'Loading Model From {self.ckpt_path}'\n                    )\n                    _, params = StreamingCheckpointer.load_trainstate_checkpoint(\n                        f'params::{self.ckpt_path}', self.train_state_shape, shard_fns\n                    )\n\n                    if self.arguments.remove_ckpt_after_load:\n                        os.remove(self.ckpt_path)\n                else:\n                    prefix_print(\n                        'Action', f'Sharding Passed Parameters'\n                    )\n                    from flax.core import unfreeze\n                    if not isinstance(model_parameters, flax.core.FrozenDict):\n                        prefix_print(\n                            'Warning', 'Model Parameters should be like FrozenDict({\"params\" : params}) make sure to '\n                                       'pass as type FrozenDict in case of not getting UnExcepted Errors '\n                        )\n                    params = model_parameters if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                        lambda f, x: f(x), shard_fns.params,\n                        model_parameters)\n\n                sharded_train_state_ = self.sharded_create_from_params_fn(params)\n\n                count_params(sharded_train_state_.params)\n            else:\n                sharded_train_state_ = self.init_fn()\n\n                count_params(sharded_train_state_.params)\n\n            pbar = tqdm(total=self.max_steps_train)\n            i = sharded_train_state_.step.tolist()\n            losses = []\n            accuracies = []\n            pbar.update(sharded_train_state_.step.tolist())\n            learning_rates = []\n            if self.arguments.use_wandb:\n                self.wandb_runtime.log(\n                    {\n                        'model billion parameters': sum(\n                            i.size for i in\n                            jax.tree_util.tree_flatten(flax.core.unfreeze(sharded_train_state_.params))[0]) / 1e9\n                    }\n                )\n            try:\n                for ep in range(self.arguments.num_train_epochs):\n                    for batch in self.dataloader_train:\n                        i += 1\n                        if i < self.max_steps_train:\n\n                            batch['labels'] = batch['input_ids'][..., 1:]\n\n                            for ssb in self.arguments.ids_to_pop_from_dataset:\n                                _ = batch.pop(ssb, None)\n                            time_s = time.time()\n                            sharded_train_state_, loss, accuracy = self.sharded_train_step_fn(sharded_train_state_,\n                                                                                              batch\n                                                                                              )\n                            ttl_time = time.time() - time_s\n                            losses.append(loss)\n                            learning_rates.append(self.scheduler(i).tolist())\n                            accuracies.append(accuracy)\n                            if self.arguments.track_memory:\n                                mem_res = get_mem(dir_prefix=dir_prefix)\n                            else:\n                                mem_res = 'Tracking Option is OFF'\n                            pbar.update(1)\n\n                            if self.arguments.use_wandb:\n                                with jax.spmd_mode(\"allow_all\"):\n                                    self.wandb_runtime.log(\n                                        {\n                                            \"loss\": loss.tolist(),\n                                            \"learning_rate\": self.scheduler(\n                                                sharded_train_state_.step.tolist()\n                                            ).tolist(),\n                                            \"step\": sharded_train_state_.step.tolist(),\n                                            \"step time\": ttl_time,\n                                            \"perplexity\": jnp.exp(loss).tolist(),\n                                            \"accuracy\": accuracy.tolist(),\n                                            \"avg_accuracy\": (sum(accuracies) / len(accuracies)).tolist(),\n                                            \"mem_res\": mem_res,\n                                        }\n                                    )\n                            if self.arguments.track_memory:\n                                IPython.display.clear_output(True)\n                                pbar.display(mem_res)\n                            pbar.set_postfix(loss=loss,\n                                             learning_rate=self.scheduler(sharded_train_state_.step.tolist()).tolist(),\n                                             step=sharded_train_state_.step.tolist(),\n                                             perplexity=jnp.exp(loss).tolist(),\n                                             accuracy=accuracy,\n                                             )\n                            if self.arguments.training_time is not None:\n                                if time.time() - start_time > self.arguments.training_time:\n                                    raise EasyDelTimer\n                        else:\n                            break\n                        if self.arguments.save_steps is not None and i % self.arguments.save_steps == 0:\n                            filename = f'{self.arguments.model_name}-{sum(losses) / len(losses)}-{i}'\n                            print(f'Saving Model to \\033[1;30m{filename}\\033[1;0m')\n                            self.ckpt_streamer.save_checkpoint(sharded_train_state_.params['params'],\n                                                               filename,\n                                                               gather_fns=gather_fns.params['params'])\n            except KeyboardInterrupt:\n                print(\n                    '\\033[1;30m KeyboardInterrupt At training model Will return current state of the model * \\033[1;0m')\n            except EasyDelTimer:\n                print(\n                    '\\033[1;30m Training reached out maximum training Time Killing training Process '\n                    'and Will return current state of the model * \\033[1;0m'\n                )\n            if self.arguments.do_eval:\n                if self.dataset_eval is not None:\n                    pbar_eval = tqdm(total=self.max_steps_eval)\n                    for i_eval, batch_eval in enumerate(self.dataloader_eval):\n                        _ = batch_eval.pop('token_type_ids', None)\n                        batch_eval['labels'] = batch_eval['input_ids'][..., 1:]\n                        for i in self.arguments.ids_to_pop_from_dataset:\n                            _ = batch_eval.pop(i, None)\n                        loss_eval, accuracy = create_fsdp_eval_step(self.arguments.step_partition_spec)(\n                            sharded_train_state_, batch_eval)\n                        pbar_eval.update(1)\n                        if self.arguments.use_wandb:\n                            self.wandb_runtime.log(\n                                {'loss_eval': loss_eval.tolist(),\n                                 'accuracy': accuracy.tolist()}\n                            )\n                        pbar_eval.set_postfix(loss_eval=loss_eval.tolist())\n            if self.arguments.save_steps is None and self.arguments.do_last_save:\n                filename = f'{self.arguments.model_name}-{sum(losses) / len(losses)}-{i}'\n                print(f'Saving Model to \\033[1;30m{filename}\\033[1;0m')\n                self.ckpt_streamer.save_checkpoint(sharded_train_state_.params['params'],\n                                                   filename,\n                                                   gather_fns=gather_fns.params['params'])\n            else:\n                filename = 'not_saved | None'\n        output = OutputFineTuner(\n            last_save_file_name=filename,\n            predict_fun=self.sharded_predict,\n            train_state=sharded_train_state_,\n            mesh=self.mesh,\n            shard_fns=shard_fns,\n            gather_fns=gather_fns,\n            ckpt_stream=self.ckpt_streamer\n        )\n        wandb.finish()\n\n        return output\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/trainer/fsdp_train.py b/lib/python/EasyDel/trainer/fsdp_train.py
--- a/lib/python/EasyDel/trainer/fsdp_train.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/trainer/fsdp_train.py	(date 1703504083518)
@@ -609,7 +609,7 @@
                                              )
                             if self.arguments.training_time is not None:
                                 if time.time() - start_time > self.arguments.training_time:
-                                    raise EasyDelTimer
+                                    raise EasyDelTimer("TimeOut")
                         else:
                             break
                         if self.arguments.save_steps is not None and i % self.arguments.save_steps == 0:
Index: lib/python/EasyDel/trainer/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os.path\nimport pathlib\nimport re\nimport typing\nfrom typing import OrderedDict, List, Union\n\nimport fjformer.optimizers\n\nimport torch.utils.tensorboard\nimport wandb\nfrom fjformer import StreamingCheckpointer\nfrom jax.experimental.mesh_utils import create_device_mesh\n\nfrom jax.sharding import Mesh\nfrom jax import numpy as jnp\nimport jax\n\nAVAILABLE_OPTIMIZERS: List[str] = ['adafactor', 'lion', 'adamw']\nAVAILABLE_SCHEDULERS: List[str] = ['linear', 'cosine', 'none', 'warm_up_cosine', \"warm_up_linear\"]\nAVAILABLE_GRADIENT_CHECK_POINTING: List[str] = ['everything_saveable',\n                                                'nothing_saveable',\n                                                'checkpoint_dots',\n                                                'checkpoint_dots_with_no_batch_dims']\nAVAILABLE_BACKENDS: List[str] = [\n    'cpu', 'gpu', 'tpu', None\n]\n\n\nclass TrainArguments(\n    OrderedDict\n):\n    def __init__(\n            self,\n            model_name: str,\n            num_train_epochs: int,\n            model_id: str = None,\n            model_class=None,\n            total_batch_size: int = 32,\n            max_steps: Union[int, None] = None,\n            optimizer: str = 'lion',\n            scheduler: str = 'linear',\n            learning_rate: Union[int, float] = 5e-5,\n            learning_rate_end: Union[None, float] = 5e-6,\n            gradient_accumulation_steps: int = 1,\n            weight_decay: float = 0.01,\n            gradient_checkpointing: str = 'nothing_saveable',\n            max_length: Union[int, None] = 4096,\n            sharding_array: Union[tuple, int] = (1, -1, 1, 1),\n            is_fine_tuning: bool = True,\n            do_train: bool = True,\n            do_eval: bool = False,\n            do_test: Union[bool, None] = False,\n            backend: Union[str, None] = None,\n            extra_optimizer_kwargs: dict = None,\n            save_steps: Union[int, None] = None,\n            save_dir: str = 'easydel_ckpt',\n            use_pjit_attention_force: bool = False,\n            dtype=jnp.bfloat16,\n            param_dtype=jnp.bfloat16,\n            fully_fsdp=True,\n            use_wandb: bool = True,\n            custom_rule=None,\n            extra_configs=None,\n            ids_to_pop_from_dataset: list = None,\n            remove_ckpt_after_load: bool = False,\n            configs_to_init_model_class=None,\n            do_last_save: bool = True,\n            model_parameters=None,\n            do_shard_fns: bool = True,\n            track_memory: bool = True,\n            loss_remat: str = '',\n            loss_chunk: int = 1024,\n            is_left_padded: bool = False,\n            warmup_steps: int = 500,\n            init_input_shape: typing.Tuple[int, int] = (1, 1),\n            step_partition_spec: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"sp\"),\n            training_time: typing.Optional[str] = None,\n            **kwargs\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and makes sure that it has all of\n        the attributes necessary for proper functioning. It also allows you to set\n        default values for those attributes if they are not provided as arguments by\n        the person creating an instance.\n\n        :param self: Refer to the class instance itself\n        :param model_name: str: Specify the model name\n        :param num_train_epochs: int: Set the number of epochs for training\n        :param model_id: str: Load a model from the save_dir\n        :param model_class: Initialize the model, and the configs_to_init_model_class parameter is used to\n        :param total_batch_size: int: Set the batch size of the model\n        :param max_steps: Union[int,None]: Determine the maximum number of steps to train for\n        :param optimizer: str: Specify which optimizer to use\n        :param scheduler: str: Set the learning rate scheduler\n        :param learning_rate: Union[int,float]: Set the learning rate , Set the dtype of the model parameters\n        :param learning_rate_end: Union[None,float]: Set the end learning rate, Set the dtype of the model parameters\n        :param gradient_accumulation_steps: int: Accumulate gradients over multiple batches\n        :param weight_decay: float: Control the weight decay\n        :param gradient_checkpointing: str: Control the gradient checkpointing method\n        :param max_length: Union[int, None]: Set the maximum length of a sequence, Pass the model_class to the trainer class\n        :param sharding_array: Union[tuple: Shard the model across multiple devices\n        :param is_fine_tuning: bool: Determine whether the model is being trained from scratch or not\n        :param do_train: bool: Determine whether the model should be trained or not\n        :param do_eval: bool: Determine whether to run the eval loop or not\n        :param do_test: Union[bool,None]: Determine whether to run the test or not, Pass the model_class to the trainer\n        :param backend: Union[str, None]:: Specify the device that will be used for training, Define the default value of a parameter\n        :param extra_optimizer_kwargs: dict: Pass extra arguments to the optimizer\n        :param save_steps: Union[int,None]: Save the model after a number of steps,  Set the default value of do_test to none\n        :param save_dir: str: Specify the directory where the model checkpoints will be saved\n        :param use_pjit_attention_force: bool: Determine whether to use the jax\n        :param dtype: Set the data type of the model parameters and inputs\n        :param param_dtype: Specify the data type of the model parameters\n        :param fully_fsdp: Control the use of fully fused sdp\n        :param use_wandb: bool: Determine whether to use wandb or not\n        :param custom_rule: Pass a custom rule to the optimizer,\n        :param extra_configs: Pass extra configurations to the model class\n        :param ids_to_pop_from_dataset: list: Pop some keys from the dataset\n        :param training_time: str: maximum time for Trainer to Train the Model\n        :param remove_ckpt_after_load: bool: Remove the checkpoint after loading it\n        :param configs_to_init_model_class: Pass the configs to the model class\n        :param do_last_save: bool: Save the model at the end of training\n        :param model_parameters: Pass the model parameters to the trainer\n        :param do_shard_fns: bool: Shard the model across multiple devices\n        :param track_memory: bool: Track the memory usage of the model\n        :param loss_remat: str: Specify how to rematerialize the loss function\n        :param loss_chunk: int: Chunk the loss function\n        :param is_left_padded: bool: Indicate whether the input is left padded or not\n        :param warmup_steps: int: Warm up the learning rate\n        :param init_input_shape: typing.Tuple[int]: Initialize the input shape of the model\n        :param step_partition_spec: jax.sharding.PartitionSpec: PartitionSpec Custom to be used in training and eval or test loop\n        :param **kwargs: Pass a variable number of keyword arguments to a function\n        :return: Nothing\n        \n        \"\"\"\n        super().__init__()\n        if ids_to_pop_from_dataset is None:\n            ids_to_pop_from_dataset = []\n        if extra_optimizer_kwargs is None:\n            extra_optimizer_kwargs = {}\n        assert model_class is not None or model_id is not None, 'you cant pass model_class and model_id both None ' \\\n                                                                'you should at least pass one of them to build ' \\\n                                                                'model with'\n        assert backend in AVAILABLE_BACKENDS, f'{backend} is not recognized, ' \\\n                                              f'available backends are {AVAILABLE_BACKENDS}'\n        assert gradient_checkpointing in AVAILABLE_GRADIENT_CHECK_POINTING, f'{gradient_checkpointing} is not ' \\\n                                                                            f'recognized, available gradient ' \\\n                                                                            f'checkpointing methods are ' \\\n                                                                            f'{AVAILABLE_GRADIENT_CHECK_POINTING}'\n        assert scheduler in AVAILABLE_SCHEDULERS, f'{scheduler} is not recognized, ' \\\n                                                  f'available schedulers are {AVAILABLE_SCHEDULERS}'\n        assert optimizer in AVAILABLE_OPTIMIZERS, f'{optimizer} is not recognized, ' \\\n                                                  f'available optimizers are {AVAILABLE_OPTIMIZERS}'\n        self.available_backends = len(jax.devices(backend))\n        total_batch_size *= gradient_accumulation_steps\n        array_devices = jnp.ones((self.available_backends, 1)).reshape(sharding_array)\n        self.array_devices_shape = array_devices.shape\n\n        self.model_id = model_id\n        self.num_train_epochs = num_train_epochs\n        self.total_batch_size = total_batch_size\n        self.max_steps = max_steps\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.extra_optimizer_kwargs = extra_optimizer_kwargs\n        self.learning_rate = learning_rate\n        self.learning_rate_end = learning_rate_end\n        self.weight_decay = weight_decay\n        self.model_name = model_name\n        self.gradient_checkpointing = gradient_checkpointing\n        self.max_length = max_length\n        self.sharding_array = sharding_array\n        self.is_fine_tuning = is_fine_tuning\n        self.do_train = do_train\n        self.do_eval = do_eval\n        self.do_test = do_test\n        self.save_steps = save_steps\n        self.save_dir = save_dir\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.dtype = dtype\n        self.warmup_steps = warmup_steps\n        self.param_dtype = param_dtype\n        self.fully_fsdp = fully_fsdp\n        self.use_wandb = use_wandb\n        self.custom_rule = custom_rule\n        self.extra_configs = extra_configs\n        self.ids_to_pop_from_dataset = ids_to_pop_from_dataset\n        self.remove_ckpt_after_load = remove_ckpt_after_load\n        self.model_class = model_class\n        self.configs_to_init_model_class = configs_to_init_model_class\n        self.do_last_save = do_last_save\n        self.model_parameters = model_parameters\n        self.do_shard_fns = do_shard_fns\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.track_memory = track_memory\n\n        self.loss_chunk = loss_chunk\n        self.loss_remat = loss_remat\n        self.init_input_shape = init_input_shape\n        self.is_left_padded = is_left_padded\n        self.step_partition_spec = step_partition_spec\n\n        self.training_time = self._time_to_seconds(training_time) if training_time is not None else None\n        torch.set_default_device('cpu')\n        self.__dict__.update(**kwargs)\n\n    @staticmethod\n    def _time_to_seconds(time_str):\n        pattern = r'(\\d+)\\s*(H|min|Min)'\n        match = re.match(pattern, time_str)\n\n        if match:\n            value = int(match.group(1))\n            unit = match.group(2).lower()\n\n            if unit == 'h':\n                return value * 3600  # Convert hours to seconds\n            elif unit == 'min':\n                return value * 60  # Convert minutes to seconds\n        else:\n            raise SyntaxError(\"Invalid input format it should be like 50Min for M and 23H for hours\")\n\n    def __call__(self):\n        return {k: v for k, v in self.__dict__.items()}\n\n    def get_meter_dict(self):\n        \"\"\"\n        The get_meter_dict function is used to return a dictionary of the hyperparameters.\n        The function iterates through all the attributes in the class and returns a dictionary with\n        the key as &quot;hyperparameters/{k}&quot; and value as v for each attribute k,v in self.__dict__ if it is an instance of int, float, str, bool or torch.Tensor.\n\n        :param self: Represent the instance of the class\n        :return: A dictionary of hyperparameters\n        \n        \"\"\"\n        return {f\"hyperparameters/{k}\": v for k, v in self.__dict__.items() if\n                isinstance(v, (int, float, str, bool, torch.Tensor))}\n\n    def get_wandb_init(self):\n        \"\"\"\n        The get_wandb_init function is a helper function that returns the wandb.init() call with\n        the project name, config object, and tags set to appropriate values for this model.\n\n        :param self: Pass the class instance to the function\n        :return: A wandb\n        \n        \"\"\"\n        return wandb.init(\n            project=f'easydel-{self.model_name}',\n            config=self(),\n            tags=[\n                'Easy Del',\n                'OST-OpenSourceTransformers',\n                'Jax/Flax'\n            ]\n        )\n\n    def __str__(self):\n        string = f'TrainingArguments(\\n'\n        for k, v in self.__call__().items():\n            if isinstance(v, typing.Callable):\n                def string_func(it_self):\n                    string_ = f'{it_self.__class__.__name__}(\\n'\n                    for k_, v_ in it_self.__dict__.items():\n                        string_ += f'\\t\\t{k_} : {v_}\\n'\n                    string_ += '\\t)'\n                    return string_\n\n                v.__str__ = string_func\n                v = v.__str__(v)\n            string += f'\\t{k} : {v}\\n'\n        string += ')'\n        return string\n\n    def get_path(self):\n        \"\"\"\n        The get_path function returns a pathlib.Path object, which is a class that\n        represents file paths and provides methods for interacting with the files at\n        those paths. The get_path function takes no arguments and returns an instance of\n        the Path class initialized with two arguments: self.save_dir (a string) and\n        self.model_name (also a string). The save directory is the directory where we'll\n        store our model checkpoints, while the model name will be used to create unique\n        filenames for each checkpoint.\n\n        :param self: Represent the instance of the class\n        :return: A pathlib\n        \n        \"\"\"\n        return pathlib.Path(\n            self.save_dir, self.model_name\n        )\n\n    def ckpt_path_exists(self):\n        \"\"\"\n        The ckpt_path_exists function checks to see if the path exists. If it does not, then it creates a new directory.\n\n        :param self: Represent the instance of the class\n        :return: A path\n        \n        \"\"\"\n        path = self.get_path()\n        if not path.exists():\n            path.mkdir(parents=True)\n\n    def get_mesh(self):\n        \"\"\"\n        The get_mesh function is used to create a mesh object that can be used\n        to define the geometry of the device. The mesh object contains two arrays:\n        a list of vertices and a list of faces. Each face is defined by three indices,\n        which correspond to three vertices in the vertex array. The get_mesh function\n        is called when creating an instance of DeviceGeometry, which is then passed\n        into an instance of DeviceSimulation.\n\n        :param self: Refer to the object itself\n        :return: A mesh object with the device array shape and the mesh names\n        \n        \"\"\"\n        return Mesh(\n            create_device_mesh(\n                self.array_devices_shape\n            ),\n            self.get_mesh_names()\n        )\n\n    def __repr__(self):\n        return self.__str__()\n\n    @staticmethod\n    def get_mesh_names():\n        return \"dp\", \"fsdp\", \"tp\", \"sp\"\n\n    def get_optimizer_and_scheduler(self, steps=None):\n        \"\"\"\n        The get_optimizer_and_scheduler function is a helper function that returns the optimizer and scheduler\n            based on the parameters passed to it.\n\n        :param self: Represent the instance of the class\n        :param steps: Calculate the number of steps to train\n        :return: A tuple of two objects:\n        \n        \"\"\"\n        steps = self.max_steps or steps\n        assert steps is not None, 'if you haven\\'t pass max steps to init you should pass init in func'\n\n        if self.optimizer == 'adafactor':\n            if self.scheduler == 'linear':\n                tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate_end,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    steps=steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'cosine':\n                tx, sc = fjformer.optimizers.get_adafactor_with_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'none':\n                tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_cosine':\n                tx, sc = fjformer.optimizers.get_adafactor_with_warm_up_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    steps=steps,\n                    weight_decay=self.weight_decay,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_linear':\n                tx, sc = fjformer.optimizers.get_adafactor_with_warmup_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    steps=steps,\n                    learning_rate_end=self.learning_rate_end,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    warmup_steps=self.warmup_steps,\n                    **self.extra_optimizer_kwargs\n\n                )\n\n            else:\n                raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n        elif self.optimizer == 'lion':\n            if self.scheduler == 'linear':\n                tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate_end,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'cosine':\n                tx, sc = fjformer.optimizers.get_lion_with_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    steps=steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'none':\n                tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_cosine':\n                tx, sc = fjformer.optimizers.get_lion_with_warm_up_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n\n            elif self.scheduler == 'warm_up_linear':\n                tx, sc = fjformer.optimizers.get_lion_with_with_warmup_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    steps=steps,\n                    learning_rate_end=self.learning_rate_end,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    warmup_steps=self.warmup_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            else:\n                raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n        elif self.optimizer == 'adamw':\n            if self.scheduler == 'linear':\n                tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate_end,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'cosine':\n                tx, sc = fjformer.optimizers.get_adamw_with_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    steps=steps,\n                    weight_decay=self.weight_decay,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'none':\n                tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    steps=steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_cosine':\n                tx, sc = fjformer.optimizers.get_adamw_with_warm_up_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    steps=steps,\n                    weight_decay=self.weight_decay,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_linear':\n                tx, sc = fjformer.optimizers.get_adamw_with_warmup_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    steps=steps,\n                    weight_decay=self.weight_decay,\n                    learning_rate_end=self.learning_rate_end,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    warmup_steps=self.warmup_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            else:\n                raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n        else:\n            raise ValueError('seems like you have choose wrong type or unavailable optimizer')\n        return tx, sc\n\n    def get_streaming_checkpointer(self):\n        \"\"\"\n        The get_streaming_checkpointer function is used to save the model's weights.\n        The streaming checkpointer saves the model's weights in a file called &quot;checkpoint&quot; and then\n        saves a copy of that file with an incrementing number appended to it (e.g., checkpoint_001,\n        checkpoint_002, etc.). This allows you to keep multiple versions of your trained models.\n\n        :param self: Represent the instance of the class\n        :return: A streamingcheckpointer object\n        \n        \"\"\"\n        return StreamingCheckpointer(StreamingCheckpointer.get_default_config(),\n                                     os.path.join(self.save_dir, self.model_name))\n\n    def get_board(self):\n        \"\"\"\n        The get_board function is a helper function that returns a TensorBoard object.\n        The TensorBoard object is used to log the training and validation loss, as well as\n        the accuracy of the model during training. The get_board function takes no arguments,\n        and returns an instance of torch.utils.tensorboard SummaryWriter class.\n\n        :param self: Represent the instance of the class\n        :return: A summary-writer object\n        \n        \"\"\"\n        return torch.utils.tensorboard.SummaryWriter(\n            log_dir=str(self.get_path()),\n            comment=f'{self.model_name}',\n            filename_suffix='easydel'\n        )\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/trainer/config.py b/lib/python/EasyDel/trainer/config.py
--- a/lib/python/EasyDel/trainer/config.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/trainer/config.py	(date 1703504083518)
@@ -14,15 +14,10 @@
 from jax.sharding import Mesh
 from jax import numpy as jnp
 import jax
+from ..etils import EasyDelGradientCheckPointers, EasyDelOptimizers, EasyDelSchedulers
 
-AVAILABLE_OPTIMIZERS: List[str] = ['adafactor', 'lion', 'adamw']
-AVAILABLE_SCHEDULERS: List[str] = ['linear', 'cosine', 'none', 'warm_up_cosine', "warm_up_linear"]
-AVAILABLE_GRADIENT_CHECK_POINTING: List[str] = ['everything_saveable',
-                                                'nothing_saveable',
-                                                'checkpoint_dots',
-                                                'checkpoint_dots_with_no_batch_dims']
 AVAILABLE_BACKENDS: List[str] = [
-    'cpu', 'gpu', 'tpu', None
+    "cpu", "gpu", "tpu", None
 ]
 
 
@@ -37,13 +32,13 @@
             model_class=None,
             total_batch_size: int = 32,
             max_steps: Union[int, None] = None,
-            optimizer: str = 'lion',
-            scheduler: str = 'linear',
+            optimizer: EasyDelOptimizers | str = EasyDelOptimizers.ADAMW,
+            scheduler: EasyDelSchedulers | str = EasyDelSchedulers.NONE,
             learning_rate: Union[int, float] = 5e-5,
             learning_rate_end: Union[None, float] = 5e-6,
             gradient_accumulation_steps: int = 1,
             weight_decay: float = 0.01,
-            gradient_checkpointing: str = 'nothing_saveable',
+            gradient_checkpointing: EasyDelGradientCheckPointers | str = EasyDelGradientCheckPointers.NOTHING_SAVEABLE,
             max_length: Union[int, None] = 4096,
             sharding_array: Union[tuple, int] = (1, -1, 1, 1),
             is_fine_tuning: bool = True,
@@ -53,7 +48,7 @@
             backend: Union[str, None] = None,
             extra_optimizer_kwargs: dict = None,
             save_steps: Union[int, None] = None,
-            save_dir: str = 'easydel_ckpt',
+            save_dir: str = "easydel_ckpt",
             use_pjit_attention_force: bool = False,
             dtype=jnp.bfloat16,
             param_dtype=jnp.bfloat16,
@@ -68,12 +63,13 @@
             model_parameters=None,
             do_shard_fns: bool = True,
             track_memory: bool = True,
-            loss_remat: str = '',
+            loss_remat: str = "",
             loss_chunk: int = 1024,
             is_left_padded: bool = False,
             warmup_steps: int = 500,
             init_input_shape: typing.Tuple[int, int] = (1, 1),
-            step_partition_spec: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(("dp", "fsdp"), "sp"),
+            step_partition_spec: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(
+                ("dp", "fsdp"), "sp"),
             training_time: typing.Optional[str] = None,
             **kwargs
     ):
@@ -131,29 +127,22 @@
         :param step_partition_spec: jax.sharding.PartitionSpec: PartitionSpec Custom to be used in training and eval or test loop
         :param **kwargs: Pass a variable number of keyword arguments to a function
         :return: Nothing
-        
+
         """
         super().__init__()
         if ids_to_pop_from_dataset is None:
             ids_to_pop_from_dataset = []
         if extra_optimizer_kwargs is None:
             extra_optimizer_kwargs = {}
-        assert model_class is not None or model_id is not None, 'you cant pass model_class and model_id both None ' \
-                                                                'you should at least pass one of them to build ' \
-                                                                'model with'
-        assert backend in AVAILABLE_BACKENDS, f'{backend} is not recognized, ' \
-                                              f'available backends are {AVAILABLE_BACKENDS}'
-        assert gradient_checkpointing in AVAILABLE_GRADIENT_CHECK_POINTING, f'{gradient_checkpointing} is not ' \
-                                                                            f'recognized, available gradient ' \
-                                                                            f'checkpointing methods are ' \
-                                                                            f'{AVAILABLE_GRADIENT_CHECK_POINTING}'
-        assert scheduler in AVAILABLE_SCHEDULERS, f'{scheduler} is not recognized, ' \
-                                                  f'available schedulers are {AVAILABLE_SCHEDULERS}'
-        assert optimizer in AVAILABLE_OPTIMIZERS, f'{optimizer} is not recognized, ' \
-                                                  f'available optimizers are {AVAILABLE_OPTIMIZERS}'
+        assert model_class is not None or model_id is not None, "you cant pass model_class and model_id both None " \
+                                                                "you should at least pass one of them to build " \
+                                                                "model with"
+        assert backend in AVAILABLE_BACKENDS, f"{backend} is not recognized, " \
+                                              f"available backends are {AVAILABLE_BACKENDS}"
         self.available_backends = len(jax.devices(backend))
         total_batch_size *= gradient_accumulation_steps
-        array_devices = jnp.ones((self.available_backends, 1)).reshape(sharding_array)
+        array_devices = jnp.ones(
+            (self.available_backends, 1)).reshape(sharding_array)
         self.array_devices_shape = array_devices.shape
 
         self.model_id = model_id
@@ -193,32 +182,33 @@
         self.do_shard_fns = do_shard_fns
         self.gradient_accumulation_steps = gradient_accumulation_steps
         self.track_memory = track_memory
-
         self.loss_chunk = loss_chunk
         self.loss_remat = loss_remat
         self.init_input_shape = init_input_shape
         self.is_left_padded = is_left_padded
         self.step_partition_spec = step_partition_spec
 
-        self.training_time = self._time_to_seconds(training_time) if training_time is not None else None
-        torch.set_default_device('cpu')
+        self.training_time = self._time_to_seconds(
+            training_time) if training_time is not None else None
+        torch.set_default_device("cpu")
         self.__dict__.update(**kwargs)
 
     @staticmethod
     def _time_to_seconds(time_str):
-        pattern = r'(\d+)\s*(H|min|Min)'
+        pattern = r"(\d+)\s*(H|min|Min)"
         match = re.match(pattern, time_str)
 
         if match:
             value = int(match.group(1))
             unit = match.group(2).lower()
 
-            if unit == 'h':
+            if unit == "h":
                 return value * 3600  # Convert hours to seconds
-            elif unit == 'min':
+            elif unit == "min":
                 return value * 60  # Convert minutes to seconds
         else:
-            raise SyntaxError("Invalid input format it should be like 50Min for M and 23H for hours")
+            raise SyntaxError(
+                "Invalid input format it should be like 50Min for M and 23H for hours")
 
     def __call__(self):
         return {k: v for k, v in self.__dict__.items()}
@@ -231,7 +221,7 @@
 
         :param self: Represent the instance of the class
         :return: A dictionary of hyperparameters
-        
+
         """
         return {f"hyperparameters/{k}": v for k, v in self.__dict__.items() if
                 isinstance(v, (int, float, str, bool, torch.Tensor))}
@@ -243,33 +233,33 @@
 
         :param self: Pass the class instance to the function
         :return: A wandb
-        
+
         """
         return wandb.init(
-            project=f'easydel-{self.model_name}',
+            project=f"easydel-{self.model_name}",
             config=self(),
             tags=[
-                'Easy Del',
-                'OST-OpenSourceTransformers',
-                'Jax/Flax'
+                "Easy Del",
+                "OST-OpenSourceTransformers",
+                "Jax/Flax"
             ]
         )
 
     def __str__(self):
-        string = f'TrainingArguments(\n'
+        string = f"TrainingArguments(\n"
         for k, v in self.__call__().items():
             if isinstance(v, typing.Callable):
                 def string_func(it_self):
-                    string_ = f'{it_self.__class__.__name__}(\n'
+                    string_ = f"{it_self.__class__.__name__}(\n"
                     for k_, v_ in it_self.__dict__.items():
-                        string_ += f'\t\t{k_} : {v_}\n'
-                    string_ += '\t)'
+                        string_ += f"\t\t{k_} : {v_}\n"
+                    string_ += "\t)"
                     return string_
 
                 v.__str__ = string_func
                 v = v.__str__(v)
-            string += f'\t{k} : {v}\n'
-        string += ')'
+            string += f"\t{k} : {v}\n"
+        string += ")"
         return string
 
     def get_path(self):
@@ -284,7 +274,7 @@
 
         :param self: Represent the instance of the class
         :return: A pathlib
-        
+
         """
         return pathlib.Path(
             self.save_dir, self.model_name
@@ -296,7 +286,7 @@
 
         :param self: Represent the instance of the class
         :return: A path
-        
+
         """
         path = self.get_path()
         if not path.exists():
@@ -313,7 +303,7 @@
 
         :param self: Refer to the object itself
         :return: A mesh object with the device array shape and the mesh names
-        
+
         """
         return Mesh(
             create_device_mesh(
@@ -337,13 +327,13 @@
         :param self: Represent the instance of the class
         :param steps: Calculate the number of steps to train
         :return: A tuple of two objects:
-        
+
         """
         steps = self.max_steps or steps
-        assert steps is not None, 'if you haven\'t pass max steps to init you should pass init in func'
+        assert steps is not None, "if you haven\'t pass max steps to init you should pass init in func"
 
-        if self.optimizer == 'adafactor':
-            if self.scheduler == 'linear':
+        if self.optimizer == EasyDelOptimizers.ADAFACTOR:
+            if self.scheduler == EasyDelSchedulers.LINEAR:
                 tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     learning_rate_end=self.learning_rate_end,
@@ -351,14 +341,14 @@
                     steps=steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'cosine':
+            elif self.scheduler == EasyDelSchedulers.COSINE:
                 tx, sc = fjformer.optimizers.get_adafactor_with_cosine_scheduler(
                     learning_rate=self.learning_rate,
                     steps=steps,
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'none':
+            elif self.scheduler == EasyDelSchedulers.NONE:
                 tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     learning_rate_end=self.learning_rate,
@@ -366,7 +356,7 @@
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'warm_up_cosine':
+            elif self.scheduler == EasyDelSchedulers.WARM_UP_COSINE:
                 tx, sc = fjformer.optimizers.get_adafactor_with_warm_up_cosine_scheduler(
                     learning_rate=self.learning_rate,
                     steps=steps,
@@ -374,7 +364,7 @@
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'warm_up_linear':
+            elif self.scheduler == EasyDelSchedulers.WARM_UP_LINEAR:
                 tx, sc = fjformer.optimizers.get_adafactor_with_warmup_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     steps=steps,
@@ -386,9 +376,11 @@
                 )
 
             else:
-                raise ValueError('seems like you have choose wrong type or unavailable scheduler')
-        elif self.optimizer == 'lion':
-            if self.scheduler == 'linear':
+                raise ValueError(
+                    "seems like you have choose wrong type or unavailable scheduler"
+                )
+        elif self.optimizer == EasyDelOptimizers.LION:
+            if self.scheduler == EasyDelSchedulers.LINEAR:
                 tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     learning_rate_end=self.learning_rate_end,
@@ -396,14 +388,14 @@
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'cosine':
+            elif self.scheduler == EasyDelSchedulers.COSINE:
                 tx, sc = fjformer.optimizers.get_lion_with_cosine_scheduler(
                     learning_rate=self.learning_rate,
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
                     steps=steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'none':
+            elif self.scheduler == EasyDelSchedulers.NONE:
                 tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     learning_rate_end=self.learning_rate,
@@ -411,7 +403,7 @@
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'warm_up_cosine':
+            elif self.scheduler == EasyDelSchedulers.WARM_UP_COSINE:
                 tx, sc = fjformer.optimizers.get_lion_with_warm_up_cosine_scheduler(
                     learning_rate=self.learning_rate,
                     steps=steps,
@@ -419,7 +411,7 @@
                     **self.extra_optimizer_kwargs
                 )
 
-            elif self.scheduler == 'warm_up_linear':
+            elif self.scheduler == EasyDelSchedulers.WARM_UP_LINEAR:
                 tx, sc = fjformer.optimizers.get_lion_with_with_warmup_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     steps=steps,
@@ -429,9 +421,10 @@
                     **self.extra_optimizer_kwargs
                 )
             else:
-                raise ValueError('seems like you have choose wrong type or unavailable scheduler')
-        elif self.optimizer == 'adamw':
-            if self.scheduler == 'linear':
+                raise ValueError(
+                    "seems like you have choose wrong type or unavailable scheduler")
+        elif self.optimizer == EasyDelOptimizers.ADAMW:
+            if self.scheduler == EasyDelSchedulers.LINEAR:
                 tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     learning_rate_end=self.learning_rate_end,
@@ -439,7 +432,7 @@
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'cosine':
+            elif self.scheduler == EasyDelSchedulers.COSINE:
                 tx, sc = fjformer.optimizers.get_adamw_with_cosine_scheduler(
                     learning_rate=self.learning_rate,
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
@@ -447,7 +440,7 @@
                     weight_decay=self.weight_decay,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'none':
+            elif self.scheduler == EasyDelSchedulers.NONE:
                 tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     learning_rate_end=self.learning_rate,
@@ -455,7 +448,7 @@
                     steps=steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'warm_up_cosine':
+            elif self.scheduler == EasyDelSchedulers.WARM_UP_COSINE:
                 tx, sc = fjformer.optimizers.get_adamw_with_warm_up_cosine_scheduler(
                     learning_rate=self.learning_rate,
                     steps=steps,
@@ -463,7 +456,7 @@
                     gradient_accumulation_steps=self.gradient_accumulation_steps,
                     **self.extra_optimizer_kwargs
                 )
-            elif self.scheduler == 'warm_up_linear':
+            elif self.scheduler == EasyDelSchedulers.WARM_UP_LINEAR:
                 tx, sc = fjformer.optimizers.get_adamw_with_warmup_linear_scheduler(
                     learning_rate_start=self.learning_rate,
                     steps=steps,
@@ -474,9 +467,13 @@
                     **self.extra_optimizer_kwargs
                 )
             else:
-                raise ValueError('seems like you have choose wrong type or unavailable scheduler')
+                raise ValueError(
+                    "seems like you have choose wrong type or unavailable scheduler"
+                )
         else:
-            raise ValueError('seems like you have choose wrong type or unavailable optimizer')
+            raise ValueError(
+                "seems like you have choose wrong type or unavailable optimizer"
+            )
         return tx, sc
 
     def get_streaming_checkpointer(self):
@@ -488,7 +485,7 @@
 
         :param self: Represent the instance of the class
         :return: A streamingcheckpointer object
-        
+
         """
         return StreamingCheckpointer(StreamingCheckpointer.get_default_config(),
                                      os.path.join(self.save_dir, self.model_name))
@@ -502,10 +499,10 @@
 
         :param self: Represent the instance of the class
         :return: A summary-writer object
-        
+
         """
         return torch.utils.tensorboard.SummaryWriter(
             log_dir=str(self.get_path()),
-            comment=f'{self.model_name}',
-            filename_suffix='easydel'
+            comment=f"{self.model_name}",
+            filename_suffix="easydel"
         )
Index: lib/python/EasyDel/etils/errors.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/etils/errors.py b/lib/python/EasyDel/etils/errors.py
new file mode 100644
--- /dev/null	(date 1703504083518)
+++ b/lib/python/EasyDel/etils/errors.py	(date 1703504083518)
@@ -0,0 +1,6 @@
+class EasyDelRuntimeError(Exception):
+    ...
+
+
+class EasyDeSyntaxRuntimeError(Exception):
+    ...
Index: lib/python/EasyDel/configs/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/configs/__init__.py b/lib/python/EasyDel/etils/__init__.py
rename from lib/python/EasyDel/configs/__init__.py
rename to lib/python/EasyDel/etils/__init__.py
--- a/lib/python/EasyDel/configs/__init__.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/etils/__init__.py	(date 1703504083518)
@@ -7,3 +7,9 @@
     llama_2_configs,
     get_config
 )
+
+from .etils import (
+    EasyDelGradientCheckPointers,
+    EasyDelOptimizers,
+    EasyDelSchedulers
+)
Index: mkdocs.yml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>site_name: EasyDel\n\ncopyright: \"Erfan Zare Chavoshi-EasyDel\"\nnav:\n  - Home: index.md\n  - Install: Install.md\n  - AvailableModels: AvailableModels.md\n  - EasyBIT: Bits.md\n  - Eval:\n      - lm_eval: lib-python-EasyDel-eval-lm_eval.md\n  - Transform:\n      - easydel_transform: lib-python-EasyDel-transform-easydel_transform.md\n      - mpt: lib-python-EasyDel-transform-mpt.md\n      - llama: lib-python-EasyDel-transform-llama.md\n      - falcon: lib-python-EasyDel-transform-falcon.md\n      - mistral: lib-python-EasyDel-transform-mistral.md\n      - utils: lib-python-EasyDel-transform-utils.md\n  - smi: lib-python-EasyDel-smi-smi.md\n  - Serve:\n      - torch_serve: lib-python-EasyDel-serve-torch_serve.md\n      - jax_serve: lib-python-EasyDel-serve-jax_serve.md\n      - utils: lib-python-EasyDel-serve-utils.md\n  - Modules:\n      - Falcon: lib-python-EasyDel-modules-falcon-modelling_falcon_flax.md\n      - Palm: lib-python-EasyDel-modules-palm-modelling_palm_flax.md\n      - flax_modelling_utils: lib-python-EasyDel-modules-flax_modelling_utils.md\n      - GPT-Neox: lib-python-EasyDel-modules-gpt_neo_x-modelling_gpt_neo_x_flax.md\n      - GPT-J: lib-python-EasyDel-modules-gpt_j-modelling_gpt_j_flax.md\n      - Mistral: lib-python-EasyDel-modules-mistral-modelling_mistral_flax.md\n      - auto_models: lib-python-EasyDel-modules-auto_models.md\n      - LucidTransformers: lib-python-EasyDel-modules-lucid_transformer-modelling_lt_flax.md\n      - OPT: lib-python-EasyDel-modules-opt-modelling_opt_flax.md\n      - MPT: lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax.md\n      - Llama: lib-python-EasyDel-modules-llama-modelling_llama_flax.md\n      - Mixtral: lib-python-EasyDel-modules-mixtral-modelling_mixtral_flax.md\n      - PHI: lib-python-EasyDel-modules-phi-modelling_phi_flax.md\n      - T5: lib-python-EasyDel-modules-t5-modelling_t5_flax.md\n  - configs: lib-python-EasyDel-configs-configs.md\n  - Utils:\n      - tensor_utils: lib-python-EasyDel-utils-tensor_utils.md\n      - prompters: lib-python-EasyDel-utils-prompters.md\n      - utils: lib-python-EasyDel-utils-utils.md\n      - checker: lib-python-EasyDel-utils-checker.md\n  - RLHF:\n      - trainer: lib-python-EasyDel-rlhf-trainer.md\n      - reward: lib-python-EasyDel-rlhf-reward.md\n      - ppo: lib-python-EasyDel-rlhf-ppo.md\n      - utils: lib-python-EasyDel-rlhf-utils.md\n\n  - Trainer:\n      - fsdp_train: lib-python-EasyDel-trainer-fsdp_train.md\n      - config: lib-python-EasyDel-trainer-config.md\n      - tf_dataset: lib-python-EasyDel-trainer-tf_dataset.md\n      - training_utils: lib-python-EasyDel-trainer-training_utils.md\n  - Linen:\n      - bits: lib-python-EasyDel-linen-bits.md\n      - utils: lib-python-EasyDel-linen-utils.md\n  - RL Trainer:\n      core: lib-python-EasyDel-rl_trainer-core.md\n      models:\n        modelling_base: lib-python-EasyDel-rl_trainer-models-modelling_base.md\n        modelling_value_head: lib-python-EasyDel-rl_trainer-models-modelling_value_head.md\n      trainer:\n        base: lib-python-EasyDel-rl_trainer-trainer-base.md\n        ppo_trainer: lib-python-EasyDel-rl_trainer-trainer-ppo_trainer.md\n  - Data Preprocessing:\n      - data_preprocessing/_processor: lib-python-EasyDel-data_preprocessing-_processor.md\n  - Examples:\n      - PytorchServer: PyTorchServer.md\n      - JAXServer: JAXServer.md\n      - DataProcessing: DataProcessing.md\n      - TrainingExample: TrainingExample.md\n      - Falcon Models: Falcon.md\n      - Llama Models: Llama.md\n      - Llama2 Models: Llama2.md\n      - Mistral Models: Mistral.md\n      - MosaicMPT Models: MosaicMPT.md\n\nsite_author: \"Erfan Zare Chavoshi\"\nrepo_url: \"https://github.com/erfanzar/EasyDel\"\n\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          options:\n            docstring_style: sphinx\n\ntheme:\n  name: material\n  highlightjs: true\n  hljs_languages:\n    - yaml\n    - python\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/mkdocs.yml b/mkdocs.yml
--- a/mkdocs.yml	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/mkdocs.yml	(date 1703504083518)
@@ -7,64 +7,67 @@
   - AvailableModels: AvailableModels.md
   - EasyBIT: Bits.md
   - Eval:
-      - lm_eval: lib-python-EasyDel-eval-lm_eval.md
+      - lm_eval: eval-lm_eval.md
   - Transform:
-      - easydel_transform: lib-python-EasyDel-transform-easydel_transform.md
-      - mpt: lib-python-EasyDel-transform-mpt.md
-      - llama: lib-python-EasyDel-transform-llama.md
-      - falcon: lib-python-EasyDel-transform-falcon.md
-      - mistral: lib-python-EasyDel-transform-mistral.md
-      - utils: lib-python-EasyDel-transform-utils.md
-  - smi: lib-python-EasyDel-smi-smi.md
+      - easydel_transform: transform-easydel_transform.md
+      - mpt: transform-mpt.md
+      - llama: transform-llama.md
+      - falcon: transform-falcon.md
+      - mistral: transform-mistral.md
+      - utils: transform-utils.md
+  - smi: smi-smi.md
   - Serve:
-      - torch_serve: lib-python-EasyDel-serve-torch_serve.md
-      - jax_serve: lib-python-EasyDel-serve-jax_serve.md
-      - utils: lib-python-EasyDel-serve-utils.md
+      - torch_serve: serve-torch_serve.md
+      - jax_serve: serve-jax_serve.md
+      - utils: serve-utils.md
   - Modules:
-      - Falcon: lib-python-EasyDel-modules-falcon-modelling_falcon_flax.md
-      - Palm: lib-python-EasyDel-modules-palm-modelling_palm_flax.md
-      - flax_modelling_utils: lib-python-EasyDel-modules-flax_modelling_utils.md
-      - GPT-Neox: lib-python-EasyDel-modules-gpt_neo_x-modelling_gpt_neo_x_flax.md
-      - GPT-J: lib-python-EasyDel-modules-gpt_j-modelling_gpt_j_flax.md
-      - Mistral: lib-python-EasyDel-modules-mistral-modelling_mistral_flax.md
-      - auto_models: lib-python-EasyDel-modules-auto_models.md
-      - LucidTransformers: lib-python-EasyDel-modules-lucid_transformer-modelling_lt_flax.md
-      - OPT: lib-python-EasyDel-modules-opt-modelling_opt_flax.md
-      - MPT: lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax.md
-      - Llama: lib-python-EasyDel-modules-llama-modelling_llama_flax.md
-      - Mixtral: lib-python-EasyDel-modules-mixtral-modelling_mixtral_flax.md
-      - PHI: lib-python-EasyDel-modules-phi-modelling_phi_flax.md
-      - T5: lib-python-EasyDel-modules-t5-modelling_t5_flax.md
-  - configs: lib-python-EasyDel-configs-configs.md
+      - Falcon: modules-falcon-modelling_falcon_flax.md
+      - Palm: modules-palm-modelling_palm_flax.md
+      - flax_modelling_utils: modules-flax_modelling_utils.md
+      - GPT-Neox: modules-gpt_neo_x-modelling_gpt_neo_x_flax.md
+      - GPT-J: modules-gpt_j-modelling_gpt_j_flax.md
+      - Mistral: modules-mistral-modelling_mistral_flax.md
+      - auto_models: modules-auto_models.md
+      - LucidTransformers: modules-lucid_transformer-modelling_lt_flax.md
+      - OPT: modules-opt-modelling_opt_flax.md
+      - MPT: modules-mosaic_mpt-modelling_mpt_flax.md
+      - Llama: modules-llama-modelling_llama_flax.md
+      - Mixtral: modules-mixtral-modelling_mixtral_flax.md
+      - PHI: modules-phi-modelling_phi_flax.md
+      - T5: modules-t5-modelling_t5_flax.md
+  - Etils:
+      - configs: etils-configs.md
+      - Errors: etils-errors.md
+      - Etils: etils-etils.md
   - Utils:
-      - tensor_utils: lib-python-EasyDel-utils-tensor_utils.md
-      - prompters: lib-python-EasyDel-utils-prompters.md
-      - utils: lib-python-EasyDel-utils-utils.md
-      - checker: lib-python-EasyDel-utils-checker.md
+      - tensor_utils: utils-tensor_utils.md
+      - prompters: utils-prompters.md
+      - utils: utils-utils.md
+      - checker: utils-checker.md
   - RLHF:
-      - trainer: lib-python-EasyDel-rlhf-trainer.md
-      - reward: lib-python-EasyDel-rlhf-reward.md
-      - ppo: lib-python-EasyDel-rlhf-ppo.md
-      - utils: lib-python-EasyDel-rlhf-utils.md
+      - trainer: rlhf-trainer.md
+      - reward: rlhf-reward.md
+      - ppo: rlhf-ppo.md
+      - utils: rlhf-utils.md
 
   - Trainer:
-      - fsdp_train: lib-python-EasyDel-trainer-fsdp_train.md
-      - config: lib-python-EasyDel-trainer-config.md
-      - tf_dataset: lib-python-EasyDel-trainer-tf_dataset.md
-      - training_utils: lib-python-EasyDel-trainer-training_utils.md
+      - fsdp_train: trainer-fsdp_train.md
+      - config: trainer-config.md
+      - tf_dataset: trainer-tf_dataset.md
+      - training_utils: trainer-training_utils.md
   - Linen:
-      - bits: lib-python-EasyDel-linen-bits.md
-      - utils: lib-python-EasyDel-linen-utils.md
+      - bits: linen-bits.md
+      - utils: linen-utils.md
   - RL Trainer:
-      core: lib-python-EasyDel-rl_trainer-core.md
+      core: rl_trainer-core.md
       models:
-        modelling_base: lib-python-EasyDel-rl_trainer-models-modelling_base.md
-        modelling_value_head: lib-python-EasyDel-rl_trainer-models-modelling_value_head.md
+        modelling_base: rl_trainer-models-modelling_base.md
+        modelling_value_head: rl_trainer-models-modelling_value_head.md
       trainer:
-        base: lib-python-EasyDel-rl_trainer-trainer-base.md
-        ppo_trainer: lib-python-EasyDel-rl_trainer-trainer-ppo_trainer.md
+        base: rl_trainer-trainer-base.md
+        ppo_trainer: rl_trainer-trainer-ppo_trainer.md
   - Data Preprocessing:
-      - data_preprocessing/_processor: lib-python-EasyDel-data_preprocessing-_processor.md
+      - data_preprocessing/_processor: data_preprocessing-_processor.md
   - Examples:
       - PytorchServer: PyTorchServer.md
       - JAXServer: JAXServer.md
Index: lib/python/EasyDel/rl_trainer/trainer/ppo_trainer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from jax import grad, jit, numpy as jnp, lax\nfrom ..models import FlaxAutoModelForCausalLMWithValueHead, FlaxPreTrainedModelWrapper\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/rl_trainer/trainer/ppo_trainer.py b/lib/python/EasyDel/rl_trainer/trainer/ppo_trainer.py
--- a/lib/python/EasyDel/rl_trainer/trainer/ppo_trainer.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/rl_trainer/trainer/ppo_trainer.py	(date 1703504083518)
@@ -1,2 +1,109 @@
+from curses import version
+import inspect
 from jax import grad, jit, numpy as jnp, lax
 from ..models import FlaxAutoModelForCausalLMWithValueHead, FlaxPreTrainedModelWrapper
+from chex import Array
+from typing import List, Optional, Union, Callable
+from transformers import PreTrainedTokenizerBase
+import datasets
+import torch
+from .ppo_config import PPOConfig
+
+Dataset = datasets.Dataset
+
+
+class PPOTrainer:
+    def __init__(
+        self,
+        config: PPOConfig = None,
+        model: FlaxPreTrainedModelWrapper = None,
+        ref_model: Optional[FlaxPreTrainedModelWrapper] = None,
+        tokenizer: PreTrainedTokenizerBase = None,
+        dataset: Optional[Union[torch.utils.data.Dataset, Dataset]] = None,
+        optimizer=None,
+        data_collator: Optional[Callable] = None,
+        num_shared_layers: Optional[int] = None,
+        lr_scheduler=None,
+    ):
+        ...
+
+    def step(
+        self,
+        queries: List[Array],
+        responses: List[Array],
+        scores: List[Array],
+        response_masks: Optional[List[Array]] = None,
+    ):
+        """
+        Run a PPO optimisation step given a list of queries, model responses, and rewards.
+
+        Args:
+            queries (List[`Array`]):
+                List of tensors containing the encoded queries of shape (`query_length`)
+            responses (List[`Array`]):
+                List of tensors containing the encoded responses of shape (`response_length`)
+            scores (List[`Array`]):
+                List of tensors containing the scores.
+            response_masks (List[`Array`], *optional*)):
+                List of tensors containing masks of the response tokens.
+
+        Returns:
+            `dict[str, Any]`: A summary of the training statistics
+        """
+
+        ...
+
+    def prepare_dataloader(self, dataset: Union[torch.utils.data.Dataset, Dataset], data_collator=None):
+        """
+        Prepare the dataloader for training.
+
+        Args:
+            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):
+                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset
+                will be preprocessed by removing the columns that are not used by the model.
+            data_collator (Optional[function]):
+                Data collator function.
+
+        Returns:
+            `torch.utils.data.DataLoader`: PyTorch dataloader
+        """
+        if isinstance(dataset, Dataset):
+            dataset = self._remove_unused_columns(dataset)
+        dataloader = torch.utils.data.DataLoader(
+            dataset,
+            batch_size=self.config.batch_size,
+            collate_fn=data_collator,
+            shuffle=True,
+            drop_last=True,
+        )
+        return dataloader
+
+    
+    def _set_signature_columns_if_needed(self):
+        if self._signature_columns is None:
+            
+            signature = inspect.signature(self.model.forward)
+            self._signature_columns = list(signature.parameters.keys())
+            
+            self._signature_columns += ["label", "query", "response"]
+
+    def _remove_unused_columns(self, dataset: "Dataset"):
+        if not self.config.remove_unused_columns:
+            return dataset
+        self._set_signature_columns_if_needed()
+        signature_columns = self._signature_columns
+
+        ignored_columns = list(
+            set(dataset.column_names) - set(signature_columns))
+
+        columns = [k for k in signature_columns if k in dataset.column_names]
+
+        if version.parse(datasets.__version__) < version.parse("1.4.0"):
+            dataset.set_format(
+                type=dataset.format["type"],
+                columns=columns,
+                format_kwargs=dataset.format["format_kwargs"],
+            )
+            return dataset
+        else:
+            return dataset.remove_columns(ignored_columns)
Index: lib/python/EasyDel/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from .serve.torch_serve import (\n    PyTorchServer as PyTorchServer,\n    PytorchServerConfig as PytorchServerConfig\n)\nfrom .serve.jax_serve import (\n    JAXServer as JAXServer,\n    JAXServerConfig as JAXServerConfig\n)\nfrom .modules.llama.modelling_llama_flax import (\n    LlamaConfig as LlamaConfig,\n    FlaxLlamaForCausalLM as FlaxLlamaForCausalLM,\n    FlaxLlamaModel as FlaxLlamaModel\n)\nfrom .modules.gpt_j.modelling_gpt_j_flax import (\n    GPTJConfig as GPTJConfig,\n    FlaxGPTJForCausalLM as FlaxGPTJForCausalLM,\n    FlaxGPTJModule as FlaxGPTJModule,\n    FlaxGPTJModel as FlaxGPTJModel,\n    FlaxGPTJForCausalLMModule as FlaxGPTJForCausalLMModule\n)\nfrom .modules.t5.modelling_t5_flax import (\n    T5Config as T5Config,\n    FlaxT5ForConditionalGeneration as FlaxT5ForConditionalGeneration,\n    FlaxT5Model as FlaxT5Model\n)\nfrom .modules.falcon.modelling_falcon_flax import (\n    FalconConfig as FalconConfig,\n    FlaxFalconModel as FlaxFalconModel,\n    FlaxFalconForCausalLM as FlaxFalconForCausalLM\n)\nfrom .modules.opt.modelling_opt_flax import (\n    OPTConfig as OPTConfig,\n    FlaxOPTForCausalLM as FlaxOPTForCausalLM,\n    FlaxOPTModel as FlaxOPTModel\n)\nfrom .modules.mistral.modelling_mistral_flax import (\n    MistralConfig as MistralConfig,\n    FlaxMistralForCausalLM as FlaxMistralForCausalLM,\n    FlaxMistralModule as FlaxMistralModule\n)\nfrom .modules.palm.modelling_palm_flax import (\n    PalmModel as PalmModel,\n    PalmConfig as PalmConfig,\n    FlaxPalmForCausalLM as FlaxPalmForCausalLM\n)\n\nfrom .modules.mosaic_mpt.modelling_mpt_flax import (\n    MptConfig as MptConfig,\n    FlaxMptForCausalLM as FlaxMptForCausalLM,\n    FlaxMptModel as FlaxMptModel\n)\n\nfrom .modules.gpt_neo_x.modelling_gpt_neo_x_flax import (\n    GPTNeoXConfig as GPTNeoXConfig,\n    FlaxGPTNeoXModel as FlaxGPTNeoXModel,\n    FlaxGPTNeoXForCausalLM as FlaxGPTNeoXForCausalLM\n)\n\nfrom .modules.lucid_transformer.modelling_lt_flax import (\n    FlaxLTModel as FlaxLTModel,\n    FlaxLTModelModule as FlaxLTModelModule,\n    FlaxLTConfig as FlaxLTConfig,\n    FlaxLTForCausalLM as FlaxLTForCausalLM\n)\n\nfrom .modules.gpt2.modelling_gpt2_flax import (\n    # GPT2 code is from huggingface but in the version of huggingface they don't support gradient checkpointing\n    # and pjit attention force\n    GPT2Config as GPT2Config,\n    FlaxGPT2LMHeadModel as FlaxGPT2LMHeadModel,\n    FlaxGPT2Model as FlaxGPT2Model\n)\n\nfrom .modules.mixtral.modelling_mixtral_flax import (\n    FlaxMixtralForCausalLM as FlaxMixtralForCausalLM,\n    FlaxMixtralModel as FlaxMixtralModel,\n    MixtralConfig as MixtralConfig\n)\n\nfrom .modules.auto_models import (\n    AutoEasyDelModelForCausalLM as AutoEasyDelModelForCausalLM,\n    get_modules_by_type as get_modules_by_type\n)\n\nfrom .utils.utils import (\n    get_mesh as get_mesh,\n    names_in_mesh as names_in_mesh,\n    get_names_from_partition_spec as get_names_from_partition_spec,\n    make_shard_and_gather_fns as make_shard_and_gather_fns,\n    with_sharding_constraint as with_sharding_constraint,\n    RNG as RNG\n)\n\nfrom .trainer import (\n    CausalLMTrainer,\n    TrainArguments,\n    create_fsdp_eval_step,\n    create_fsdp_train_step,\n    get_training_modules\n)\n\nfrom .linen import (\n    from_8bit as from_8bit,\n    Dense8Bit as Dense8Bit,\n    array_from_8bit as array_from_8bit,\n    array_to_bit8 as array_to_bit8,\n    to_8bit as to_8bit\n)\nfrom .smi import (\n    run as run,\n    initialise_tracking as initialise_tracking,\n    get_mem as get_mem\n)\n\nfrom .transform.llama import (\n    llama_from_pretrained as llama_from_pretrained,\n    llama_convert_flax_to_pt as llama_convert_flax_to_pt,\n    llama_convert_hf_to_flax_load as llama_convert_hf_to_flax_load,\n    llama_convert_hf_to_flax as llama_convert_hf_to_flax,\n    llama_easydel_to_hf as llama_easydel_to_hf\n)\nfrom .transform.mpt import (\n    mpt_convert_flax_to_pt_1b as mpt_convert_flax_to_pt_1b,\n    mpt_convert_pt_to_flax_1b as mpt_convert_pt_to_flax_1b,\n    mpt_convert_pt_to_flax_7b as mpt_convert_pt_to_flax_7b,\n    mpt_convert_flax_to_pt_7b as mpt_convert_flax_to_pt_7b,\n    mpt_from_pretrained as mpt_from_pretrained\n)\n\nfrom .transform.falcon import (\n    falcon_convert_pt_to_flax_7b as falcon_convert_pt_to_flax_7b,\n    falcon_convert_flax_to_pt_7b as falcon_convert_flax_to_pt_7b,\n    falcon_from_pretrained as falcon_from_pretrained,\n    falcon_convert_hf_to_flax as falcon_convert_hf_to_flax,\n    falcon_easydel_to_hf as falcon_easydel_to_hf\n)\nfrom .transform.mistral import (\n    mistral_convert_hf_to_flax as mistral_convert_hf_to_flax,\n    mistral_convert_hf_to_flax_load as mistral_convert_hf_to_flax_load,\n    mistral_convert_flax_to_pt as mistral_convert_flax_to_pt,\n    mistral_from_pretrained as mistral_from_pretrained,\n    mistral_convert_pt_to_flax as mistral_convert_pt_to_flax,\n    mistral_easydel_to_hf as mistral_easydel_to_hf\n)\n\n__version__ = \"0.0.40\"\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/__init__.py b/lib/python/EasyDel/__init__.py
--- a/lib/python/EasyDel/__init__.py	(revision 6c3c346fc493796ceb932c09a1456a6fe0896a85)
+++ b/lib/python/EasyDel/__init__.py	(date 1703504083518)
@@ -143,4 +143,10 @@
     mistral_easydel_to_hf as mistral_easydel_to_hf
 )
 
+from .etils import (
+    EasyDelGradientCheckPointers as EasyDelGradientCheckPointers,
+    EasyDelOptimizers as EasyDelOptimizers,
+    EasyDelSchedulers as EasyDelSchedulers
+)
+
 __version__ = "0.0.40"
diff --git a/.idea/shelf/Uncommitted_changes_before_Checkout_at_12_16_23,_10_35_PM_[Changes]1/shelved.patch b/.idea/shelf/Uncommitted_changes_before_Checkout_at_12_16_23,_10_35_PM_[Changes]1/shelved.patch
deleted file mode 100644
diff --git a/docs/lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax.md b/docs/modules-mosaic_mpt-modelling_mpt_flax.md
rename from docs/lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax.md
rename to docs/modules-mosaic_mpt-modelling_mpt_flax.md
diff --git a/docs/lib-python-EasyDel-trainer-config.md b/docs/trainer-config.md
rename from docs/lib-python-EasyDel-trainer-config.md
rename to docs/trainer-config.md
diff --git a/docs/lib-python-EasyDel-transform-llama.md b/docs/transform-llama.md
rename from docs/lib-python-EasyDel-transform-llama.md
rename to docs/transform-llama.md
diff --git a/docs/lib-python-EasyDel-modules-auto_models.md b/docs/modules-auto_models.md
rename from docs/lib-python-EasyDel-modules-auto_models.md
rename to docs/modules-auto_models.md
diff --git a/docs/lib-python-EasyDel-modules-palm-modelling_palm_flax.md b/docs/modules-palm-modelling_palm_flax.md
rename from docs/lib-python-EasyDel-modules-palm-modelling_palm_flax.md
rename to docs/modules-palm-modelling_palm_flax.md
diff --git a/docs/lib-python-EasyDel-rl_trainer-models-modelling_base.md b/docs/rl_trainer-models-modelling_base.md
rename from docs/lib-python-EasyDel-rl_trainer-models-modelling_base.md
rename to docs/rl_trainer-models-modelling_base.md
diff --git a/docs/lib-python-EasyDel-serve-utils.md b/docs/serve-utils.md
rename from docs/lib-python-EasyDel-serve-utils.md
rename to docs/serve-utils.md
diff --git a/docs/lib-python-EasyDel-rl_trainer-trainer-ppo_config.md b/docs/rl_trainer-trainer-ppo_config.md
rename from docs/lib-python-EasyDel-rl_trainer-trainer-ppo_config.md
rename to docs/rl_trainer-trainer-ppo_config.md
diff --git a/docs/lib-python-EasyDel-serve-torch_serve.md b/docs/serve-torch_serve.md
rename from docs/lib-python-EasyDel-serve-torch_serve.md
rename to docs/serve-torch_serve.md
diff --git a/docs/lib-python-EasyDel-modules-gpt_neo_x-modelling_gpt_neo_x_flax.md b/docs/modules-gpt_neo_x-modelling_gpt_neo_x_flax.md
rename from docs/lib-python-EasyDel-modules-gpt_neo_x-modelling_gpt_neo_x_flax.md
rename to docs/modules-gpt_neo_x-modelling_gpt_neo_x_flax.md
diff --git a/docs/lib-python-EasyDel-modules-opt-modelling_opt_flax.md b/docs/modules-opt-modelling_opt_flax.md
rename from docs/lib-python-EasyDel-modules-opt-modelling_opt_flax.md
rename to docs/modules-opt-modelling_opt_flax.md
diff --git a/docs/lib-python-EasyDel-modules-phi-modelling_phi_flax.md b/docs/modules-phi-modelling_phi_flax.md
rename from docs/lib-python-EasyDel-modules-phi-modelling_phi_flax.md
rename to docs/modules-phi-modelling_phi_flax.md
diff --git a/docs/lib-python-EasyDel-modules-llama-modelling_llama_flax.md b/docs/modules-llama-modelling_llama_flax.md
rename from docs/lib-python-EasyDel-modules-llama-modelling_llama_flax.md
rename to docs/modules-llama-modelling_llama_flax.md
diff --git a/docs/lib-python-EasyDel-modules-mixtral-modelling_mixtral_flax.md b/docs/modules-mixtral-modelling_mixtral_flax.md
rename from docs/lib-python-EasyDel-modules-mixtral-modelling_mixtral_flax.md
rename to docs/modules-mixtral-modelling_mixtral_flax.md
diff --git a/docs/lib-python-EasyDel-modules-mistral-modelling_mistral_flax.md b/docs/modules-mistral-modelling_mistral_flax.md
rename from docs/lib-python-EasyDel-modules-mistral-modelling_mistral_flax.md
rename to docs/modules-mistral-modelling_mistral_flax.md
diff --git a/docs/lib-python-EasyDel-erros-errors.md b/docs/erros-errors.md
rename from docs/lib-python-EasyDel-erros-errors.md
rename to docs/erros-errors.md
diff --git a/docs/lib-python-EasyDel-smi-smi.md b/docs/smi-smi.md
rename from docs/lib-python-EasyDel-smi-smi.md
rename to docs/smi-smi.md
diff --git a/docs/lib-python-EasyDel-utils-utils.md b/docs/utils-utils.md
rename from docs/lib-python-EasyDel-utils-utils.md
rename to docs/utils-utils.md
diff --git a/docs/lib-python-EasyDel-trainer-tf_dataset.md b/docs/trainer-tf_dataset.md
rename from docs/lib-python-EasyDel-trainer-tf_dataset.md
rename to docs/trainer-tf_dataset.md
diff --git a/docs/lib-python-EasyDel-utils-checker.md b/docs/utils-checker.md
rename from docs/lib-python-EasyDel-utils-checker.md
rename to docs/utils-checker.md
diff --git a/docs/lib-python-EasyDel-modules-gpt_j-modelling_gpt_j_flax.md b/docs/modules-gpt_j-modelling_gpt_j_flax.md
rename from docs/lib-python-EasyDel-modules-gpt_j-modelling_gpt_j_flax.md
rename to docs/modules-gpt_j-modelling_gpt_j_flax.md
diff --git a/docs/lib-python-EasyDel-eval-lm_eval.md b/docs/eval-lm_eval.md
rename from docs/lib-python-EasyDel-eval-lm_eval.md
rename to docs/eval-lm_eval.md
diff --git a/docs/lib-python-EasyDel-rlhf-trainer.md b/docs/rlhf-trainer.md
rename from docs/lib-python-EasyDel-rlhf-trainer.md
rename to docs/rlhf-trainer.md
diff --git a/docs/lib-python-EasyDel-modules-t5-modelling_t5_flax.md b/docs/modules-t5-modelling_t5_flax.md
rename from docs/lib-python-EasyDel-modules-t5-modelling_t5_flax.md
rename to docs/modules-t5-modelling_t5_flax.md
diff --git a/docs/lib-python-EasyDel-transform-easydel_transform.md b/docs/transform-easydel_transform.md
rename from docs/lib-python-EasyDel-transform-easydel_transform.md
rename to docs/transform-easydel_transform.md
diff --git a/docs/lib-python-EasyDel-rl_trainer-trainer-base.md b/docs/rl_trainer-trainer-base.md
rename from docs/lib-python-EasyDel-rl_trainer-trainer-base.md
rename to docs/rl_trainer-trainer-base.md
diff --git a/docs/lib-python-EasyDel-serve-jax_serve.md b/docs/serve-jax_serve.md
rename from docs/lib-python-EasyDel-serve-jax_serve.md
rename to docs/serve-jax_serve.md
diff --git a/docs/lib-python-EasyDel-partitioning-partitioner.md b/docs/partitioning-partitioner.md
rename from docs/lib-python-EasyDel-partitioning-partitioner.md
rename to docs/partitioning-partitioner.md
diff --git a/docs/lib-python-EasyDel-rl_trainer-trainer-ppo_trainer.md b/docs/rl_trainer-trainer-ppo_trainer.md
rename from docs/lib-python-EasyDel-rl_trainer-trainer-ppo_trainer.md
rename to docs/rl_trainer-trainer-ppo_trainer.md
diff --git a/docs/lib-python-EasyDel-rlhf-utils.md b/docs/rlhf-utils.md
rename from docs/lib-python-EasyDel-rlhf-utils.md
rename to docs/rlhf-utils.md
diff --git a/docs/lib-python-EasyDel-trainer-fsdp_train.md b/docs/trainer-fsdp_train.md
rename from docs/lib-python-EasyDel-trainer-fsdp_train.md
rename to docs/trainer-fsdp_train.md
diff --git a/docs/lib-python-EasyDel-transform-mistral.md b/docs/transform-mistral.md
rename from docs/lib-python-EasyDel-transform-mistral.md
rename to docs/transform-mistral.md
diff --git a/docs/lib-python-EasyDel-linen-utils.md b/docs/linen-utils.md
rename from docs/lib-python-EasyDel-linen-utils.md
rename to docs/linen-utils.md
diff --git a/docs/lib-python-EasyDel-transform-utils.md b/docs/transform-utils.md
rename from docs/lib-python-EasyDel-transform-utils.md
rename to docs/transform-utils.md
diff --git a/docs/lib-python-EasyDel-linen-bits.md b/docs/linen-bits.md
rename from docs/lib-python-EasyDel-linen-bits.md
rename to docs/linen-bits.md
diff --git a/docs/lib-python-EasyDel-rlhf-ppo.md b/docs/rlhf-ppo.md
rename from docs/lib-python-EasyDel-rlhf-ppo.md
rename to docs/rlhf-ppo.md
diff --git a/docs/lib-python-EasyDel-trainer-training_utils.md b/docs/trainer-training_utils.md
rename from docs/lib-python-EasyDel-trainer-training_utils.md
rename to docs/trainer-training_utils.md
diff --git a/docs/lib-python-EasyDel-transform-mpt.md b/docs/transform-mpt.md
rename from docs/lib-python-EasyDel-transform-mpt.md
rename to docs/transform-mpt.md
diff --git a/docs/lib-python-EasyDel-modules-gpt2-modelling_gpt2_flax.md b/docs/modules-gpt2-modelling_gpt2_flax.md
rename from docs/lib-python-EasyDel-modules-gpt2-modelling_gpt2_flax.md
rename to docs/modules-gpt2-modelling_gpt2_flax.md
diff --git a/docs/lib-python-EasyDel-rl_trainer-models-modelling_value_head.md b/docs/rl_trainer-models-modelling_value_head.md
rename from docs/lib-python-EasyDel-rl_trainer-models-modelling_value_head.md
rename to docs/rl_trainer-models-modelling_value_head.md
diff --git a/docs/lib-python-EasyDel-rlhf-reward.md b/docs/rlhf-reward.md
rename from docs/lib-python-EasyDel-rlhf-reward.md
rename to docs/rlhf-reward.md
diff --git a/lib/python/EasyDel/configs/configs.py b/lib/python/EasyDel/etils/configs.py
rename from lib/python/EasyDel/configs/configs.py
rename to lib/python/EasyDel/etils/configs.py
diff --git a/docs/lib-python-EasyDel-modules-flax_modelling_utils.md b/docs/modules-flax_modelling_utils.md
rename from docs/lib-python-EasyDel-modules-flax_modelling_utils.md
rename to docs/modules-flax_modelling_utils.md
diff --git a/docs/lib-python-EasyDel-modules-falcon-modelling_falcon_flax.md b/docs/modules-falcon-modelling_falcon_flax.md
rename from docs/lib-python-EasyDel-modules-falcon-modelling_falcon_flax.md
rename to docs/modules-falcon-modelling_falcon_flax.md
diff --git a/docs/lib-python-EasyDel-utils-prompters.md b/docs/utils-prompters.md
rename from docs/lib-python-EasyDel-utils-prompters.md
rename to docs/utils-prompters.md
diff --git a/docs/lib-python-EasyDel-modules-lucid_transformer-modelling_lt_flax.md b/docs/modules-lucid_transformer-modelling_lt_flax.md
rename from docs/lib-python-EasyDel-modules-lucid_transformer-modelling_lt_flax.md
rename to docs/modules-lucid_transformer-modelling_lt_flax.md
diff --git a/docs/lib-python-EasyDel-transform-falcon.md b/docs/transform-falcon.md
rename from docs/lib-python-EasyDel-transform-falcon.md
rename to docs/transform-falcon.md
diff --git a/docs/lib-python-EasyDel-utils-tensor_utils.md b/docs/utils-tensor_utils.md
rename from docs/lib-python-EasyDel-utils-tensor_utils.md
rename to docs/utils-tensor_utils.md
diff --git a/docs/lib-python-EasyDel-data_preprocessing-_processor.md b/docs/data_preprocessing-_processor.md
rename from docs/lib-python-EasyDel-data_preprocessing-_processor.md
rename to docs/data_preprocessing-_processor.md
diff --git a/docs/lib-python-EasyDel-rl_trainer-core.md b/docs/rl_trainer-core.md
rename from docs/lib-python-EasyDel-rl_trainer-core.md
rename to docs/rl_trainer-core.md
