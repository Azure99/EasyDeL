Index: lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import functools\nimport typing\nfrom typing import Sequence\n\nimport fjformer.attention\nimport flax.core\nfrom jax import numpy as jnp, Array, lax\nfrom jax.experimental.shard_map import shard_map\nfrom jax.sharding import PartitionSpec as PS\nimport jax\nfrom flax import linen as nn\nfrom flax.traverse_util import unflatten_dict, flatten_dict\nfrom flax.core import freeze, unfreeze\nfrom typing import Union, Optional, Tuple\nfrom transformers import PretrainedConfig, FlaxPreTrainedModel\nfrom flax.linen import partitioning as nn_partitioning, combine_masks, dot_product_attention_weights\nfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput\n\nfrom ..flax_modelling_utils import (\n    ACT2FN,\n    with_sharding_constraint,\n    get_gradient_checkpoint_policy,\n    repeat_kv_bnsh,\n    apply_rotary_pos_emb,\n    precompute_freq_cis,\n    JaxBaseClassModel,\n    get_flash_attention,\n    smart_flash_attention, get_dot_general_by_bits\n)\nimport chex\nfrom fjformer.bits import config as q_config, q_flax\n\n\nclass MistralConfig(JaxBaseClassModel):\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=14336,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=8,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096 * 32,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            sliding_window=4096,\n            gradient_checkpointing: str = 'nothing_saveable',\n            use_pjit_attention_force: bool = False,\n            use_flash_attention: bool = False,\n            use_sacn_mlp: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 1024,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            attn_pdrop: float = 0.0,\n            c_max_position_embeddings: int = 4096,\n            freq_max_position_embeddings: int = 4096,\n            bits: Optional[int] = None,\n            **kwargs,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It allows the class to initialize the attributes of a class.\n        The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n        :param self: Represent the instance of the class\n        :param vocab_size: Define the size of the vocabulary\n        :param hidden_size: Determine the size of the embedding layers\n        :param intermediate_size: Define the size of the intermediate layer in each transformer block\n        :param num_hidden_layers: Determine the number of layers in the encoder and decoder\n        :param num_attention_heads: Determine the number of attention heads in each layer\n        :param num_key_value_heads: Specify the number of heads for key and value\n        :param hidden_act: Specify the activation function used in the hidden layers\n        :param max_position_embeddings: Set the maximum length of the sequence\n        :param initializer_range: Initialize the weights of the model\n        :param rms_norm_eps: Avoid division by zero in the rms normalization\n        :param use_cache: Determine whether to use the cache in the decoder\n        :param pad_token_id: Specify the token id of the padding token\n        :param bos_token_id: Specify the beginning of sentence token id\n        :param eos_token_id: Specify the end of sentence token\n        :param tie_word_embeddings: Tie the word embeddings and the output layer\n        :param rope_theta: Control the number of tokens in a rope\n        :param sliding_window: Control the number of tokens that are processed in parallel\n        :param gradient_checkpointing: str: Specify whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Force the use of pjit attention\n        :param use_flash_attention: bool: Enable the flash attention mechanism\n        :param use_sacn_mlp: bool: Determine whether or not to use the scan_mlp function\n        :param flash_attn_query_chunk_size: int: Determine the number of rows in each chunk\n        :param flash_attn_key_chunk_size: int: Control the size of chunks that are used for the key matrix in flash attention\n        :param scan_mlp_chunk_size: int: Specify the chunk size of the scan mlp\n        :param number_rep_kv: int: Specify the number of times to repeat the key and value vectors\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of tokens in a sequence\n        :param freq_max_position_embeddings: int: Set the maximum number of frequency bins that can be used in the model\n        :param bits: Optional[int]: Specify the number of bits used for quantization\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Specify the names of each axis in the tensor\n        :param &quot;mp&quot;): Define the maximum position embeddings\n        :param **kwargs: Pass a variable number of keyword arguments to a function\n        :param : Define the number of layers in the model\n        :return: An instance of the class\n        \n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n        self.bits = bits\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = True):\n        \"\"\"\n        The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        :param fully_fsdp: bool: Determine whether to use the fully_fsdp partitioning scheme or not\n        :return: A list of tuples\n        \n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PS(\"dp\", \"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS(\"fsdp\", \"dp\")),\n            (\"self_attn/o_proj/kernel\", PS(\"dp\", \"fsdp\")),\n\n            (\"mlp/gate_proj/kernel\", PS(\"fsdp\", \"dp\")),\n            (\"mlp/down_proj/kernel\", PS(\"dp\", \"fsdp\")),\n            (\"mlp/up_proj/kernel\", PS(\"fsdp\", \"dp\")),\n\n            (\"input_layernorm/kernel\", PS(None)),\n            (\"post_attention_layernorm/kernel\", PS(None)),\n\n            (\"model/norm/kernel\", PS(None)),\n            (\"lm_head/kernel\", PS(\"fsdp\", \"dp\")),\n            ('.*', PS(None)),\n        ) if not fully_fsdp else (\n\n            (\"model/embed_tokens/embedding\", PS(\"fsdp\")),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS(\"fsdp\")),\n            (\"self_attn/o_proj/kernel\", PS(\"fsdp\")),\n\n            (\"mlp/gate_proj/kernel\", PS(\"fsdp\")),\n            (\"mlp/down_proj/kernel\", PS(\"fsdp\")),\n            (\"mlp/up_proj/kernel\", PS(\"fsdp\")),\n\n            (\"input_layernorm/kernel\", PS(None)),\n            (\"post_attention_layernorm/kernel\", PS(None)),\n\n            (\"model/norm/kernel\", PS(None)),\n            (\"lm_head/kernel\", PS(\"fsdp\")),\n            ('.*', PS('fsdp')),\n        )\n\n    def add_jax_args(self,\n                     gradient_checkpointing: str = 'nothing_saveable',\n                     use_pjit_attention_force: bool = False,\n                     use_flash_attention: bool = False,\n                     use_sacn_mlp: bool = False,\n                     flash_attn_query_chunk_size: int = 1024,\n                     flash_attn_key_chunk_size: int = 1024,\n                     scan_mlp_chunk_size: int = 1024,\n                     number_rep_kv: int = 1,\n                     attn_pdrop: float = 0.0,\n                     c_max_position_embeddings: int = 4096,\n                     freq_max_position_embeddings: int = None,\n                     bits: Optional[int] = None,\n                     **kwargs,\n                     ):\n        \"\"\"\n        The add_jax_args function adds the following arguments to the model:\n\n        :param self: Bind the attributes and methods of a class to an instance of that class\n        :param gradient_checkpointing: str: Determine whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Determine whether to use the pjit_attention_force function\n        :param use_flash_attention: bool: Determine if the flash attention module is used or not\n        :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n        :param flash_attn_query_chunk_size: int: Specify the number of tokens that will be processed at a time\n        :param flash_attn_key_chunk_size: int: Chunk the keys for flash attention\n        :param scan_mlp_chunk_size: int: Chunk the input to the mlp\n        :param number_rep_kv: int: Control the number of times that the key and value vectors are repeated\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of positional embeddings for the causal axis\n        :param freq_max_position_embeddings: int: Set the maximum length of the frequency axis\n        :param bits: Optional[int]: Specify the number of bits to use for quantization\n        :param axis_dims: Sequence[int]: Specify the dimensions of each axis in the tensor\n        :param axis_names: Sequence[str]: Name the axes of the tensors\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Name the axes of the tensor\n        \n        :param backend: typing.Optional[str]: backend to use for model\n        :param : Enable gradient checkpointing\n        :return: A tuple of the following:\n\n        \"\"\"\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n\n\nre_mat = nn_partitioning.remat\n\n\ndef matmul_4d_loop(x, y):\n    \"\"\"Computes the matrix product of two 4D arrays x and y using a loop.\"\"\"\n    result = jnp.zeros(*x.shape[:-2] + x.shape[-2] + y.shape[-1])\n    for i in range(x.shape[0]):\n        for j in range(y.shape[1]):\n            for key in range(x.shape[2]):\n                for l in range(y.shape[3]):\n                    result[i, j, key, l] += x[i, j, key, :] * y[key, l, :, :]\n    return result\n\n\ndef _make_sliding_window_causal_mask(\n        input_ids_shape,\n        dtype: jnp.dtype,\n        past_key_values_length: int = 0,\n        sliding_window: int = 4096,\n):\n    \"\"\"\n    Make causal mask used for sliding window attention\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n\n    tensor = jnp.full(\n        (tgt_len, tgt_len),\n        fill_value=1,\n    )\n    mask = jnp.tril(tensor, 0)\n    mask = jnp.triu(mask, -sliding_window)\n    mask = jnp.log(mask).astype(dtype)\n\n    if past_key_values_length > 0:\n        mask = jnp.concatenate([jnp.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n    return mask[None, None, :, :].repeat(bsz, 0)\n\n\nclass MistralRMSNorm(nn.Module):\n    dim: int\n    eps: float = 1e-6\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n\n    def setup(self) -> None:\n        self.weight = self.param(\n            'kernel',\n            nn.initializers.ones,\n            (self.dim,),\n            self.param_dtype,\n        )\n\n    def _norm(self, x: jnp.ndarray) -> jnp.ndarray:\n        return x * jax.lax.rsqrt(jnp.square(x).mean(-1, keepdims=True) + self.eps)\n\n    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n        x = x.astype(jnp.promote_types(self.dtype, jnp.float32))\n        output = self._norm(x).astype(self.dtype)\n        weight = jnp.asarray(self.weight, self.dtype)\n        return output * weight\n\n\nclass FlaxMistralRotaryEmbedding(nn.Module):\n    dtype: jnp.dtype = jnp.float32\n\n    def __call__(self, key, query, freq_cis, position_ids):\n        sin, cos = freq_cis\n\n        sin = sin[position_ids][:, None, :, :]\n        cos = cos[position_ids][:, None, :, :]\n\n        key = apply_rotary_pos_emb(key, sin, cos)\n        query = apply_rotary_pos_emb(query, sin, cos)\n\n        return query.astype(self.dtype), key.astype(self.dtype)\n\n\nclass FlaxMistralMLP(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            dot_general=get_dot_general_by_bits(self.config.bits)\n        )\n        self.gate_proj = dense(self.config.intermediate_size)\n        self.up_proj = dense(self.config.intermediate_size)\n        self.down_proj = dense(self.config.hidden_size)\n        self.act_fn = ACT2FN[self.config.hidden_act]\n\n    def __call__(self, x: chex.Array):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass FlaxMistralAttention(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            dot_general=get_dot_general_by_bits(self.config.bits)\n        )\n\n        self.q_proj = dense(self.num_heads * self.head_dim)\n        self.k_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.v_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.o_proj = dense(self.hidden_size)\n        self.rotary = FlaxMistralRotaryEmbedding(self.dtype)\n\n    @nn.compact\n    def concatenate_to_cache_(self, query: chex.Array, key: chex.Array, value: chex.Array, attention_mask: chex.Array):\n        is_cache_available = self.has_variable('cache', 'key')\n        key_cache = self.variable('cache', 'key', jnp.zeros, key.shape, key.dtype)\n        value_cache = self.variable('cache', 'value', jnp.zeros, key.shape, value.dtype)\n        index_cache = self.variable('cache', 'index', lambda: jnp.array(0, dtype=jnp.int32))\n        if is_cache_available:\n            *bd, ml, nh, dph = key_cache.value.shape\n            indices = (0,) * len(bd) + (index_cache.value, 0, 0)\n            key = jax.lax.dynamic_update_slice(key_cache.value, key, indices)\n            value = jax.lax.dynamic_update_slice(value_cache.value, value, indices)\n            key_cache.value = key\n            value_cache.value = value\n            num_updated_cache_vector = query.shape[1]\n            index_cache.value = index_cache.value + num_updated_cache_vector\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(ml) < index_cache.value,\n                tuple(bd) + (1, num_updated_cache_vector, ml)\n            )\n            attention_mask = nn.combine_masks(pad_mask, attention_mask)\n        return query, key, value, attention_mask\n\n    @staticmethod\n    def _t(query, key, value):\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def t_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._t(query, key, value)\n        query, key = self.rotary(position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._t(query, key, value)\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n        The __call__ method takes an input tensor (x) and returns an output tensor (y).\n        In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n        :param self: Refer to the object itself\n        :param hidden_state: chex.Array: Pass in the hidden state of the model\n        :param freq_cis: chex.Array: Create the t_rotary variable\n        :param attention_mask: chex.Array: Mask the attention weights\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param position_ids: chex.Array: Specify the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attention weights\n        :return: A tuple of (out, attn_output)\n        \n        \"\"\"\n        batch_size, sequence_length = hidden_state.shape[:2]\n        query, key, value = self.q_proj(hidden_state), self.k_proj(hidden_state), self.v_proj(hidden_state)\n\n        if self.config.use_pjit_attention_force:\n            query = with_sharding_constraint(query, PS('fsdp', 'mp', None))\n            key = with_sharding_constraint(key, PS('fsdp', 'mp', None))\n            value = with_sharding_constraint(value, PS('fsdp', 'mp', None))\n        query, key, value = self.t_rotary(\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            query=query,\n            key=key,\n            value=value,\n            freq_cis=freq_cis,\n            position_ids=position_ids\n        )\n        if self.has_variable('cache', 'key') or init_cache:\n            query, key, value, attention_mask = self.concatenate_to_cache_(query, key, value, attention_mask)\n\n        q_l, k_l = query.shape[1], key.shape[1]\n        if self.has_variable('cache', 'key'):\n            mask_shift: int = self.variables['cache']['index']\n            dl = self.variables['cache']['key'].shape[1]\n            causal_mask = jax.lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, q_l, dl)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :q_l, :k_l]\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n\n        attention_mask = nn.combine_masks(attention_mask, causal_mask)\n\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(query, rtp_axis),\n                k=jnp.transpose(key, rtp_axis),\n                v=jnp.transpose(value, rtp_axis),\n                q_ps=self.config.q_ps,\n                k_ps=self.config.k_ps,\n                v_ps=self.config.v_ps,\n                b_ps=self.config.b_ps,\n                a_ps=self.config.a_ps,\n                bias=attention_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=q_l,\n                kv_seq_len=k_l,\n                attn_pdrop=self.config.attn_pdrop,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            attention_bias = lax.select(\n                attention_mask > 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n            if self.config.use_shard_map:\n                attn_weights = shard_map(\n                    functools.partial(\n                        dot_product_attention_weights,\n                        dtype=jnp.promote_types(self.dtype, jnp.float32),\n                        deterministic=deterministic,\n                        dropout_rate=self.config.attn_pdrop,\n                        precision=self.precision,\n                    ),\n                    mesh=self.config.jax_mesh(),\n                    in_specs=(\n                        self.config.q_ps,\n                        self.config.k_ps,\n                        self.config.b_ps\n                    ),\n                    out_specs=PS((\"dp\", \"fsdp\"), None, None, None),\n                    check_rep=False\n                )(\n                    query, key, attention_bias\n                )\n            else:\n                attn_weights = dot_product_attention_weights(\n                    query=query,\n                    key=key,\n                    bias=attention_bias,\n                    dtype=jnp.promote_types(self.dtype, jnp.float32),\n                    deterministic=deterministic,\n                    dropout_rate=self.config.attn_pdrop,\n                    precision=self.precision,\n                )\n\n            if self.config.use_pjit_attention_force:\n                attn_weights = with_sharding_constraint(attn_weights, PS((\"dp\", \"fsdp\"), \"mp\", None, None))\n\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n\n        out = self.o_proj(attn_output.reshape(batch_size, sequence_length, self.hidden_size))\n        outputs = (out, attn_weights) if output_attentions else (out,)\n        return outputs\n\n\nclass FlaxMistralDecoderLayer(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        self.self_attn = FlaxMistralAttention(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.mlp = FlaxMistralMLP(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.input_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attention_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_state (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        :param self: Represent the instance of the class\n        :param hidden_state: chex.Array: Represent the input to the encoder layer\n        :param freq_cis: chex.Array: Pass the frequency information to the attention layer\n        :param attention_mask: chex.Array: Mask out the attention weights for certain positions\n        :param causal_mask: chex.Array: Mask the future tokens\n        :param position_ids: chex.Array: Indicate the position of each token in the sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache for the self-attention layer\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :return: A tuple of hidden_state and attention_output\n        \n        \"\"\"\n        residual = hidden_state\n        attention_output = self.self_attn(\n            hidden_state=self.input_layernorm(hidden_state),\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            causal_mask=causal_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions\n        )\n\n        hidden_state = attention_output[0] + residual\n\n        hidden_state = self.mlp(self.post_attention_layernorm(hidden_state)) + hidden_state\n        outputs = (hidden_state,)\n        if output_attentions:\n            outputs += attention_output[1]\n        return outputs\n\n\nclass FlaxMistralPretrainedModel(FlaxPreTrainedModel):\n    config_class = MistralConfig\n    base_model_prefix = 'mistral'\n    module_class: nn.Module = None\n\n    def __init__(self,\n                 config: MistralConfig,\n                 input_shape: Tuple = (1, 1),\n                 seed: int = 0,\n                 dtype: jnp.dtype = jnp.bfloat16,\n                 _do_init: bool = True,\n                 **kwargs\n                 ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: flax.core.FrozenDict = None\n    ) -> flax.core.FrozenDict:\n\n        \"\"\"\n        The init_weights function is used to initialize the weights of a model.\n        It takes in an rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        :param self: Access variables that belong to the class\n        :param rng: jax.random.PRNGKey: Initialize the weights of the model\n        :param input_shape: Tuple: Initialize the input_ids, attention_mask and position_ids\n        :param params: flax.core.FrozenDict: Pass in the parameters of a pre-trained model\n        :return: A frozendict of parameters\n        \n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rng_s,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(rng_s, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask=None,\n            position_ids=None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input sequence to the model\n        :param attention_mask: Mask out the padding tokens\n        :param position_ids: Specify the position of each token in the sequence\n        :param params: dict: Pass in the parameters of the model\n        :param past_key_values: dict: Pass the past key values to the model\n        :param dropout_rng: jax.random.PRNGKey: Pass in a random number generator key to the model\n        :param train: bool: Determine whether to use dropout or not\n        :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n        :param output_hidden_states: Optional[bool]: Determine whether to return the hidden states of all layers\n        :param return_dict: Optional[bool]: Return a dictionary of the outputs\n        :param add_params_field: bool: Add a params field to the inputs dictionary\n        :return: A tuple of (last_hidden_state, past_key_values)\n        \n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if self.config.bits is not None:\n            rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            None,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n\n\nclass FlaxMistralDecoratorCollection(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -> None:\n        block = FlaxMistralDecoderLayer\n        if self.config.gradient_checkpointing != '':\n            block = re_mat(\n                block,\n                static_argnums=(5, 6, 7),\n                policy=get_gradient_checkpoint_policy(\n                    self.config.gradient_checkpointing\n                )\n            )\n        self.layers = [\n            block(\n                config=self.config,\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision,\n                name=str(i)\n            ) for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False\n    ):\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n        for layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_state,)\n            output = layer(\n                hidden_state,\n                freq_cis,\n                attention_mask,\n                causal_mask,\n                position_ids,\n                deterministic,\n                init_cache,\n                output_attentions\n            )\n            hidden_state = output[0]\n\n            if output_attentions:\n                output_attentions += (output[1],)\n\n        return hidden_state, all_hidden_states, all_attentions\n\n\nclass FlaxMistralModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n\n        self.layers = FlaxMistralDecoratorCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm = MistralRMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n        self.freq_cis = precompute_freq_cis(\n            max_position_embedding=self.config.freq_max_position_embeddings if self.config.freq_max_position_embeddings is not None else self.config.max_position_embeddings,\n            head_dim=self.config.hidden_size // self.config.num_attention_heads\n        )\n        self.causal_mask = nn.make_causal_mask(jnp.ones((1, self.config.c_max_position_embeddings), dtype='i4'))\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            input_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n\n    ) -> typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n        \"\"\"\n        The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n        The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n\n        :param self: Represent the instance of the class\n        :param input_ids: chex.Array: Pass in the input ids\n        :param attention_mask: chex.Array: Mask out the attention weights for certain tokens\n        :param position_ids: chex.Array: Determine the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param input_embeds: chex.Array: Pass in the embedding of the input_ids\n        :param init_cache: bool: Initialize the cache for the decoder\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :param output_hidden_states: bool: Return all hidden states or just the last one\n        :param return_dict: bool: Return a dictionary of the outputs or not\n        :param : Determine whether the model is in training mode or not\n        :return: A tuple of the hidden states, all hidden states, and attentions\n        \n        \"\"\"\n        if input_embeds is None:\n            input_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n        if attention_mask.ndim == 2:\n            b, s = attention_mask.shape\n            attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n        outputs = self.layers(\n            hidden_state=input_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            freq_cis=self.freq_cis,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            causal_mask=self.causal_mask\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(value for value in outputs if value is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n\n\nclass FlaxMistralModel(FlaxMistralPretrainedModel):\n    module_class = FlaxMistralModule\n\n\nclass FlaxMistralForCausalLMModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model: FlaxMistralModule = FlaxMistralModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n            dot_general=get_dot_general_by_bits(self.config.bits)\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            input_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"\n            The __call__ function is the main function of a Flax module. It defines how the model will be called,\n            and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n            as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n            the call function: deterministic (whether to use dropout), input_embeds (if you want to pass your own embeddings),\n            output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n            :param self: Refer to the object itself\n            :param input_ids: chex.Array: Pass in the input tokens\n            :param attention_mask: chex.Array: Mask out the padding tokens\n            :param position_ids: chex.Array: Specify the position of each token in the sequence\n            :param deterministic: bool: Determine whether to use dropout in the model\n            :param input_embeds: chex.Array: Pass in the embeddings of the input tokens\n            :param init_cache: bool: Initialize the cache for the decoder\n            :param output_attentions: bool: Return the attention weights\n            :param output_hidden_states: bool: Return the hidden states of all layers\n            :param return_dict: bool: Return a dictionary of the outputs or just the logits\n            :param : Determine whether to return the logits or not\n            :return: A tuple of (lm_logits, hidden_states, attentions)\n            \n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        if attention_mask is None: attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            input_embeds=input_embeds,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        # lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n\n\nclass FlaxMistralForCausalLM(FlaxMistralPretrainedModel):\n    module_class = FlaxMistralForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = jax.lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    @staticmethod\n    def update_inputs_for_generation(model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py b/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py
--- a/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py	(revision 3031818c8b802088e00f0bddd69481ee48ecc180)
+++ b/lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py	(date 1702753526171)
@@ -176,22 +176,21 @@
             ("lm_head/kernel", PS("fsdp", "dp")),
             ('.*', PS(None)),
         ) if not fully_fsdp else (
+            ("model/embed_tokens/embedding", PS(("fsdp", "sp"))),
 
-            ("model/embed_tokens/embedding", PS("fsdp")),
+            ("self_attn/(q_proj|k_proj|v_proj)/kernel", PS(("fsdp", "sp"))),
+            ("self_attn/o_proj/kernel", PS(("fsdp", "sp"))),
 
-            ("self_attn/(q_proj|k_proj|v_proj)/kernel", PS("fsdp")),
-            ("self_attn/o_proj/kernel", PS("fsdp")),
-
-            ("mlp/gate_proj/kernel", PS("fsdp")),
-            ("mlp/down_proj/kernel", PS("fsdp")),
-            ("mlp/up_proj/kernel", PS("fsdp")),
+            ("mlp/gate_proj/kernel", PS(("fsdp", "sp"))),
+            ("mlp/down_proj/kernel", PS(("fsdp", "sp"))),
+            ("mlp/up_proj/kernel", PS(("fsdp", "sp"))),
 
             ("input_layernorm/kernel", PS(None)),
             ("post_attention_layernorm/kernel", PS(None)),
 
             ("model/norm/kernel", PS(None)),
-            ("lm_head/kernel", PS("fsdp")),
-            ('.*', PS('fsdp')),
+            ("lm_head/kernel", PS(("fsdp", "sp"))),
+            ('.*', PS(("fsdp", "sp"))),
         )
 
     def add_jax_args(self,
@@ -348,7 +347,7 @@
             param_dtype=self.param_dtype,
             precision=self.precision,
             kernel_init=nn.initializers.normal(),
-            dot_general=get_dot_general_by_bits(self.config.bits)
+            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)
         )
         self.gate_proj = dense(self.config.intermediate_size)
         self.up_proj = dense(self.config.intermediate_size)
@@ -381,7 +380,7 @@
             param_dtype=self.param_dtype,
             precision=self.precision,
             kernel_init=nn.initializers.normal(),
-            dot_general=get_dot_general_by_bits(self.config.bits)
+            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)
         )
 
         self.q_proj = dense(self.num_heads * self.head_dim)
@@ -460,9 +459,9 @@
         query, key, value = self.q_proj(hidden_state), self.k_proj(hidden_state), self.v_proj(hidden_state)
 
         if self.config.use_pjit_attention_force:
-            query = with_sharding_constraint(query, PS('fsdp', 'mp', None))
-            key = with_sharding_constraint(key, PS('fsdp', 'mp', None))
-            value = with_sharding_constraint(value, PS('fsdp', 'mp', None))
+            query = with_sharding_constraint(query, PS("fsdp", "sp", None))
+            key = with_sharding_constraint(key, PS("fsdp", "sp", None))
+            value = with_sharding_constraint(value, PS("fsdp", "sp", None))
         query, key, value = self.t_rotary(
             batch_size=batch_size,
             sequence_length=sequence_length,
@@ -555,7 +554,7 @@
                         self.config.k_ps,
                         self.config.b_ps
                     ),
-                    out_specs=PS(("dp", "fsdp"), None, None, None),
+                    out_specs=PS(("dp", "fsdp"), "sp", "tp", None),
                     check_rep=False
                 )(
                     query, key, attention_bias
@@ -572,7 +571,7 @@
                 )
 
             if self.config.use_pjit_attention_force:
-                attn_weights = with_sharding_constraint(attn_weights, PS(("dp", "fsdp"), "mp", None, None))
+                attn_weights = with_sharding_constraint(attn_weights, PS(("dp", "fsdp"), "sp", "tp", None))
 
             attn_output = jnp.einsum("...hqk,...khd->...qhd", attn_weights, value)
 
@@ -1027,7 +1026,7 @@
             use_bias=False,
             kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),
             precision=self.precision,
-            dot_general=get_dot_general_by_bits(self.config.bits)
+            **get_dot_general_by_bits(self.config.bits, self.config.easy_method)
         )
 
     def __call__(
