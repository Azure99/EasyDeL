{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU and TPU Inference with EasyDeL\n",
    "In this tutorial, weâ€™ll walk through setting up and performing efficient inference on both GPU and TPU devices using EasyDeL. This setup utilizes model parallelism and quantization for optimized performance.\n",
    "\n",
    "### Requirements\n",
    "Ensure you have EasyDeL, Transformers, JAX, and Torch installed in your environment.\n",
    "(torch is not needed in case that your not loading a torch model and using already converted EasyDeL Models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/erfanzar/EasyDeL.git -q\n",
    "!pip install jax[cuda12] -q\n",
    "# or install jax for TPUs\n",
    "!pip install torch -q # For GPUS only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Required Libraries\n",
    "Begin by importing the necessary libraries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydel as ed\n",
    "import jax\n",
    "import transformers\n",
    "from jax import numpy as jnp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 2. Configure Model and Inference Parameters\n",
    "Define the model and inference settings. Adjust the sharding_axis_dims, dtype, and precision for either GPU or TPU:\n",
    "\n",
    "##### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "max_length = 8192  # Maximum length of input sequences\n",
    "num_devices = jax.device_count()\n",
    "\n",
    "# Initialize the model with specific sharding and quantization settings\n",
    "model, params = ed.AutoEasyDeLModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    sharding_axis_dims=(1, 1, 1, -1),  # Adjust this based on device type\n",
    "\t\t# Use Sequence Sharding or Tensor Parallelization for this\n",
    "    auto_shard_model=True,\n",
    "    dtype=jnp.float16, \n",
    "    param_dtype=jnp.float16,\n",
    "    precision=None,\n",
    "    input_shape=(num_devices, max_length),\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_method=ed.EasyDeLQuantizationMethods.A8BIT,  # Use 8-bit quantization for inference efficiency\n",
    "    config_kwargs=ed.EasyDeLBaseConfigDict(\n",
    "        quantize_kv_cache=True,\n",
    "        attn_dtype=jnp.float16,\n",
    "        attn_mechanism=ed.AttentionMechanisms.FLASH_ATTN2,  # Faster attention mechanism\n",
    "        mask_max_position_embeddings=max_length,\n",
    "        freq_max_position_embeddings=max_length,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Parameters\n",
    "\n",
    "- *Sharding Axis*: Set to (1, 1, 1, -1) to optimize for sequence sharding or tensor parallelization on TPUs or GPUs.\n",
    "\n",
    "- *Quantization*: We use 8-bit quantization to reduce memory usage and improve inference speed.\n",
    "\n",
    "- *Attention Mechanism*: FLASH_ATTN2 provides efficient attention handling for large sequences.\n",
    "\n",
    "- *Precision*: Set to float16 for efficient computation on hardware accelerators.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Prepare the Tokenizer\n",
    "Load the tokenizer for preprocessing the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 4. Initialize the Inference Class\n",
    "\n",
    "Set up the vInference class to handle inference requests. Configure generation parameters to control text generation, such as temperature, top-k sampling, and maximum token length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = ed.vInference(\n",
    "\tmodel=model,\n",
    "\tparams=params,\n",
    "\ttokenizer=tokenizer,\n",
    "\tgeneration_config=ed.vInferenceConfig(\n",
    "\t\ttemperature=0.8,\n",
    "\t\ttop_k=10,\n",
    "\t\ttop_p=0.95,\n",
    "\t\tbos_token_id=model.generation_config.bos_token_id,\n",
    "\t\teos_token_id=model.generation_config.eos_token_id,\n",
    "\t\tpad_token_id=model.generation_config.pad_token_id,\n",
    "\t\tstreaming_chunks=32,\n",
    "\t\tmax_new_tokens=1024,\n",
    "\t),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation Configuration\n",
    "\n",
    "- *Temperature*: Controls randomness; higher values produce more diverse outputs.\n",
    "- *Top-k and Top-p*: Top-k sampling selects the k most likely next tokens, while top-p sampling dynamically adjusts the number of tokens based on cumulative probability.\n",
    "- *Max New Tokens*: Limits the number of tokens generated per inference.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Precompile the Model for Inference.\n",
    "\n",
    "For inference, it is beneficial to precompile the model to optimize execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.precompile(batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 6. Deploy an API Server for Inference\n",
    "Use vInferenceApiServer to expose the model as an API, allowing asynchronous requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_inference = ed.vInferenceApiServer(\n",
    "\t{inference.inference_name: inference}\n",
    ")  # you can load multi inferences together\n",
    "api_inference.fire()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This server will be ready to receive inference requests, making it ideal for deploying in a production environment.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary\n",
    "This tutorial demonstrated how to configure and run inference on both `GPU` and `TPU` devices with `EasyDeL`. The setup used sharding, quantization, and efficient attention mechanisms to optimize inference. Adjustments in precision, sharding configuration, and precompilation steps allow seamless usage across different hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
