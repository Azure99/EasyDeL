# Model ðŸ¤–ðŸ’«

## Available Models Are

1. **_[Llama](https://erfanzar.github.io/EasyDeL/docs/Llama)_**:

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing
        * Flash Attention
        * BlockWise Attention


- **_[Llama2](https://erfanzar.github.io/EasyDeL/docs/Llama2)_**:

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing
        * Flash Attention
        * BlockWise Attention


- **_GPT-J_** :

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing
        * Flash Attention
        * BlockWise Attention


- **_LT_** :

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing


- **_[MosaicMPT](https://erfanzar.github.io/EasyDeL/docs/MosaicMPT)_**:

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing
        * Flash Attention
        * BlockWise Attention


- **_GPTNeoX_**  :

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing


- **_[Falcon](https://erfanzar.github.io/EasyDeL/docs/Falcon)_**:

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing
        * Flash Attention
        * BlockWise Attention


- **_Palm_**:

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing


- **_T5_**:

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing


- **_OPT_**:

    * Supports:
        * Fully Sharded Data Parallel `(FSDP)`
        * MultiProcessing `(MP)`
        * Data Parallel `(DP)`
        * Distributed Data Parallel  (DDP) `(DP)`
        * Gradient CheckPointing
